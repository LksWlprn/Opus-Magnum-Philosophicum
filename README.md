# Metaphysics

- [On Ontology](#Ontology)
- [On Epistemology](#Epistemology)
- [On Personal Identity](#Identity)
- [On the Theory of Mind](#Consciousness)
- [On Morality](#Morality)
- [On Free Will](#Control)

## Ontology

### Existence
The most fundamental metaphysical question is: Why is there anything at all? Until recently, science could not help us in answering this question, but I now believe that it can. 

Since nothing can come from nothing, absolute nothingness should persist, yet reality exists. 
One possible answer, the zero-ontology, holds that reality can be both there and not there and sums to nothing overall, much like how matter and antimatter cancel out. But this isn't more than a hunch.
A better approach begins by examining the nature of nothingness. 
One kind of nothingness from which everything could have started is the total absence of anything. This would include the lack of any boundary conditions, since there would have to be something to define a boundary condition. There cannot be any definitions in such nothingness. 
Yet, this absence of constraints would itself be a constraint (a definition), meaning that absolute nothingness contradicts itself and is impossible. It will become clear, when considering my thoughts on epistemology, that contradiction is the only ultimate truth-finding tool we possess. Logical contradiction is the only true impossibility, and even nothingness cannot contradict itself, for it needs to be in the realm of possibility. The only non-contradictory kind of nothingness would be one that is at least definable: 
a state of complete emptiness, where empty does not mean empty spacetime, but rather spacelessness and timelessness. But in such a state necessary truths, meaning logical truths, are still necessarily existent. 
If they did not exist, there would be no constraints on reality at all, allowing anything to exist arbitrarily, which would be exactly the contradictory form of nothingness we already ruled out as self-defeating. Necessary truths must exist necessarily. 
There are infinitely many possible formal systems, and all such systems must exist in an abstract sense. This corresponds to Stephen Wolfram’s Ruliad, the space of all possible rule-based structures, within which our universe is a fragment. 
Therefore, I claim that Wolfram's model of everything must be, from logical a priori reasoning alone, very close to being the correct model of everything, given our evidence that something exists at all.

Wolfram himself comes to the same conclusion: "The set of all possible rules is something purely formal. It can be represented in an infinite number of ways. But it’s always there, existing as an abstract thing, completely independent of any particular instantiation.
It’s crucial that we’re talking about all possible rules. If we were talking about particular rules, then we’d need to specify which rules those are, and we’d need a whole language and structure for doing that. But that’s not our situation. We’re talking about all possible rules. We can construct some explicit symbolic representation for these rules, but the deductions we make ultimately won’t depend on this; they would work the same whatever representation we chose to construct.
We might have assumed that to get our universe we’d need some definite input, some specific information. But what we’re discovering is that our universe is in some sense like a tautology; it’s something that has to be the way it is just because of the definition of terms. In effect, it exists because it has to, or in a sense because everything about it is a “logical inevitability”, with no choice about anything. [...] At some level, we can just think of rules abstractly being applied. But in our models of physics, there’s an interpretation of that: sequences of rules being applied define the passage of time. There are “logically inevitable” chains of rule applications. But for us as observers embedded in the system—particularly with our way of setting up our thread of consciousness—we perceive this as time passing, or in effect, “things happening in the universe”.
Talking about things this way might lead one to ask “What got the universe started?” “What makes rules actually get applied?” Well, nothing. Because the rules are just defining how abstract sequences can be constructed. And if you follow a sequence, it can be interpreted as reflecting the passage of time. But there’s no “driver” that’s saying anything like “now this rule gets applied”. The sequences generated by the successive application of rules are somehow just abstract “logical possibilities”."

"Across our hypergraph there’s a huge amount of irreducible computation that goes on—with all the various individual “atoms of space” in effect continually getting regenerated. But we humans have a particular way of experiencing all this—that involves sampling just those features that allow us to construct a coherent “thread of consciousness”.
It’s not obvious that such sampling could maintain its consistency over time, or could make different “consciousnesses” come to the same conclusions about the universe. But the phenomenon of causal invariance (which itself can be an emergent consequence of constructing a thread of consciousness) makes such things inevitable.
And in the end we can conclude that with our particular “sequentialized” way of “reading the universe” as computationally bounded observers, it’s inevitable that we’ll end up concluding that the universe follows the particular laws of general relativity and quantum mechanics that physics has established.
Underneath, there’s all sorts of computational irreducibility that we’re not directly sensitive to. But as computationally bounded observers we’re picking out a certain slice of computational reducibility—and with our particular sequentialization of experience in time, the slice we pick out corresponds to our known laws of physics."

"The overall structure of the whole rulial universe is inevitable. But the particular place in rulial space at which we find ourselves is a matter of how we choose things."

### Mereological Nihilism, Platonic Realism, Particulars and Universals

This view of reality supports mereological nihilism, as everything we observe reduces to vertices in a hypergraph, and platonic realism, as abstract logical rules must ideed have an objective existence. This, in turn, also supports my epistemological claims below, that objective truths do exist. However, even though abstract information does objectively exist, the fact that something is objectively true doesn't create reality. Reality, as we experience it, is a result of abstract informational relations. For example, the existence of the number 7 doesn't have meaning in itself, unless this information has some relation to other information, which makes it the number 7. The relations create reality and our reality is the ultimate abstract object of all possible relations. One might mistakenly conclude that this implies the existence of only particulars and no universals, apart from the ultimate Ruliad itself. However, this is not the logical conclusion. Instead, equivalent structures within the hypergraph give rise to multiply instantiated universals.

Take, for example, an electron, that is a specific hypergraph configuration within the larger structure. Since all electrons share the same underlying configuration, they are universals. Similarly, larger emergent structures arise within the hypergraph, carrying new information not inherent in their smaller components. We can refer to these structures as "systems." Such emergent systems include physics, chemistry, biology, and consciousness, which emerge from pockets of computational reducibility.
While the hypergraph structures of these systems are not themselves universals—for instance, no two biological cells have identical hypergraph configurations—they represent a new level of emergent information, within which universals can exist. For example, if we model cells as nodes in a functional network forming an organism, certain parts of the network can be structurally identical and thus form universals within their own emergent ontologies.

There are two types of universals, arising from two distinct principles: structural equivalence and functional equivalence. Electrons, for instance, are functionally equivalent because any electron exhibits the same input-output relationships as any other. Whether an electron is from Mars or Earth, its repulsion behavior remains unchanged. This functional equivalence stems from a deeper structural equivalence, as all electrons share the same underlying configuration.
But in emergent systems like biology, exact structural equivalence is rare. As I've said, the structure of cells is never truly identical, yet their emergent ontology is based not on precise structure but on shared input-output relationships within their network, making it a functional network. This functional equivalence allows them to be treated as universals within their new ontology.

As John Boodin wrote in his paper "Functional realism" (1934): " As opposed to the assumption of things in themselves with properties in themselves, functional realism holds in common with present science that the bifurcation of thing and environment is vicious, that things exist only in fields, in mutuality with other things, and that they have properties only in their dynamic interrelations. We may illustrate this conception with the property of weight, one of the most fundamental properties of the physical world. Democritus and Plato thought that weight is due to the amount of matter. Since the atom is supposed to be a plenum, i.e., absolutely full of matter, the difference in the weight of atoms must be due to difference in size. Weight was supposed to be an absolute property and in no way affected by the environment of the atom. Newton discovered that it is functional, that it depends upon the relation of matter to its environment. Without an environment of matter there would be no meaning to saying that an atom has weight. [...] A thing does not do anything by itself. It exists and has properties only when it reacts with something. It is itself only as it is mutual, as Hegel might say. Its properties are due to what it acts on as well as to itself. The physicist has learned that he cannot say what an electron is when it does not act. Its character of individual particle is very likely due to the encounter with radiation or another electron. All we know about it is what it is when it acts. Anything else is a post facto projection. And that is true of everything else."

It needs to be stressed, though, that many of the so-called universals in everyday life are not truly universals, just because of some similarity between them. For example, a drawn triangle is not a universal, just because it has the shape of a triangle. It is not a structural universal, because it doesn't have an identical structure to other drawn triangles and it isn't a functional universal, because it is not interacting with a larger system. Only the representation of the drawn triangle in our mind could be a universal because of the specific functional relationship it forms with our mind. But then, we are not talking about the drawn triangle anymore. We would be talking about the mental representation of it. Conceptualists are right in thinking about many classic examples of universals as conceptual universals rather than real universals, but conceptualism cannot be right overall. Functional realism and ontic structural realism are complementary more accurate answers to what really ontologically exists.

It follows from this that there are different levels of ontologal reality (what might be called levels of emergence), corresponding to different perspectives. Within biological systems, components can be determined by their functional roles. Within such a system, anything performing the same function becomes the same module/component. Within a chemical system, any molecules performing the same reaction are functionally equivalent components. Within the mind, any brain-process performing the same mental function is the same quale.
In a society, anyone performing the same role is functionally equivalent. However, on a societal level and also on the biological level, it is very rare to actually encounter two functionally equivalent components, since to be functionally equivalent means to have exactly the same kind of input/output relationships. Even though there are similarities between all car-drivers, switching one car driver with another would not be functionally equivalent. Such things, of which we invented concepts just to categorize them without them being functionally equivalent, are not universals. The concepts themselves might be universals, but not the things that these concepts try to represent. Men and women can be regarded as universals only from certain perspectives and particulars from other perspectives. Imagine a regular thermostat and a person performing the operation of a thermostat (a similar thought experiment is the Chinese room by John Searle). Let's say that in both cases the input-output relations are the same. From an outsider's perspective these thermostats are functionally equivalent, while from the insider's perspective they are not. This is not an argument against functional realism. It highlights the perspectival nature of reality and what functional realism entails. The status of universality or particularity depends perspectivally on the ontological level in question.

It might also be said, though it's not a rigorous argument, that merely societally agreed upon concepts are in a way less real than concepts based on ontic structural realism or functional realism, because they depend on the existence of that specific society, while functionally or structurally real concepts would be rediscovered by any society. (This argument is inspired by and expands the argument that some societally agreed upon religion would not be rediscoverable if that society were to be wiped out, while the scientifc knowledge of that society would be rediscoverable by any other society.)


## Epistemology

### The Problem of Induction

There are two logical solutions to the problem of induction. To find these solutions it is paramount to think clearly about the problem. Arnold Zuboff writes:

"An a priori discovery is one that is made independent of any 
empirical confirmation because it is the discovery of a necessary truth, whose contrary is a 
contradiction. That which is known a priori can therefore be known (as the phrase suggests) in a 
way that is logically prior to experience. In a posteriori discovery we acquire beliefs empirically, 
based on the patterns of experience, like the belief that a lectern is in a room. The contraries of such 
propositions are not contradictory, and such propositions therefore, with only one kind of 
exception, do not possess the certainty of necessary truths."

The problem of induction arises from the fact that after repeatedly observing "A is X," it remains non-contradictory to observe "A is not X." 
Therefore, we cannot conclude that "All A are X." The only thing we can logically establish from observation are falsifications: while observing "A is not X" is not inherently contradictory, asserting "All A are X" after such an observation would be contradictory. 

A single counterexample refutes a hypothesis because it would be a contradiction for the hypothesis to remain true.
However, for falsification to be reliable, we must ensure that observations are repeatable and properly controlled. 
In Popperian style, through intersubjective corroboration and coherence with broader theoretical frameworks, we would be able to increase the verisimilitude of our conclusions and assume "A is X" to be provisionally falsified. 
The principle of contradiction is the key to understand the solutions to Hume's problem of induction: it is not a contradiction to assert, for example, that "not all swans are white" when one has only observed white swans. 
Thus, we cannot assume "all swans are white".
Falsification means that if a black swan is observed, "all swans are white" is necessarily false, allowing us to refine our understanding by eliminating incorrect hypotheses. 
However, falsification alone does not determine which remaining hypothesis is most likely true. For example, when Uranus was found in 1781, its orbit diverged from the predictions of Newtonian Mechanics. However, this was not seen to be falsifying Newtonian Mechanics. 
Rather, an eighth planet was postulated to have influenced the orbit and this led to the discovery of Neptune. This was seen as being the far more probable explanation compared to all of Newtonian Mechanics being wrong. This leads us to the second solution: probability. 
While it is not contradictory to state that "not all swans are white" when only white swans have been observed, it would be contradictory to say, "it is improbable that all swans are white," if only white swans had consistently been observed. 
If the unseen population contained a large number of non-white swans, it would be highly improbable to have exclusively observed white ones and it is improbable that something improbable happened. It is a contradiction that something improbable was probable to happen. 

So, we follow Zuboff's principle of highest probability: given our sample, "all swans are white" is likely true until contrary evidence emerges.
This principle also applies to falsified hypotheses. If we observe a black swan, we must attribute some confidence to that observation. 
Popper was therefore mistaken in claiming that we cannot assign probabilities to hypotheses, because he did not see this solution to the problem of induction. However, he was correct that we must first conjecture hypotheses and proceed by falsification. 
Consider a world before 1900, where Newton’s theory of gravity was considered highly probable. It later turned out to be strictly false. Today, we know Einstein’s theory is also likely incomplete, as it is incompatible with quantum mechanics. 
Nevertheless, it remains the best-supported theory given current evidence.
In an ideal scenario where all possible hypotheses were conjectured and all falsification tests were known, probability could determine which hypothesis would be most likely true. 
However, in reality, we lack such completeness. We also acknowledge that falsification is never absolute—errors, oversight, or hallucinations are always possible. Yet, probabilistically, it is improbable that, for example, an entire scientific community hallucinated a black swan. 

Zuboff explains his principle using a story of observing an imaginary species called humebirds, who, until now, have been observed to be blue: "There is indeed, as Hume
insisted, no logical necessity that the next humebird be blue; but there is a logical
necessity that it is probable that the next humebird will be blue given this evidence.
For it is necessarily probable that this collection of random samples has a similar
proportion of blueness to that of the general population from which it has been taken.
Let me explain.
I think that while we are observing the 100 humebirds, rather than forming a Humean
habit of expectation, we are calculating implicitly the most probable hypothesis
concerning the general population of humebirds from which these observed birds are
being randomly sampled. That hypothesis regarding this population that we are
justifiably coming to favour as most probable is the one that would make the
occurrence of the evidence, our observations, as highly probable an event as it can
be. For that high probability of the evidence within the hypothesis necessarily lends
its weight to the probability of the hypothesis itself, as I shall explain.
It may help us in this to consider an example of a hypothesis that we would justifiably
reject as improbable. The worst of these would be the hypothesis that the only birds
that are blue among the population of humebirds we were sampling happened to be
the 100 we have already seen. If that were true, then it would have been highly
probable that non-blue birds would have got mixed into the first hundred
observations. And our actual observations of only blue humebirds would have had to
be an extremely improbable event. But, as I often observed in our earlier discussion,
something improbable necessarily had a low probability of occurring. Hence the
improbability of the evidence given this hypothesis makes the hypothesis combined
with the evidence necessarily improbable.
That the observations were of nothing but blue humebirds, however, gets less and
less improbable in those hypotheses that increase the proportion of humebirds that
are blue. The truth of these hypotheses therefore, along with that of the evidence,
involves less improbability. The least improbable hypothesis must be that the
humebirds across the whole population being sampled were generally blue. That is
the hypothesis we implicitly settle on as most probable. And it goes along with this
hypothesis, of course, that the next humebird sampled from the same population
(and under the same general conditions) should be expected to be blue. And this, I
contend, is the implicit thinking that rightly makes us expect that the next humebird
will be blue.
This reasoning constitutes an a priori justification of induction. But I have run into
people with suspicions that I am cheating in claiming that all my reasoning about the
observed evidence was done a priori, based on nothing but necessary truth whose
denial would be a contradiction. Why? Well, I had used probabilities and they
thought that probabilities must have been empirically derived. But anyone who looks
carefully will see that none of the probabilities appealed to are established
empirically. For example, I claimed along the way that the first 100 random samples
being purely blue would be improbable within a hypothesis that only 100 humebirds
are blue and the rest not blue. For it would be a necessary, mathematical truth that
there would be many more ways in which the first 100 observations could have gone
differently, with other colours showing earlier, within the hypothesis of only 100 out of
the whole humebird population being blue. Nothing there is established empirically.
The only empirical part of the whole thing would be observing the colour of the
actually sampled humebirds (which in our case, to help us raise Hume’s problem, is
simply given to have been blue). But the probabilities in our competing hypotheses
are fixed a priori, within them. We then correctly judge to be most probable among
them that hypothesis in which the observations would have been produced with the
highest probability. For it is a necessary truth that that which is more probable is
more probable to have occurred."

Zuboff concludes: "I call the single a priori principle that I believe governs one’s overall empirical thinking the ‘highest 
probability principle’. It requires us always to favour in our beliefs, as most probable to have 
occurred, that overall context of our current experience that would, as discovered purely in our 
concepts of it, have had inherent in it the highest probability of having produced the pattern of our 
current experience. We must do so because it is a necessary truth that the pattern’s being produced 
in the most probable way is an event that was in itself more probable to have occurred than the 
pattern’s being produced in any less probable way. Let me just add that sometimes, of course, we 
must believe that an event which was locally improbable is the most probable to have occurred; but 
we can only properly do so when this local improbability has been needed in strictly the most 
probable overall hypothesis."


So, both falsification and Bayesian inference contribute to solving the problem of induction. They represent two distinct types of contradiction in relation to a given hypothesis, forming the foundation for knowledge acquisition. 


### Relativism

"The claim may be that all truths are relative, true only in the
context of a time or a culture or for a person, within a conceptual scheme. A
proponent of such a thesis apparently hasn’t noticed that any total relativism about
truth, whether historical, cultural or personal, must refute itself in either of two ways:
The supposed relativism must either rule out or else rule in its own objective (nonrelative) truth. Either way it refutes itself. Even if relativism takes itself to be
renouncing its own objective truth it can only be doing so from an implicit position of
assumed objective truth.
This is like what we’ve seen already. But in a relativist thesis we find also a new
twist beyond the sort of rejection of the absolute we have so far encountered. For
although relativism is a rejection of objectivity, it presents itself at the same time as
an acceptance of all competing views as somehow equally true or valid. We might
characterize the first positions we considered as sounding extremely negative. They
wished to say no to any value judgment or any truth, any absolute principle or any
certainty, or the establishment of any philosophical thesis. Relativist theses, on the
other hand, may sound extremely positive. They seem to welcome every value
judgment as valid, every proposition as true or every philosophical thesis as capable
of being established. Yet the negative and the positive-sounding positions are closely
allied. Both kinds of claims tend to appeal to the same people, as different ways of
saying that nobody can occupy a privileged platform of objective validity. The
positive-sounding claims of relativism, however, do render it distinctively vulnerable
to a second layer of criticism as self-refuting. For, not only are such positions, in a
fashion that resembles the self-refuting we have already seen, committed to rejecting
their own objective significance, but, in addition, the relativist positions, according to
their seeming tolerance and generosity, must welcome (as well as reject) any views
that oppose them. Let me explain.
The truth of the view that all truth is relative (to times, to cultures, or to persons) must
itself be merely relative if the view itself is to be true. But we can see that the truth of
relativism cannot for itself be relative by noticing how relativism must, if merely
relatively true, treat the opposing view, that truth may be objective across times,
cultures, persons and all their conceptual schemes. If a time, a culture, or a person
was objectivist, then the relativist, if he’s a relativist about relativism, must grant the
truth of the objectivist view within that time or that culture or for that person. Note
that such a relative truth, the contradictory relative truth of objectivism, cannot be
what the objectivist himself could count as the truth of his own theory--so by the
objectivist’s own standards, relative to him, he is not herein being granted any sort of
truth for his theory at all. But the objectivism, even if we did allow that it could be
true in some such merely relative fashion, would then only be true because the
relativism was requiring objectivism’s truth and its own falsity. And thus relativism
must be regarding itself as governing truth (and therein as being true) even in those
cases where the relativism itself is thus requiring that relativism not be true.
Total relativism cannot be taken by the relativist to be true only at a time or in a
culture or for a person. Relativism, which is the rejection of objective truth, could
only be true, if it were true, objectively. Thus total relativism destroys itself by
making the conditions of its own truth impossible.
Let’s briefly consider what I regard as one of the main inspirations of relativism. An
object like a hand can be viewed and described in many ways--some true and some
false. It can be true at once both that this is a single hand and that it consists of a
multiplicity of parts. A bad argument for idealism or relativism could run as follows:
“A hand has at once the contradictory properties of unity and multiplicity. Which of
these inconsistent properties it has depends purely on how you look at it. Seen one
way it is one hand; seen the other way it is five fingers, a palm and so on. Now
nothing objectively real could be at once one and five (so idealism is right). And
neither of these descriptions, though both are ‘valid’ or true in the eye of the
beholder, could be an objectively true description (so relativism is right).”
But no contradictory properties have really been discovered in that hand. If it is
simply specified that the number is that of the hand and not the fingers, the number is
definitely one. If the specification is of fingers and not hand, the number is five. The
properties of being one hand and of being a multiplicity of parts not only fail to
contradict each other; they mutually support each other. There are, of course, cases
in which inconsistent properties are attributed to the same thing; but then, if the
object is real, at least one of the attributions must be wrong.
Our relativist, then, whether or not he himself is clear about this, requires equally true
contradicting, inconsistent descriptions of the world. But he cannot make use of the
endless variety of consistent aspects of the world that an objectivist is happy to
recognize. The objectivist knows that what one age or culture or person may perceive
as true of the world may not be recognized by another, while what the other
perceives may not be recognized by the first. As long as there is no contradiction
between two such discoveries, there is no problem in the objective truth of both. A
relativist would need them to be true but contradicting of each other."


### Objective Truth

The pursuit of knowledge is, at its core, a pursuit of truth. Zuboff notes that beyond critical rationalism and empiricism, many theories of truth attempt to counter skepticism by lowering the standards for what counts as truth. He writes: 
“These are self-refuting because it turns out that they can make no sense except as claims that these theories themselves are true according to the rejected higher standard. 
But then they make no sense at all since it is essential to them that nothing can be true in the way that they would have to be. One important example of this is pragmatism, the doctrine that beliefs are true if, and only if, they are useful. 
How is this thought to defeat scepticism? Well, according to pragmatism if our usual beliefs in an external world are useful then we can know them to be true despite any sceptical worries about whether such beliefs correspond to a world that is actually there. 
But the pragmatist is failing to notice that he is refuting himself. Pragmatism itself can only be true in a way that it cannot allow. What brings this out is to notice that a pragmatist must hold that if a rejection of pragmatism became useful then that rejection would therein be true—but only because it was useful. Hence the pragmatist would still be regarding pragmatism as the underlying truth about truth quite apart from whether pragmatism was useful. 
In other words, pragmatism cannot see itself as true merely pragmatically.” 
Zuboff argues that any theory rejecting objectivism about truth, where objectivism follows the correspondence theory (i.e., a thought, statement, or representation is true if and only if reality is as it is described), ultimately falls into this trap of self-defeat. 

He writes: 

"Another important example would be a theory that insisted that the truth of what we
say depends on nothing but rules for when it is right to say such a thing in our
‘language game’. This insistence can seem to defeat skepticism because we usually
allow ourselves to say that we know all manner of things to be true that a sceptic is
maintaining we don’t really know.
But it could make no sense for a holder of this theory to regard the truth of this theory
itself as decided by the rules of a language game. For the view must hold that if a
rejection of this language game theory itself were to be incorporated into the rules of
the language game then that rejection would therein be true—but only because it
was in the rules of the language game to say that it was. Hence this theory would still
be regarding itself as the underlying truth about truth quite apart from whether it
accorded with our linguistic practices to say that it was. And thus it cannot be
regarding its truth as fixed by the rules of a language game."

"The undermining effect of this critique has a pretty wide reach. A few more
examples:
Do Wittgensteinians want to say that their own description of language as a game is
itself merely a set of moves in that game? Then if rival descriptions of language
develop within the game, they should be accepted there just like Wittgenstein’s. But
the reason for this equal acceptance would have been dictated by Wittgenstein’s
description of language as a game, which would therefore actually still be holding
sway over its rivals and quite apart from its role and position in a game. So the
language game description must be regarding itself simply as true--as corresponding
to what language really is.
Have the deconstructors of texts deconstructed their own texts recently? Do the
Hegelian idealists think the mind whose thought defines all that is real is real itself
only in its thinking that it is? It seems that every attempt to reject truth as objective
correspondence to reality must run itself into the same brick wall."

And therefore, there must be objective truths.

### Knowledge and Gettier-Cases

What is knowledge? This question is at the heart of epistemology, since philosophers want to know what they can know. The classic account of knowledge involves that it must be justified, true, and believed. 
“Whenever a knower (S) knows some fact (p), several conditions must obtain. A proposition that S doesn’t even believe cannot be, or express, a fact that S knows. Therefore, knowledge requires belief. False propositions cannot be, or express, facts, and so cannot be known. 
Therefore, knowledge requires truth. Finally, S’s being correct in believing that p might merely be a matter of luck. […] Therefore, knowledge requires a third element, one that excludes the aforementioned luck, and so that involves S’s belief being, in some sense, 
justifiably or appropriately held. If we take these three conditions on knowledge to be not merely necessary but also sufficient, then: S knows that p if and only if p is true and S justifiably believes that p. 
According to this account, the three conditions—truth, belief, and justification—are individually necessary and jointly sufficient for knowledge of facts.”

We call this the JTB definition of knowledge. 
Popperians use a completely different definition, but by doing that, talk about something completely different that is more akin to information. Here, I want to adopt the JTB definition and revise it in accordance with Zuboff's justification for empirical thinking. 
The famous problem for the JTB theory is Gettier cases. We have known of Gettier cases since 1963. Edmund Gettier showed “that there are cases of JTB that are not cases of knowledge. JTB, therefore, is not sufficient for knowledge. 
Cases like that—known as Gettier cases—arise because neither the possession of adequate evidence, nor origination in reliable faculties, nor the conjunction of these conditions, is sufficient for ensuring that a belief is not true merely because of luck. 
Consider the well-known case of barn-facades: Henry drives through a rural area in which what appear to be barns are, with the exception of just one, mere barn facades. From the road Henry is driving on, these facades look exactly like real barns. 
Henry happens to be looking at the one and only real barn in the area and believes that there’s a barn over there. So Henry’s belief is true, and furthermore, his visual experience makes it reasonable, from his point of view, to hold that belief. 
Finally, his belief originates in a reliable cognitive process: normal vision of ordinary, recognizable objects in good lighting. Yet Henry’s belief is true in this case merely because of luck: had Henry noticed one of the barn facades instead, his belief would have been false. 
There is, therefore, broad agreement among epistemologists that Henry’s belief does not qualify as knowledge.”

The coincidence between the evidence for one's beliefs (determining the justification) and the actual truth of the matter is what keeps the belief from being knowledge. 
What mattered in both the introduction of justification and Gettier cases was epistemic luck. What knowledge, therefore, actually requires is the absence of epistemic luck: knowledge is justified, non-coincidentally true belief. 
Non-coincidence relates to the justification because something can be true because of reasons given in the justification, or it can be unrelatedly true. The justification is based on falsification and the principle of highest probability. 
But, in Gettier cases, the truth of my belief (e.g., that the barn is real) is not actually related to this justification, but it is true despite the justification for believing it being wrong. 
What is then needed for non-coincidence is that the justification of my belief in X is also coupled to the truth of X. 

In the barn-Gettier-case our justification is that our "visual experience makes it reasonable, from his point of view, to hold that belief". But Henry could not distinguish between fake and real barns from this point of view, meaning that his justification would have been exactly the same for believing another fake barn to be real. This shows a decoupling between justification and truth, since the justification cannot actually justify anything, for it could not have justified the realness of the other fake barn. Believing the barn to be real is in this way only coincidentally true and not actually justifiably true. To be able to really tell a fake barn from a real barn, one would have to examine these barns more closely and find a reliable method of examination, for only then one would be able to differentiate them and could come up with an actual justification coupled to the truth.


### Cartesian Scepticism and A Priori Truths

The pendant to the most fundamental question of ontology ("Why does anything exist?") in epistemology is the question "What is true a priori?". For any truth to be established, there is no way around the need for experience, so in that sense nothing can be established a priori. Rather, what we're asking is if there are actual necessary truths that are 100% certain to be true. 

Descartes, in his First Meditation, sought absolute certainty by systematically doubting everything. He began with sensory deception—optical illusions show that perception can be influenced by expectations. Distant objects, for example, may be misinterpreted: a silhouette could appear as an animal but turn out to be a rock or plant. Hallucinations and mirages further demonstrate that our senses are unreliable. Even verifying perceptions through touch or smell is insufficient since he could be dreaming, making all sensory impressions uncertain.
This leads him to consider whether mathematical truths, grounded in logic, remain certain. However, he imagines an all-powerful demon who could manipulate his reasoning, deceiving him into believing he thinks logically when he does not. 

Zuboff writes: "There is a self-refuting view, momentarily considered and rejected (on grounds other
than these) by Descartes, that the only thing about which we can be certain is that
nothing can be certain. If the obviously self-destructive claim of being certain that
nothing can be certain is dropped, we may still be left with a merely tentative idea
that there can be nothing about which we can be certain. Now, this idea is a more
general version of the idea, in its tentative form, that nothing can be definitely
established in philosophy. And my reply to this more general version is similar to
what I said earlier about that one: We can indeed even now be certain that if nothing
can be certain then we can never be certain of that. So we can be certain of
something."

In the Second Meditation, Descartes reaches his foundational conclusion about what it is that we can indeed be certain of: the one certainty is thought itself. Even if deceived, the act of doubting confirms his existence—Cogito ergo sum ("I think, therefore I am"). Since deception requires a subject to be deceived, something must exist: a thinking entity, a mind.

While Descartes later argues that the external world is also certain, his reasoning depends on the assumption that God exists and would not deceive us. Since this premise is questionable, we set aside his further meditations.

By establishing one's existence as a necessary truth, we also implicitly establish the necessary truth of logic, since my existence stands in opposition to my non-existence. By establishing my existence, I am implying that my non-existence is a contradiction. Logic is therfore even prior to my existence, because I can only establish my existence through logic. Even though a Cartesian demon could deceive me into believing more abstract logical reasoning about mathematics, this simplest kind of tautological logic must be necessarily true, for if it weren't I would not be able to differentiate my thoughts existence from their non-existence.

By this reasoning, we can establish the following a priori necessary truths:

1. The existence of something.
2. The existence of a thinking entity.
3. The existence of logic.

Kant further claimed that space, time, and causality are conditions of perception, not objects of perception. Therefore, they would need to be systems that also exist independently of individual thought. For example, he argued that change can only be perceived if time exists and if there is something stable relative to which change occurs. Consequently, something beyond perception must exist to establish the relationship between change and persistence. But this is an illusion, as Hume points out.
Zuboff explains: "Hume stresses that there can be no a priori discovery of any causal relation, since necessarily a 
cause, which must precede and be contiguous to its effect, must be distinct from its effect. To cause 
itself a thing would, impossibly, have to exist before and next to itself; and therefore only distinct 
things can be in a relation of cause and effect. And this logical distinction between cause and effect 
means that one can always conceive of the cause without the effect and the effect without the 
cause; that one can always therein conceive of the causal relation not holding; that the contrary of 
any causal relation holding is never a contradiction; and that, therefore, a causal relation could 
never be discovered a priori. [...]
Hume recognizes full well that there are causal conceptual truths that can be known a priori. For 
example, though, as we’ve seen, he maintains that it could never be established a priori that every 
(or any) event has a cause, since any event can be conceived of as occurring without a cause, he is 
nevertheless happy to admit that it is necessarily true that every effect has a cause. But this 
conceptual truth, that an effect, to be an effect, must have a cause, is, as Hume points out, only like 
the similar conceptual truth that every husband necessarily has a wife. The terms ‘cause’ and 
‘effect’, like the terms ‘husband’ and ‘wife’, are correlatives, defined as applying to one of these 
things only if that thing happens to be in the relevant relation to the other. Hence it is a relation of 
ideas (ideas of such correlatives) and not a matter of fact that every ‘effect’ has a cause or every 
‘husband’ has a wife. I might add to this that it is true a priori that every ‘poison’ has a power to 
produce harmful effects in some living thing. 
It would be a contradiction, absurd, for a thing to be an ‘effect’ without a cause, a ‘husband’ 
without a wife or a ‘poison’ without a power to harm. But, since it is equally absurd that any effect, 
husband or poison not be distinct from its own cause, wife or harmful effects, it can never be 
known a priori that any one item in these pairs, described neutrally, will exist in combination with 
the other in its pair, that the term that requires the combination will in fact properly apply to it, or to 
anything in reality. Thus it could never be shown a priori that any event is an ‘effect’, that any man 
is a ‘husband’ or that any stuff is a ‘poison’, because it can never be a contradiction that one 
member of the combination that defines the quoted terms exists without the other. Only experience 
can show whether such things exist in the defining combinations. 
Thus we can only discover relations of cause and effect through observing in our experience, in our 
impressions, that what we shall call the ‘cause’ exists in the right sort of combination with what we 
shall call the ‘effect’. And our belief that a cause will be, as required in the idea of a cause, 
consistently followed by the event we call its ‘effect’, is acquired purely through our instinctual 
mechanism of association of ideas. Through custom, through habit, after many repetitions in our 
experience, we come to expect always to occur that combination of kinds of events whose 
complete consistency is required to qualify the events in such a combination to be called the ‘cause’ 
and the ‘effect’ of each other."

But if causal relations cannot be established a priori, space and time cannot be established either, because they rely on causation. We intuitively only think of cause and effect as temporal processes, but they are just as much spatial processes, for a cause-effect process would not be itself without both its temporal and spatial components. 



### An Answer to Scepticism

"How can you know that your present experience doesn’t owe its existence to an artificial stimulation of your brain, disembodied in a vat, or to a merely chance and causeless occurrence of its pattern in the absence of any world 
or even any time outside of it? The classic scepticism regarding the possibility of intellectual justification for judgments about the character of the world beyond the present appearances in a mind, 
including the rest of time outside this moment’s impressions of memory and anticipations, shows the same inspiration as Hume’s scepticism about induction. Based on the conceptual distinctness of a current impression of the world from the world 
and times external to that impression (which may include causes of the impression), the sceptic argues correctly for the impossibility of discovering an a priori necessity for any combination of the impression with any particular character of that external world 
or even with its existence. I maintain that the sceptic, however, in Humean fashion, is overlooking the a priori justification of empirical inference on which our judgments about the external world actually depend. 
This is the rational requirement of an assignment of more or less probability to the occurrence of competing hypotheses based on whether the hypotheses make the occurrence of the evidence, the overall pattern of the impression 
(including apparent memories and apparently previously formed beliefs and anticipations), more or less probable, as is discoverable in our concepts of the hypotheses and the evidence. Consider, for example, the sceptical hypothesis that there simply is no external world. 
This would make it terrifically improbable that my therefore uncontrolled experience, merely by chance, as this would have to have been, had taken on the seemingly disciplined patterns I find it now has. 
Combined with that evidence—such a pattern—such a hypothesis makes up a package inherently improbable to have occurred (like the combination of the hypothesis that a coin was fair with the evidence of its landing consistently heads), 
as we can discover in the very concepts involved. And that which is improbable to have occurred is, indeed, improbable to have occurred. We might call the single a priori principle that thus governs one’s overall empirical thinking the *'highest probability principle'*. 
It requires us always to favour in our beliefs, as most probable, that overall context of our current experience that would, as discovered purely in our concepts of it, have had inherent in it the highest probability of having produced the pattern of our current experience. 
We must do so because it is a necessary truth that the pattern’s being produced in the most probable way is an event that was in itself more probable than the pattern’s being produced in any less probable way. 
Let me just add that sometimes, of course, we must believe that an event which was locally improbable is the most probable to have occurred; but we can only properly do so when this local improbability has been needed in strictly the most probable overall hypothesis. 
Ad hoc sceptical hypotheses, like that of a tricky powerful demon as the sole source of all my experience, must be rejected as extremely improbable because they contain causes that in their general character would have made the evidence improbable 
and can only seem to have made the evidence probable because of arbitrary and therefore inherently improbable specification in the detail of the hypothesis. Such would be the specification of a powerful spirit’s specific interest in producing in me an impression of a world 
that would far more naturally have flowed from the general characters of the sorts of innumerable varied causes that I rightly think to be vastly more probable as sources of the impression. Such ad hoc elaboration in the demon hypothesis is no better at increasing its probability 
than would be such an ad hoc specification regarding a fair coin—that it happens to be one, in its detailed description, that is landing all heads many times—at increasing the probability of that incredible hypothesis. 
In both cases, although the specification is guaranteed conceptually to get us the evidence, the same specification can also be conceptually discovered to be utterly arbitrary and therefore extremely improbable given the general character of the causes within these hypotheses. 
Among the things that you experience in the physical world that you believe in are human bodies that make motions and sounds that you interpret as behaviour and speech, as caused by minds other than your own. 
You interpret the marks on this paper as writing, as a product of a mind. The sceptic about other minds questions in particular the step beyond these bodily motions and sounds or marks to their conceptually distinct causes, the beliefs and desires that are, you believe, 
their inspirations. The answer to this scepticism regarding other minds is, of course, once again the reasoning that gave us the probability of the loaded coin and of the rest of the external world—the inference to the highest probability. 
Consider the hypothesis that there was no mind responsible for such movements and sounds or marks, that, for example, they resulted from random electro-chemical discharges in brains. But this would have been extremely improbable to have produced such patterns, 
ones that would have been made probable by only a mind intending behaviour and speech. (Whether it is probable that the minds are sincere or deceiving in their communication and other behaviour is a further question of the pattern of the evidence.) [...] 
Much attention has been given to how the definition of ‘knowledge’ might allow us to say that we can know things when the sceptic says we can’t. The classic definition of knowledge is ‘justified true belief’. 
What the sceptic questions is whether we are properly justified in our fundamental beliefs. Without that justification, he then might go on to say, we cannot truly be said to have knowledge. 
But this judgment that we don’t have knowledge is secondary for the sceptic to the problem that we don’t have justification. Some philosophers try to avoid that secondary sceptical conclusion about knowledge 
by pointing to our actual attributions of knowledge in which they see the bar of justification as lower than that set by the sceptic. Well, they say, if justification according to that lower standard still gives us ‘knowledge’ according to our usage of the term, 
who are the skeptics to be asking us to change the meaning of the word? I have argued that we simply are justified in the beliefs that we actually do form, on the basis of mathematical probability. 
We have the very justification that the sceptics were requiring, and therefore according to these high standards we have justified true beliefs about the world. They are fundamentally all beliefs about probabilities. 
But the belief that something is extremely probable we can speak of also as a belief that it simply is the case. My knowledge that there is an external world that is at least roughly of the sort that I believe it to be depends for its justification on the knowledge 
that it is most probable that my pattern of experience is caused in a way that would have made this evidence most probable to occur. This justifying knowledge is not in itself knowledge that there is an external world, however. 
And for me to be said to have knowledge of that it must also be true that there is an external world (as distinguished from its being true that its existence is highly probable, which is what justifies the belief)." 
"A hypothesis that would genuinely make our evidence probable must display a principled connection between the inherent character of the hypothesis and the production of that evidence. The hypothesis of a simple curve produced according to a regular principle would do this. 
The hypothesis of a curve that was merely simple through chance, however, would be useless for making any evidence of its simplicity probable. In their descriptions of and prescriptions for empirical reasoning, philosophers have sometimes invoked ‘Ockham’s razor’ 
and the ‘principle of sufficient reason’ as unexplained axiomatic strictures. But we can explain them as aspects of the inference to the highest probability. 
For the most probable account of our experience is that which gives us both nothing less and nothing more than that which would make our experience most probable to occur. 
The hypothesis that there is no external world gives us obviously less than we need to make our evidence probable; and it thus fails to conform to the principle of sufficient reason. But that hypothesis also gives us something more and other than what we need, 
since it claims that the world has a general character that is opposed by our evidence. Such posits against the evidence must be shaved away with Ockham’s razor. Anything in our theory of the world that is not needed to make the evidence probable is a probability risk— 
it would be something whose presence in the world could be nothing but an improbable coincidence. The hypothesis of the tricky powerful demon is perhaps more obvious as a candidate for Ockham’s razor than was the hypothesis that there is no external world, 
because the deceiver hypothesis more obviously asserts the positive existence of something against the evidence—a controller of experience who does not distinctively reveal himself within its pattern. 
But therein the hidden controller also violates the principle of sufficient reason; for despite the ad hoc specifications that would have him with certainty producing deceptive evidence that was like our experience, 
an agent conceived of more generally is extremely improbable to be doing so." 



### The new Problem of Induction

Zuboff's principle of highest probability can also deal with the new problem of induction:

"Nelson Goodman thought he had already dealt with Hume through making the sort of response to him that I labeled as bad. 
He had simply defined induction as rational despite his admission that he could give no justification for it in terms of necessary truth (like the justification I provide). It is in relation to his own solution that Goodman raises his famous ‘new riddle of induction’. 
[...] We have always observed emeralds to be green. It seems then, according to his understanding of induction, that we would be therefore entitled to conclude rationally, by merely generalising this predicate, that all emeralds are green. 
But, he then asks, how can we know just which predicates to generalise? The emeralds we have observed so far could instead be thought of as possessing the strange predicate ‘grue’, which means that those already observed before a certain date are green 
but those that will be observed after that date will be blue. Our observations would be just as consistent with this. (Note that this is logical consistency—there’s not a thought about probability.) 
But if the predicate we are to generalise could just as well be ‘grue’, we could not use induction to predict that newly observed emeralds will be green after the crucial date. [...]" Zuboff imagines two urns. One urn contains a million green beads. 
The other urn contains one green bead and 999,999 blue beads. "A fair coin is tossed to determine which urn is pushed forward for sampling. If the single randomly sampled bead is green, we conclude that it is a million times more probable that the urn from which it came was that 
with all green beads instead of the urn with only one green and the rest blue. And, of course, based on this we would predict that the second bead removed from the urn would also be green and not blue. Let’s try to apply Goodman’s puzzle to this. 
Well, our first observation would have been equally (logically) consistent with the beads in the urn being not green but grue, where in this case ‘grue’ will mean green if drawn from the urn at the time of the first selection but blue if drawn out at a later time. 
And if we couldn’t decide between the beads being green and the beads being grue based on this observation we could have no justification for predicting that the next bead drawn will be green instead of blue. But it is necessarily improbable that the beads in the urn are grue. 
For the beads to be grue, the urn sampled would have to have been that containing only the single green bead—and that one bead would have to have been drawn out first. But it had to be immensely improbable that the first bead drawn from that urn would be the green bead 
and therefore immensely improbable that the beads in that urn would qualify as being grue. Of course a specification that this happened to be a case where that was the bead drawn first could seem to make the drawing out of the green bead probable even if the beads were grue, 
but that specification—of the green bead being the one observed at the time of the first drawing—would be merely ad hoc and inherently improbable given the stipulated general character of that urn. 
‘If we were asked to keep selecting beads from an urn about which we had been told nothing, and, if, having reached in and stirred them up to make sure our sampling was random, we then found that we were drawing out one green bead after another, 
we would be acquiring with each additional observation the same sort of gradually strengthening conviction that we have acquired with our actual observations of emeralds, that the next one observed would also be green. 
For we would know for certain that it was probable that the population of beads in that urn was such as made probable what we were observing. With each additional one it would become less probable that the result of pure green would have occurred 
if the randomly sampled population were not generally green.’ Hypotheses that construed these beads or emeralds to be grue—and therefore the green to be abruptly succeeded by blue after this time or that time or another time— could only be improbable, 
since they would require that our observations be conforming to an improbable order—first missing all the blues and hitting all the greens and then hitting all the blues. 
Sometimes, in later versions of the riddle, ‘grue’ means green before some arbitrary future time (close enough to upset predictions) at which all, including those already seen, will change to blue. 
But [...] stability is required in a hypothesis that would make evidence probable. That a loaded coin was somehow threatening to become fair would have made less probable that it had so far landed heads a thousand times in a row. 
And the threat being future would just be an ad hoc specification."

### The Duhem-Quine Thesis

One big problem, if one could only use falsification to establish fallible, conjectural truth, is that falsification itself cannot deductively falsify hypotheses nor theories. 
If you toss a coin a billion times and it lands on heads every time, it would still not be contradictory to claim that the coin is truly fair. One cannot deductively refute this hypothesis. 
Similarly, no claim about probability can be strictly falsified, since there would never be a logical contradiction in asserting that it remains true.

This issue is particularly problematic for medical studies. 
Any medical treatment observed to lower the probability of an illness could never be definitively said to do so, because there would be no contradiction in conjecturing that it actually has no effect. 
Even non-probabilistic theories cannot be deductively falsified. According to the Duhem-Quine thesis, hypotheses are never tested in isolation but within theoretical frameworks that include auxiliary hypotheses. 
If an observation contradicts a hypothesis H, and H depends on auxiliary hypotheses A, B, C, D, and so on, we cannot determine whether H has been falsified or whether the contradiction arises from A, B, C, D,... or some combination of them. 
To decide which of them is false, we cannot use deductive reasoning.

However, the highest probability principle resolves these issues, as it provides an alternative method for acquiring knowledge about truth (in addition to deductive reasoning). 
When deciding which auxiliary hypothesis to reject, we can discard the least probable ones, because it is probable that improbable hypotheses are probably false.
In the case of probability-dependent hypotheses, it may not be contradictory to conjecture that they remain true. 
But it is contradictory to assert that they are probable when the evidence demonstrates their improbability.
Consequently, the scientific method is vindicated by the principle of highest probability.


### The Problem of Prior Probabilities

Zuboff’s empiricism draws upon Bayesian inference, in which Bayes’ theorem has an objective a priori status. One issue with this theorem is that it requires prior probabilities to update the probability of a hypothesis being true in light of new evidence. 
This has led some philosophers to adopt a subjectivist stance, arguing that ultimate prior probabilities cannot be known objectively and must instead be assigned based on subjective credences. Zuboff contends that these philosophers conflate subjectivity with perspectivity. 
While probability is perspectival, that is a separate issue for another time.
Zuboff defends objective Bayesianism, which is possible due to his solution to the problem of induction. 
Max Albert showed that the issue of a priori priors in Bayesianism is equivalent to Hume’s problem of induction. Thus, if the problem of induction can be solved, then the problem of priors can be solved as well. 
A thought experiment illustrating Zuboff's solution involves two urns, each containing a trillion beads. In one urn (A), all trillion beads are blue, whereas in the other (B), only one of the trillion beads is blue. 
"This second urn has been well stirred so that the single blue bead has nestled into a random location among the other beads. First, let us say, a toss of a fair coin decides which of the two urns is pushed forward for sampling. 
Then a single bead that is randomly drawn from that urn is shown to an observer who has no other basis for judging what it contains and who understands all the circumstances I have described. If the bead that is shown is blue, the observer should infer 
that it is a trillion times more probable that the urn being sampled is the urn with beads that were all of them blue. If it were instead the urn with only one blue bead, then this random drawing of a bead that was blue would have had to be something overwhelmingly improbable. 
But it is overwhelmingly improbable that something overwhelmingly improbable is what has occurred. Hence that hypothesis, combined with this evidence, is in itself overwhelmingly improbable and we must infer that the other hypothesis, of the urn being that with all blue beads, 
is overwhelmingly more probable to be true. We should expect this inference to give us the wrong answer in something like once in every trillion times this is tried. But it is overwhelmingly improbable that this is such a time.“ 
In this case, the prior probabilities were 50%, since the fair coin toss determined which urn was chosen. However, in many real-world situations, prior probabilities are unknown. 
This uncertainty seemingly prevents us from making any probabilistic claims about which urn was selected based on the evidence of drawing a blue bead. For instance, if it were a trillion times more probable that urn B would be selected, 
this could counterbalance the one-in-a-trillion chance of drawing the single blue bead from it. However, according to Zuboff, it was overwhelmingly improbable that the prior probabilities were like this. He writes: 
"Can we not still, as in our earlier uncontroversial case, confidently say that it is overwhelmingly more probable that the urn pushed forward was the one with all blue beads? Because if it had been the other urn, something overwhelmingly improbable must have occurred— 
and it is overwhelmingly improbable that something overwhelmingly improbable occurred."

If the prior probability of selecting urn A (which contains only blue beads) were low, then drawing a blue bead would be improbable regardless of which urn had been selected. 
Consider all possible scenarios:

1) If we suspect that urn B was chosen but believe the prior probability of selecting it was low, then drawing a blue bead would have been improbable. 

2) If we suspect that urn B was chosen but believe the prior probability of selecting it was high, then drawing a blue bead would still have been improbable. 

3) If we suspect that urn A was chosen but believe the prior probability of selecting it was low, then drawing a blue bead would have been improbable. 

4) If we suspect that urn A was chosen and believe the prior probability of selecting it was high, then drawing a blue bead would have been certain.

Drawing a blue bead is improbable in all cases except the last one.
Zuboff concludes: 
"The right view of the mathematics, I think, is that weighing the hypotheses simply in terms of their favorability to the evidence already gives you their objective probabilities when combined with that evidence. 
Then, the objective prior probabilities would, if not directly known, be merely missing further information—information that one should anyway expect to favor the hypothesis most favorable to the evidence. 
Otherwise, a less probable event would have to be what actually occurred, which is itself less probable.
When the difference in how much the hypotheses favor the evidence is small, the unknown prior probabilities might be decisive, making their absence significant. 
However, in cases like the trillion-bead urn scenario, where the difference in evidential favorability is vast, the unknown prior probabilities do not undermine the inference." 


### Perspectival Probability

Lastly, on the topic of epistemology, it is crucial to understand the perspectival nature of probability. This is very easily confused with subjective probability. There's only a small difference, which can be revealed by thinking about tigers. 
The dangerousness of a tiger is not subjective, in the sense that a tiger is not dangerous for a person far away and dangerous for a person in the tigers vicinity. The tiger is dangerous either way, 
because the unconditional attribution of dangerousness to a tiger can be nothing but shorthand for the tiger’s being a danger to those he is close enough to hurt. It's just that the person near the tiger is actually in danger, while the person far away is not. 
The danger of the tiger is therefore perspectival, not subjective. The same is true for probabilities. They only seem to be, because we can evaluate facts with personal credences, but we form these because of our perspective, not our subjectivity.

Here's what Zuboff has to say: 
"It is a mistake to think simply that if a deuce of spades is drawn randomly from a deck something has just happened with the low probability of one in fifty-two. For this event at once has many other descriptions that are associated with differing probabilities: 
‘drawing out a spade’ (one in four), ‘drawing out a deuce’ (one in thirteen), ‘drawing out a card’ (if this is treated as a given, one in one), ‘drawing out a card other than the queen of hearts’ (fifty-one in fifty-two). 
The salience of a description and therein the probability of the event will depend on a person’s relationship to it. If each person in a large group had made a prediction of which card would be drawn, with the predictions pretty much covering the deck, 
this event would have involved a low one in fifty-two probability for someone who had predicted the deuce of spades (as it would have done for the deuce of spades itself if the card were conscious) but a high probability of fifty-one in fifty-two for someone 
who had predicted the queen of hearts. For someone just watching there would be no improbability at all in the result. Improbability depends always on coincidence, the coincidence of two descriptions that have been independently designated— 
such as that in the only prediction that was mine and that of the card that was actually drawn. When we routinely state that the deuce of spades has a one in fifty-two probability of being drawn we are implicitly but crucially imagining the deuce of spades being designated 
independently of its being drawn. If it is merely read off as the card that was drawn its probability of being drawn was one in one. 
If only one person in the group mentioned earlier had made a prediction and had predicted the deuce of spades, the event would have been improbable to the tune of one in fifty-two for the whole group. 
But for an observer of many such groups there would have been no improbability in one of the groups getting it right. That event would remain an improbable coincidence for everyone in the group while at the same time involving no improbability for that observer. 
Similarly, for the winner of a lottery an improbable coincidence has occurred in the winner being him, whereas in this same event there was nothing improbable for an uninvolved observer. 
(Notice, by the way, that all these probability judgments hold objectively within each of these perspectives. They are perspectival but not subjective.) Now, there is an a priori principle of inference that has already featured in my justification of empirical thought 
and will power much of the reasoning in the next three discussions. Here is an expression of the principle: It is most probable that the most probable thing will occur, or, in the past, has occurred. 
And thus, when we have evidence and competing hypotheses, that hypothesis must be judged most probable that would have made most probable the occurrence of the evidence. But whether an event is probable or improbable has been shown to be perspectival. 
Therefore such inferences will also have to be perspectival. Consider again how the event of the winning of a lottery is improbable for the winner but not for an uninvolved observer. 
The circumstances of most lotteries will force the winner simply to swallow happily the indigestible improbability of his winning (after he’s pinched himself to make sure he isn’t dreaming) on account of other parts of his overall evidence having to be even more improbable 
if he was not the winner. (By the way, the lottery having been fixed would not have made his winning more probable unless somehow it would independently have been probable that he rather than another entrant would have been the beneficiary.) 
But in a lottery he’s won where each entrant has been isolated from the others and where there are two rival hypotheses equally available—only his winning or every entrant winning—the improbability of the coincidence from his perspective if only he has won 
will make it by that much more probable that every entrant winning is the case. Informing an uninvolved observer of this winning, however, would be giving him no evidence at all for preferring as more probable that all the entrants have won 
rather than only this one he’s been informed of. Since there would be no improbable coincidence for that uninvolved observer in the hypothesis of only one winner, he can have no reason to infer its improbability." 
Going back to the tiger analogy, Zuboff says about subjective Bayesians: "It is as though an objective reckoning of the dangerousness of tigers had been abandoned because it was thought that the description of a tiger as dangerous simply always applied 
and instead there had been nothing but a charting of the pattern of degrees of fear that tigers aroused. If a tiger is equally dangerous from every perspective, then we can only describe and never explain or justify the differing reactions of someone 
who has just fallen into the tiger’s enclosure and someone at home reading in bed about tigers. Why should it be that one of these and not the other feels tremendous fear along with a need to brace himself for a tiger’s attack? 
Well, all we can say is that these differing sorts of reactions to tigers are typical of people in situations like those. They are subjective reactions that cannot be justified by objective differences in how much danger a tiger is posing, since a tiger is always dangerous."


### The Sleeping Beauty Problem

Zuboff is also the inventor of the Sleeping Beauty Problem. The problem involves Sleeping Beauty participating in an experiment where she is put to sleep after being informed of the rules. A fair coin is flipped to determine her awakenings:

1.If heads, she wakes up once on Monday.
2. If tails, she wakes up twice—on Monday and Tuesday—but her Monday memory is erased.

Upon waking, she is asked: What is the probability that the coin landed on heads? She has no information other than the fact that she is awake.
From an external perspective, one might assume the probability is 50% (the "halfer" position). However, since there are three possible awakenings—one with heads (Monday) and two with tails (Monday and Tuesday)—the "thirder" position argues that, given she is awake, the probability of heads is 1/3. The key insight is that the correct answer depends on perspective: 

An external observer must take the "halfer" position (50%).

Sleeping Beauty herself must take the "thirder" position (1/3).

This distinction becomes clear when experiencing the experiment firsthand: If one takes Sleeping Beauty’s position, one wakes up on Monday with heads one-third of the time and on Monday or Tuesday with tails two-thirds of the time. For Sleeping Beauty, the probability that tails was flipped is therefore higher. 

Zuboff writes about the problem: "I like presenting the problem using much larger numbers than you’ll find in the 
established discussion. 
Imagine an 'awakening game', as we shall call it, in which, at the start, a single 
player is to be put to sleep by a hypnotist. He will then be kept in this hypnotic 
sleep for a trillion days. Except that after he is put to sleep a fair coin will be 
tossed to determine which of two procedures will be followed: Either he will be 
awakened for a short period every one of the trillion days or he will be 
awakened for a short period only once—on only one day randomly chosen 
among the trillion. 
To this we must add that, at the end of any period of awakening, the hypnotist, 
before he puts the player to sleep again, will wipe permanently from his mind 
the memory of having been awakened. Thus, whichever the number of 
awakenings, one or a trillion, each will seem like a first awakening. 
Let us say that a player knows all this but is not told which of the two 
procedures is being followed in his game. Is there any way he can infer whether 
he is being awakened one time or a trillion? 

Imagine that you are the player and you now find yourself awake. It seems you 
can reason as follows: It would be a trillion times less probable that I would on 
this day be awake if only one day was to be chosen for an awakening instead of 
all trillion days. What I do find today—my evidence—that I am now awake—
 would therefore have been immensely improbable with only one awakening in 
the game. But it has to be immensely improbable that something immensely 
improbable is what is occurring. So, given this evidence of my today being 
awake, I must infer that the hypothesis that there are a trillion awakenings is 
immensely more probable than the hypothesis that there is only one. 
On the incredibly rare day of awakening in a one-awakening game, such an 
inference would have misled you into preferring the hypothesis of a trillion 
awakenings, which would then have been false. But if this game had only one 
awakening it would also have been overwhelmingly more probable that today 
you would have been sleeping and therefore in no condition to engage in a 
misleading inference. That the inference is not misleading is thus 
overwhelmingly more probable than that it is."

To solve the Sleeping Beauty paradox, Zuboff distinguishes objective and subjective individuation:

"According to what I shall call the 'objective individuation' of experience, 
however, that an experience actually is this experience rather than another 
depends not only on the feeling within it of being 'this' (without which it would 
not even be an experience) but also on further factors that are purely 
objective—the factors of whose it is and when it occurs. An experience that was 
someone else’s or that was mine but at another time could not have been this 
particular one."

"The second way of thinking about the individuation of experience is relaxed. 
When we are contemplating the single awakening version of the game, despite 
our ability also to view it as governed like the trillion awakenings by objective 
individuation, we are nevertheless strongly inclined to think instead of the player 
as being in one and the same experience of awakening no matter when it might 
be occurring within the trillion days. [...] But this relaxed style of individuation must therein rob the awakening’s 
haecceity of all the evidential force that it had seemed to have within objective 
individuation. For now neither hypothesis would make it any more probable than 
the other that this awakening would be occurring. In effect, the player himself, 
much like an earlier described external observer of the game through 
recordings, would be engaging in a guaranteed, directed observation of 
awakening rather than in a random one, since awakening would be guaranteed 
to be viewed within this same one experience whichever way the game was 
played. And, as we earlier saw regarding the external observer, that must spoil 
the inference."

"The Sleeping Beauty problem is seen from the angle of the player just before 
the start of the game. It seems certain that before the start of the game—before 
the coin determining the number of awakenings has even been tossed—you 
can say nothing about whether in the coming game you’ll be awakened either 
once or a trillion times. Yet you can know then that in your very next episode of 
thought you will be rightly inferring that a trillion awakenings are occurring. 
It is as though you are in a room where you will soon be opening a door into the 
next room. You know already what you will see when you open it, and what you 
will rightly infer on the basis of seeing that; but somehow you cannot yet make 
the inference. Similarly, it seems, before the game you both can and cannot 
make an inference. And this contradiction, because one is led to it down 
seemingly unavoidable paths of reasoning, is a paradox. 
Those I’ve talked to who have had trouble seeing the problem have found they 
could feel the power of the paradox after I explained how the awakening game 
can go piggy back on an uncontroversial probability inference. And examining 
that inference, as we’ll do after the limbering up exercise of the next section, 
can assist us in understanding much better the probability involved. 
So my advice is to persevere beyond this point even if you find that you cannot 
yet see the promised paradox. For I think that you can still have a real hope of 
seeing this strange and beautiful bird. 
And please don’t worry, if you were so inclined, that my discussion of probability 
will become very technical. I am allergic to equations.

[...]

The paradox depends on believing three claims, two of which are not consistent 
with each other. The three claims are: 
1. Before the game starts I could have no way of saying that either number of 
awakenings is more or less probable than the other. (Both objective and 
subjective individuation would seem to agree with that claim.) 
2. Before the game starts I can fully anticipate the particularity of the next 
experience—its being simply 'this one'—just as it will be discovered during the 
game. (Only subjective individuation can agree with that.) 
3. From within any awakening I could use its particularity to infer the immensely 
higher probability that there are a trillion awakenings. (Only objective 
individuation can agree with that.)

It is from these three beliefs put together that we get the contradictory 
impression of before the game not being able to make the inference and yet 
then having hold of all that will be required to make the inference. 
If we stayed consistently within subjective individuation, we would think that the 
player before the game could fully anticipate the particularity of the first 
experience of the game, whether it was the experience of the first of a trillion 
awakenings or else the (probably) much later experience of a sole awakening in 
the game. The current imagining of it would have present in it everything that 
will be required to make it 'this'. But that now fully available particularity could in 
no way serve, either in the game or now, as evidence for an inference regarding 
how many awakenings there would be.  
If we stayed consistently within objective individuation, however, we would think 
that the player before the game could only be imagining what the first 
awakening would feel like. He could not somehow now be grasping the later 
particularity that will have to depend on the objective time of its occurrence. 
Thus he is not at all now in the position he knows that he later will occupy any 
time he awakens in the game, the position of being able to use the fact that this 
experience is occurring to infer the overwhelmingly greater probability of the 
hypothesis that would be making its existence overwhelmingly more probable. 
One might mistakenly take this perspectival nature of the inference within 
objective individuation to be itself the whole problem and then think to have 
solved it merely through recognizing that such an inference can indeed have 
such a nature. That is something that Elga seems to do in his paper. He says, 
'Thus the Sleeping Beauty example provides a new variety of counterexample 
to Bas Van Fraassen’s ‘Reflection Principle’.2 The Reflection Principle is one 
that would have required uniformity between our player’s pre-game and in
game judgments. 
Yet the perspectival nature of the inference, though perhaps it is surprising, is 
not paradoxical. A paradox occurs when natural tendencies of thought produce 
a contradiction. But there is no contradiction in the perspectival character of 
inference. 
Within objective individuation the player before the inference is merely 
anticipating in a general way a future moment of inference whose particular 
haecceity is in no way available to him. So of course he can’t make the 
inference. 
You may remember my providing an image of our paradox: It is as though you 
already knew both what you would see when you opened the door to another 
room and what you would rightly conclude based on seeing it but somehow you 
could not yet arrive at that conclusion. That does represent well enough what 
we would have if we inconsistently shifted views between anticipation and 
inference in the way I have been claiming is the source of the paradox. 
But this image is not parallel with what we would have if we stayed strictly within 
objective individuation regarding both the inference and the anticipation of it. A 
suitable parallel might be rather that only your later presence in some particular 
room among a trillion will be serving you as evidence for an inference that 
therefore you cannot make now. There is no suggestion here of a paradox in 
comparing what you know now with what you will be in a position to know later. 
The Sleeping Beauty problem only arises, then, when we inconsistently take the 
player before the game to have hold of proper evidence without being able to 
make the inference. But only the subjective view allows him to have hold of 
what would only be proper evidence in the objective view."



### The Incoherence of Objective Time

"Imagine that your experience of reading this section is somehow 
instantaneously frozen at its mid-point and then one billion years later 
instantaneously thawed to continue just as it would have done without the 
freezing. Well, the objective billion year gap could not register on you at all. The 
experience would be subjectively the same as it would have been without the 
gap. The content of the later half would seamlessly flow from the earlier. 
Now imagine that, first, your entire experience of reading this section occurs in 
normal fashion. And, then, all is instantaneously frozen and somehow worked 
upon to restore it to precisely the condition it was in just at the beginning of the 
second half of the experience. And, after a billion year gap, it is instantaneously 
thawed and made to continue precisely as it did, in every subjective detail, the 
first time. 

In this case there is a precise objective repetition of the second half of the 
reading of this section. But what will there be subjectively? I would argue that, 
despite the objective repetition, there could be no subjective repetition. In the 
previously considered case of the billion year gap, the first half of the 
experience of reading this section subjectively continued seamlessly into the 
second. Well, that must still happen in this repetition case—but it must happen 
also with the continuation that first occurred, the one without a gap. But surely 
there can be experienced here only one such seamlessly joined continuation. 
So precise objective repetitions cannot register subjectively as multiple 
episodes of experience but only as one, no matter when or how many they may 
be. 

If differences in content were to be introduced between the two continuations of 
the experience of reading this section, then there would indeed be subjectively 
more experience—where the differences came in there would be unconnected 
experiencing of the variant parts. But still no repetition of what was precisely the 
same. 

If this is right, then, startlingly, in a strange choice between there being one 
thousand precisely repeated day-long episodes of torture and only one such 
episode to which, however, a stepping on one’s foot was added, it would be 
wise to choose the thousand repetitions because they would be subjectively 
only the single episode but without a stepping on one’s foot.  
One can easily do thought experiments in which objective temporal order 
comes apart from the subjective order. A science fiction or magic reordering in 
objective time of stages of your experience of reading this section that perfectly 
preserved all the subjective character of the experience in each of those stages 
could result in no subjective difference in your experience of the reading. What 
would naturally have been the first stage would be experienced as first and the 
last as last even if objectively they came the other way around. (Line up 
alphabet blocks in any order you like: That will have no effect at all on the 
alphabetical order of the letters on them.) And all these moments would be 
experienced simply as ‘now’ whenever objectively they had been made to 
occur. 

Objective individuation is confused—it is not only incredibly improbable but 
incoherent. It would have us expect that when an objective time was gone, was 
past, then the experience that was tied to it would be subjectively gone as well. 
But, as we have just seen, what does not yet exist may be subjectively earlier 
than what is past. And all of that is subjectively ‘this time’, ‘now’, whatever the 
objective time of its occurrence."



## Identity

What is personal identity? Why do we have a persistent identity over time? Why are we the same person at 80 as we were at 8, even if we don’t remember anything from when we were 8? One approach is psychological connectedness, what Derek Parfit called “Relation R.” 
Through this, Parfit rejected personal identity altogether and instead focused on survival. According to him, what matters in survival is not strict identity but rather the continuity of psychological connections. 
But a paradox arises if we imagine a case of brain fission, where Person A’s brain is split into two halves, B and C. It seems we cannot continue as both B and C simultaneously, since we can only experience one perspective continuously. 
However, it also seems that we should live on in one of these brain halves; if one half of A’s brain were destroyed, we would assume the person would survive in the remaining half. But it is also not plausible to pick B or C as the continuing half. We reach an impasse. 

This paradox requires us to reconsider identity in a new way. The question we asked was just wrong. The question we should be asking is: “What is the constant that always coincides with the existence of our survival as ourselves?” 
There must be a reason for the persistence of our experience, separate from our memories, that doesn’t discard identity entirely. This reason, the constant tied to the survival of experience, is our specific experience as immediate and ours. 
These aspects of experience are universal and felt by us in every conscious moment, and they are the only properties essential to identity persistence. But the sense of being immediate and mine is an experience shared by any consciousness. 
Therefore, any consciousness must be the same subject—the same identity.

Universalism is the claim that what makes an experience mine is only determined by the immediate first-person character it has, being here (spatially) and now (timely), being "this" (resulting in self-interest). 
It's so simple, it's compatible with any theory of consciousness.

Arnold Zuboff writes:

"Now, an experience being this as opposed to that (other) experience could have nothing to do with any of the particular specifications of either its subjective content or its objective context. 
After all, with the very same specifications in its content and its context it will count as this experience from within it and as that (other) experience from outside it. Being this experience is just what any experience is within itself, from the inside. 
That’s all that makes it this, now (at this time), here (at this place) and mine (belonging to this experiencer). And all experiences have all such being this equally within them. All are this, now, here and mine. 
Think about what you ordinarily would recognize to be “these experiences”, “mine”. What makes them "mine" for you? Is it the detail of their content? If the colours you were seeing had been different, would the experiences have failed to be these, yours? 
Think of all the features of this experience that could be varied while its character of being "mine" remained untouched. If you had fallen asleep and were now in the midst of a wild dream that had little in common with any of the usual content of your experience, 
would that experience have therein failed to be experienced as "mine"? If you had eaten different particular items of food over the past years (as you might so easily have done), so that all the particular atoms in the structure of the body were different in numerical identity 
from those in your body now, would the experience have failed to have that character of being "mine"? Must you take care with the particularity of the food that you eat because it is determining the identity of an experiencer, of the subject of self-interest? 
If the experience were had in a different location, if it were at a different time, would the experience not still have had that same character within it of being this and being mine? 
What makes an experience yours is none of the specification of its content or of the particularity or other properties of its possessor. All that is required for an experience to be yours, to be “mine”, is that it be immediate in its character 
as its character is experienced within it, that it be first person. My pains are pains that are not remote like those that belong to another. My pains are those that are immediate. They have internality. They are experienced in a first person way. 
They are subjectively at the centre of the world, here in me. But all real pains must be had with this quality of immediacy that makes them “mine”."

Many people, when they encounter the argument of Universalism, ask themselves: if no pattern of experience is special and I am everyone, why doesn’t my consciousness jump between different beings? Why do I perceive time linearly and continuously? 
However, this argument has the logic backwards. For this inverted logic to even make sense, you would first need to stipulate the existence of a soul that could jump from one being to another. 
You would have to believe that your thoughts are continuously yours because your consciousness belongs only to you. But in reality, your consciousness is yours precisely because your thoughts can physically only occur in the being you perceive as yourself. 
Even if Universalism is true, your experience would remain the same as it is now.

Consider, for instance, the idea that your consciousness could jump through time. For you to perceive and recognize this jump, you would need to experience it. 
If you jumped backward in time, you could only recognize it if you retained your memories from the future. If you jumped forward in time, to recognize it, you would need to forget everything that happened in between. 
Thus, because our thoughts do not jump, we cannot jump through time either.

A similar principle applies to the idea of consciousness jumping between beings. For you to recognize this jump, you would need to retain your previous experiences before the jump in your memory. 
But this would require telepathy, which is not what we are discussing. The point is that even if your consciousness were constantly jumping through time or beings, or if all experiences were happening simultaneously and eternally, your experience would still be the same as it is. 


### Some Thought Experiments about Identity

Universalism becomes evident when we consider the concept of identity clearly. It seems to resemble the ideas of the number zero or imaginary numbers in this way. While these concepts are now simple to grasp, they were overlooked or unrecognized for thousands of years. 
The conventional view of personal identity does not withstand scrutiny.
Let's consider the "Ship of Theseus" thought experiment applied to two human brains A and B. Imagine we gradually exchange one functional brain region at a time, transplanting them from A to B and vice versa. 
Let's assume an ideal exchange mechanism that causes no damage to the neurons. According to the conventional view of personal identity, there would have to be a critical moment when identity suddenly switches bodies. 
By the end, brain A would reside in body B, and brain B in body A, implying that person A and B had switched bodies. This view necessitates an abrupt transition since identity, as conventionally understood, does not have levels, it is binary: one is either "there" or "not there." 
However, neuroscience has revealed that there is no single functional unit in the brain that constitutes identity. Instead, identity arises from the interplay of many regions working together to produce consciousness. 
Consequently, there cannot be a single exchange of a brain region that definitively transfers identity from A to B, nor can there be a gradual transfer. If no single or gradual exchange of personal identity is possible, then identity "transfer" is just impossible. 
But, since at the end, A must be in body B and vice versa, they had to have had the same identity all along.

Derek Parfit proposed a variation of this thought experiment, where brains are switched atom by atom. 
While this version introduces additional problems, since atoms are constantly in motion and interact, making the exchange process blurry. Parfit argued that there would need to be a specific atom in this process that triggers an abrupt identity switch. 
He also found this idea absurd.
But instead of embracing universalism, Parfit concluded that identity plays no role in survival and that personal identity does not truly exist. 
Parfit's conclusion is obviously flawed, as every person can verify subjectively that they remain themselves from moment to moment.
Parfit’s argument for psychological continuity as the basis for survival is too complex for the binary nature of personal identity. 
Arnold Zuboff writes: "the ordinary view tries to make your identity depend as well on certain complex conditions of token, type, and content integration. Unfortunately, these admit of division, differences of degree, and indeterminacy. 
There is no way to reconcile this complexity with the simplicity of the self." 
Another way to illustrate that identity doesn't depend on anything else other than the immediacy of experience is by thinking about amnesia. If one forgets a memory, this doesn't change the fact that one is experiencing one's thoughts. Suppose, one forgets more and more memories. 
The change in content leaves the identity to which the content is happening completely unchanged. Even if one reaches complete amnesia, the experience of amnesia is still one's own. 
Now, we can fill the brain again with different memories, resulting in a new personality, while still being experienced by the same identity.

For anyone who is still in doubt and wishes to claim, like Parfit did, that somewhere along the way the subject of experience must have changed, consider this:

The person with amnesia begins a new life, forming new memories and developing a new personality, with no recollection of her past. From your perspective, she must have a new identity. 
But suppose her old memories eventually resurface, bringing her former personality with them. Does this mean two subjects now exist within one experiencer? No—there is only one experience, and therefore only one subject. 
Rather than two subjects sharing a single brain, the new personality comes to recognize that she has always been the same subject as the old personality. 
And ultimately this scenario isn't different from injecting one person's memories and personality into another person's brain, which must lead us to conclude that everyone is actually the same subject/identity. 

### Teleportation

Derek Parfit's famous thought experiment of the teletransporter imagines a person being teleported from Earth to Mars. For our discussion, let’s assume macroscopic teleportation is physically possible. Of interest to us is only what happens to our experience. Will we survive? 
The common view holds that teleportation would end our experience due to a discontinuity. But discontinuity doesn't end our experience in other cases, like sleep, coma, memory loss, near-death experiences or brain surgery. 
Consider a teleportation process that disassembles a person in an attosecond and reconstructs them in the same spot an attosecond later. No one, including the person, would notice. They would just go about their day as if nothing happened and they would certainly not die. 
Destroying a piece of paper (which contains certain written information) after copying it does not erase the information it carried. Likewise, we are not defined by our material atoms (which are completely replaced over time) but by the information structured within them. 
So, teleportation cannot destroy us.

Parfit, however, concluded otherwise. He emphasized psychological connectedness and considered a case where teleportation malfunctions: person A remains on Earth while an identical replica, B, appears on Mars. 
If A’s body is fatally damaged in the scanning process while B remains unharmed, who survives? Parfit argued that identity itself is a fiction and that what matters is psychological continuity. But he admitted that A and B could be two streams of the same consciousness: 
"I can imagine myself having a divided mind. Since this is so, I need not assume that my Replica on Mars is someone else. Here on Earth, I am not aware of what my Replica on Mars is now thinking. [...] 
I can believe that I do now have another stream of consciousness, of which, in this stream, I am now unaware. And, if it helps, I can take this view about my Replica. I can say that I now have two streams of consciousness, one here on Earth, and another on Mars." 
At t₀ (before teleportation), there is one person, A. At t₁, there are two persons, B (on Earth) and C (on Mars), who are psychologically and physiologically similar enough to A as to consider themselves in every way that matters, to be A. This raises four possibilities: 

1) A survives in B, but not C.

2) A survives in C, but not B.

3) A survives in B and C.

4) A doesn't survive at all. 

Parfit chose (4), because A doesn't even exist as an identity, since identity doesn't matter for survival in his view. Kolak writes: "according to Parfit, our borders are always indeterminate and personal identity is indeterminate (relative) and (but) whether personal identity extends across some particular border is not what matters primarily in survival." Every moment of experience is only there for itself and in this sense "I" do not extend over multiple moments but only exist in every moment separately. 
But how can this view possibly be right? Clearly, there is something persisting through all my experiences and I would survive in at least one of the replicas. There is also a probability argument to be made, which I'm not going to elaborate here. 
But, as we've seen, it is far more probable from your perspective for you to exist if universalism is true, than if the usual view was true. On Parfit's view it would be even more unlikely for you to exist, than on the usual view. 

Zuboff wrote for example: "I think that something like a Buddhistic view can seem less obviously vulnerable to the incoherence of the usual view in such cases as brain bisection. 
I believe that is the main reason for the great popularity within philosophy of Derek Parfit’s view, which is more or less Buddhistic."
"The usual view imposes insulating boundaries on who I am that confine me within the life of a single human being. 
The rival view we shall now consider confines me to a much smaller momentary existence that does not extend beyond the present moment of a human being. And it thus makes my far more pinpointed existence even more improbable than that I have on the usual view. 
In a Buddhistic view any psychological process would be forging on through a succession of non-continuous experiencers. At any point in this process there would be an experiencer with its momentary pains and pleasures, but it could have no non-illusory self-interest in any further accumulation of experience as this would not belong to it—since not that subject but only other momentary subjects would exist in any further experience. The experience would belong to no continuing subject. 
Neither self-interest nor other-interest (interest, that is, for the self-interests of others) would be appropriate. As I’ve mentioned, the improbability for itself of existing of any of the momentary subjects would make such a hypothesis statistically untenable just as it did the usual view of the person. For in every moment of the ongoing mental process an inference would be supported that from this momentary perspective the existence of this momentary subject would be overwhelmingly improbable 
by contrast with the existence of the universal subject in universalism."
"Consider all the detail that makes up [the] package of contents [in our visual experience]. Let’s 
concentrate on the detail in the visual field. In theory a ten by ten grid could be 
drawn that divided the visual content of a moment of that vision into a hundred 
squares. And we could easily specify some ten ways in which a colour or shape 
in each square of visual content might have been different. The number of 
possible variations in visual content thus highlighted would amount to ten to the 
hundredth power (a googol), which is immensely greater than the number of 
particles in the entire visible universe. 
If the player’s experience would only have existed should all of its package of 
content have been exactly as he finds it to be, the odds against the existence of 
this, the experience in which his inference is occurring, are immense beyond 
belief. If, however, subjective individuation is true and therefore it does not 
matter to the existence of 'this' experience what content it has—because all 
content must be equally 'this' merely due to its quality of immediacy—then there 
is no improbability in the existence of either this experience or the content that 
has been relieved of that responsibility for individuating it. So the haecceity of 
experience is independent of the content of the experience as well as of the 
objective time of the experience."


David Lewis argued for answer (1) and (2), proposing that A already contained two subjects who were later separated. But this view is absurd, because the mere possibility of replicating a person infinitely, necessitates an infinite amount of subjects residing in every person. 
Only universalism gives a satisfying answer to teleportation. The only thing needed for me to persist through time is for my experiences to be immediate in first-person style, being now, here and this. This style of experience is what makes it mine. 
Therefore, as long as an experience has that style of immediacy to it, it will be mine. A, B and C all experience their experiences in this way and are for that reason the same experiencer. Of course there is nothing that decides that either (1) or (2) is right. 
If one thinks clearly about what identity truly is, (3) must be correct. A survives in B AND C and there is no paradox, because identity has nothing to do with random psychological attributes, each of us may have. Zuboff writes: 
"You don't discover which [conscious being] you are by checking an objective description—that you are the one with a certain name or a certain origin. 
For before you can consider any such objective facts about yourself, you much more simply know that you are the conscious being whose experience is immediate, first-person in its style. You are the one that seems to be at the centre. 
But now consider that every conscious being has experience that fully shares that character you thought belonged only to the one that was you—the immediate, first-person character of experience that supposedly distinguishes you from all the others. 
All consciousness in every conscious thing is equally immediate and first-person.
[...] it falsely seems in each that it is the only one that is me because the specific content of experience in each one is cut off from the specific content in all the others." 



### Qualia Universals

The thought experiments behind Universalism demonstrate that universals are ontologically real. First, let's assume there exists some abstract information (which we could call a model, a system, a framework, a representation, or a pattern) in our mind. 
This abstract information is what we experience as qualia, from within our conscious mind. Now, let's further assume that two identical brains are created and given the same experience. Brain A is placed to our right, and Brain B to our left. 
If we were to swap their positions, we could safely assume that person A is now on our left and person B on our right.

Now, let's say we can instantaneously freeze both brains, and while they are frozen, we swap one fourth of their brains with each other. 
After this surgery, we unfreeze them. Neither A nor B notices that anything has happened, and A still experiences the same abstract information as B. At this point, we can safely assume that A is still on the right, and B is still on the left. 
We could repeat this procedure for the remaining 3 quarters of their brains, but in none of these steps would person A be swapped with B. The end result is that person A remains on our right and person B on our left, even though physically, we have entirely swapped their brains. 
This apparent contradiction is resolved by recognizing that A and B must actually be the same person. Though they are physically distinct brains, they are mentally identical as long as they experience the same abstract information. 
This tells us that abstract information is, in fact, a universal. It can exist only once, no matter how many times it is physically instantiated. Why is that exactly? Because abstract information is essentially cut off from physical reality. 
This was recognized by Descartes and Peirce called it the Phaneron. If it can observe anything, abstract information can only ever observe itself, as is the case for neural networks. Since abstract information can only ever observe itself, it is self-contained. 
The reason why the two physical brains are particulars is due to their relationship to reality, their context. They differ in position, material/atoms, reference frame, etc. If we were to eliminate all these differences, there would be just one physical brain. 
This is essentially what happens from the inside perspective of abstract information. Since abstract information is self-contained, it is free from context, which eliminates any differences from the inside perspective between the physical instantiations of the information. 
This is not a controversial claim. We could conduct a similar experiment with books. Even if we were to swap the pages of identical books, the abstract information (the story) within them would remain unchanged. Thus, if abstract information exists, it is a universal. 
The question then becomes: does abstract information ontologically exist? Conceptualists might argue, for example, that the story in these books is merely a concept, a figment of our imagination—it exists only for our pattern detectors, but not in objective reality. 
This is a fair point. However, what is undeniably real, and something we are uniquely equipped to recognize, are the qualia we experience. From this, we can infer that at least some abstract information does indeed exist, otherwise qualia would not exist. 
Therefore, some universals must also exist ontologically. If stories are universals remains open, but at least our qualia are universals according to this argument. 



### Eternalism

The argument for eternalism is also an argument favoring universalism. Eternalism posits that any existence in time is equally real. No moment of experience is special. All experiences are equally real eternally and simultaneously. 
And just like my experience is separated by time, it is also separated by space. Just like time separates your past self from your current self, space separates your experience in one being from your experience in another being. 
Just as you were yourself yesterday but can no longer experience the immediacy of that moment, you are yourself in every conscious being but cannot experience it due to spatial separation. This view is of course compatible with Occam's Razor. 
If we wanted to doubt eternalism, we would need to explain why a certain moment is more special than any other moment, yet there is no universal time that would make a moment special. Every moment still exists if you take the right reference frame in special relativity. 
Likewise, if we were to doubt universalism, we would need to explain why a certain perspective (our perspective) is more special than any other perspective. We would need to invent something akin to a soul, which glues my experience to my physical body. 
But this means, postulating something we haven't observed and maybe even in principle cannot observe. It also violates ontological parsimony and therefore Occam's Razor. 


### Occam's Razor

Occam's razor can derive its effectiveness from the uselessness of introducing a claim into one's hypotheses that doesn't contribute to explain one's evidence. It is not obvious that believing in the separateness of subjects violates Occam's razor, but it does. 
The evidence that we experience only our own perspective within a single human being can be fully explained by the observation that experiences are isolated from one another. 
The hypothesis of actually being a separate subject is indeed a claim on top of this, which has no explanatory value. Our inability to experience others' perspectives is already accounted for by the fact that the content of our experiences are inherently isolated from each other. 
The notion that identity itself is also separate is an extra claim that does not contribute to explaining the evidence. 
For this reason, if we follow Occam's razor, which is only rational, we must conclude Universalism to be the more likely theory of personal identity compared to the usual view, that Daniel Kolak calls Closed Individualism. 
To make this even clearer, let's consider a thought experiment where a scientist invents a time machine. (The feasibility of such a machine is not important for this argument—it simply serves to illustrate the point) The scientist travels back in time to observe her younger self. 
In this past timeline, she now exists twice, occupying two different locations simultaneously.

This scenario highlights the possibility of the same subject existing in multiple locations at once, even without any physical communication between its instantiations. 
Despite the separation, the scientist cannot access her younger self’s thoughts. Yet, they are undeniably the same subject. 
It is completely sufficient to assume the separateness of experience to explain why one brain cannot access the thoughts of another. However, this separateness of experience does not necessarily imply that the underlying subjecthood is different.

### The Set Theory Argument

Every mental process can be described in two ways: physically, and subjectively as phenomenal experience. Consider merging two brains to form a unified consciousness with no distinct perspectives. Subjectively, the consciousnesses of two individuals merge into one. 
Using set theory, we can represent the attributes of person A's subjective experience as set A and those of person B as set B. Each set includes a component we call 'subject', which corresponds to the immediacy of experience as here, now, and mine in the first-person style. 
After merging, there is only one subject attributed to the combined set A+B, as there is only a single perspective of experience being mine in the merged state. This is possible only if the subject attributed to A and B was the same in both sets before the merging took place. 
If functionalism is correct, subjective experiences arise from the functional organization of physical systems, and there is a surjective relationship between physical and subjective descriptions. 
This means that every subjective experience corresponds to at least one physical functional organization but could arise from multiple realizations of that organization. Wherever a certain function is instantiated, there is a corresponding subjective experience brought about. 
If no other factor is involved, the same function at different times or places will result in the same subjective experience, making it a universal. 
If the merging of two functional systems can result in a single unified subject, this would mean that the individuality of subjective experience arises solely from physical separation, not from differences in the underlying 'subject.' 
Thus, under functionalism, every subject is ultimately the same. The possibility of merging brains and experiencing the sameness of subjecthood will hopefully lead people to adopt this view in the distant future. 

### The Probability Argument

Another argument for universalism is the probabilistic argument. In the conventional view, your existence depends on your physical body. If this body had not been conceived, you would not exist. 
If a different sperm cell had fertilized the egg at your conception, a different person would exist in your place. According to this view, your existence hinges on winning a kind of sperm cell lottery, with odds of less than 1 in 200 million. 
When we also consider the probabilities of your parents’ existence, as well as your ancestors’, the odds of you being here become astronomically small.

Arnold Zuboff refers to this as the hard game. 
The hard game makes explaining the evidence of your existence from your own perspective extraordinarily difficult. From another person’s perspective, your existence is not particularly improbable, since someone had to be born to your parents. 
But from your perspective, the probability of your existence appears vanishingly small.

To clarify this point, imagine a hotel where 200 million people sleep unconsciously. In the easy game, everyone wakes up after some time. 
In the hard game, only one person is randomly chosen to wake up. Now imagine you wake up and must decide: was the easy game or the hard game being played? From your perspective, it is overwhelmingly more likely that the easy game was played, since the chance of waking up in the hard game was just 1 in 200 million. To a stranger, who is just being told one persons' name that woke up either in the easy or hard game, observing from the outside, there’s no improbability at all—someone had to wake up either way. 
The easy game in the sperm cell lottery corresponds to the hypothesis that universalism is true, since you would, if universalism were true, wake up in every awakened sleeper in our gigantic hotel. If universalism is true, you are every subject in existence, so whoever would have been awakened, would have been you.
In that case, there is no improbability to your existence in this particular body, because you would have existed no matter which sperm cell "won" the lottery. Thus, universalism offers a far more probable explanation for the evidence of your existence than the conventional view. 
Initially, I thought this argument might have a flaw: determinism. If everything is deterministic, then there is no true randomness in your sperm cell winning the race; its success would always have been inevitable, with a probability of 100%. 
I am still not sure if this really is a flaw. To see why, let’s revisit the hotel analogy. In the hard game, the person who wakes up is randomly chosen. 
If the "randomness" of this choosing is not truly random but deterministic (just unpredictable), the chosen person had a 100% probability of being awakened in both the easy and the hard game. 
This would not matter to the chosen person, though, because it does not change the conclusion of the probability argument, if the person can't predict the outcome. If you would play this game, 200 million people would be right to infer that the easy game was being played and only 1 person wold be right to infer that the hard game was played. Even in a deterministic universe, one should not bet on the usual view, if one can't predict who will be chosen. 

To make this point clear, imagine a lottery game without any randomness. The winner has already been determined before the game even started. This ultimate determinacy doesn't take away from the coincidence it will be for the player who actually wins. Zuboff writes: "By
the way, the lottery having been fixed would not have made [the winners] winning more
probable unless somehow it would independently have been probable that he rather
than another entrant would have been the beneficiary." 
Unless, the winner would have a high chance of winning, every time the lottery were to be repeated, there is a coincidence in his winning, even if the lottery is fixed and the universe is deterministic. 

Zuboff explains: "This is a misunderstanding of what we are thinking about when we reckon 
probability. The question is almost always not one of the sort of micro level 
causation a determinist has in mind but rather of the matching of descriptions 
on a much higher level of abstraction. The description of an urn as having all 
blue beads and the description of randomly drawing a blue bead from an urn 
are certain to agree, whatever the micro causation may be regarding the 
particular movements of the hand drawing the bead, the precise placement of 
the urn, etc. But the description of an urn as having only one blue bead out of a 
trillion will only one time in a trillion agree with that of an urn from which a blue 
bead was randomly drawn. 
Let's for the sake of argument be among the strictest determinists regarding the 
tossing of a coin. Consider a case in which there are two coins involved, a fair 
coin and a double headed coin. The probability of getting a thousand 
consecutive heads with the fair coin is one in two to the thousandth power—
 terrifically low. The probability of getting a thousand consecutive heads with the 
double headed coin is simply one—certainty. Thus we predict that it is incredibly 
improbable that with the fair coin we shall get this result (or, if the consecutive 
heads have already occurred, that it was with such a coin that we have got this 
result). Why? It is because in the case of a coin that satisfies the description of 
being fair the proportion of ways that it could follow its deterministic trajectories 
on the micro level and yet land only heads in a thousand consecutive tosses is 
two to the thousandth power minus one times smaller than the proportion of 
deterministic ways a coin could follow trajectories that would have it always 
landing heads if it instead satisfied the description of being double headed. In 
either case our determinism would have us believing that whatever happened in 
a description at the ultimate micro level of causation had probability one of 
occurring, but that would not in the least prevent us from assigning appropriate 
probabilities at the higher level of description—that at which the coin was either 
fair or double headed. 
[...]
Probability requires consciousness. Ten heads in ten consecutive flips of a 
fair coin can be considered improbable. But that this result has occurred 
somewhere within the field of all coin flips seems not improbable at all. And that 
is because probabilities add together. The small probability of ten consecutive 
heads when considered locally becomes instead the enormous probability of 
such an occurrence within an enormously widened area of consideration; and 
this extreme transformation of probabilities depends on nothing but a shift in 
perspective. Probability always thus depends on assuming a perspective, on 
circumscribing an area of consideration, and hence on consciousness, which is 
the only thing capable of perspective and consideration.


### Quantum Observer Theory

It can be argued that what has driven the biggest revolutions in science is refining the definition of what observers are.
In relativity, observers can know the order of time-like separated events but cannot know the order of space-like separated events. 
Similarly, in quantum mechanics, the observer plays a special role in collapsing the wavefunction, becoming entangled with the system. Hence, it seems plausible that new insights into the nature of observers might further deepen our understanding of reality. 

In Wolfram's theory of hypergraph dynamics, Jonathan Gorard postulates that "An observer is any persistent structure within a multiway system that perceives a single, definitive evolution history." 
Within this framework, observers branch and converge within parts of the multiway system. For an observer to perceive a single evolution history, its internal representation of the world must be causal invariant. 
Gorard showed that a causal invariant observer must exploit a minimal set of completion rules, which would cause reality to appear causally invariant from the observer’s perspective. In this way, the observer’s properties shape reality to present itself with its quantum nature. 
Now, functionalism and universalism also contribute to our understanding of observers. Universalism is especially relevant to Gorard’s claim, as an observer branching within the multiway system cannot be understood through conventional subjective identity theory. 
Instead, we must assume that the subject is universal, since, when enough equivalence classes exist and our subjective experience is the same across different branches—despite differing physical realities in each—we can observe quantum phenomena. 
The subject is a universal, equivalent across all branches, and thus we are the same subject in all branches. As long as there is no difference in subjective experience between these branches, we experience all of them simultaneously (but only if universalism is true). 
However, when subjective experience differs between branches, the observer's function changes, and the observer can no longer recognize the other branches. This is because the observer is the function itself, as functionalism suggests. 
Imagine a branching observer evolving into two functions: in one branch, the observer forms a representation of blue, and in the other, a representation of gold in its mind. 
While both are the same universal subject, the observer in the first universe cannot recognize the observer in the second universe. This is because there is no corresponding function that would allow the observer to perceive both experiences simultaneously. 

The observer in the first universe, if it recognized blue and gold at the same time, would need to be able to tell people about this experience of gold-blue-ness in both universes. 
But the observer is not able to do this, because there is no corresponding function and so the observer is cut-off from the other experiences it has, even though these experiences belong to the same subject. 
Ultimately, this argument claims that universalism needs to be true, if the observer's properties cause reality to be causal invariant in the way Wolfram's physics project suggests. 


### Falsifying Universalism

It seems impossible to falsify universalism objectively, which would render the claim unscientific. However, it seems to me that universalism could be falsified subjectively. For example, one could anesthetize a brain or invent a switch to toggle the corpus callosum on and off. 
On the usual view, if we were to anesthetize a brain and then connect it to another brain, similar to how the brains of craniopagus twins can be connected, there would still be two subjective perspectives. 
The two identities would not merge subjectively, even if they were merged physically. For person A (Alice), it might feel as though she could now access new memories, but these memories would not be perceived as her own. Person B (Bob) would feel exactly the same. 
Under universalism, however, these individuals would now recognize what they couldn’t recognize before the merger: that they were the same subject all along. In the merged state, the subject would acknowledge that it had lived both lives, A and B, up until this point. 
There would no longer be separate perspectives for A and B because there never were separate subjects in the first place. If consciousness is based on a consensus mechanism and if there are still two subjects, this would mean that consensus between two brains is impossible. 
But as of now, there doesn't seem to be anything preventing consensus.

William Hirstein has already written about a mindmelding proposal to unify the consciousnesses of two people. This will not remain a purely theoretical topic.

Similarly, if we could switch off the corpus callosum, the two halves of the brain would suddenly experience the world independently. 
Using brain-machine interfaces, we could even create separate virtual realities for each hemisphere. The left brain might experience playing a game, while the right hemisphere might study for a test. After these experiences, the hemispheres can be reunited. 
Would this lead to two permanently separate subjects residing in the same brain, or would they simply reintegrate into the original subject, who now feels as though it experienced both gaming and studying? Isn't the latter far more likely, since it is already the status-quo? 



### Mindmelding

Hirstein's proposal to achieve mindmelding revolves around a device connecting person A's prefrontal lobes to person B's posterior cortex using white matter fibers, which would connect person A's sense of self to person B's conscious thoughts and sensations:

"Nature has provided the perfect structure to allow us to perform
mindmelding thought experiments. The temporal and parietal lobes are extensively
interconnected with the executive processes in the prefrontal lobes by several different
white matter fiber tracts, called long association fibers (Schmahmann and Pandya, 2006 ).
Filley ( 2001 , p.23) observes that these fibers “share the interesting feature that they
all have one terminus in the frontal lobe. No other lobe of the brain enjoys such rich
connectivity. The white matter is therefore structurally organized to facilitate frontal lobe
interaction with all other regions of the cerebrum.” 

Hirstein believes these fibers to play an important role in conscious qualia experience:

"One piece of evidence that these fiber bundles have important connections to consciousness is that damage to them has immediate effects on the patient’s consciousness.
The fasciculi are abnormal in schizophrenics who experience auditory hallucinations
(Friston, 1998 ). According to Kier et al. ( 2004 ), the uncinate fasciculus and the inferior
occipitofrontal fasciculus play a role in producing hallucinations when their connected
areas are damaged. In their 2004 article entitled, “Pathways that make voices,” Hubl and
her colleagues found that schizophrenics who experienced auditory hallucinations had
marked differences in their association fibers (in the arcuate fasciculus, connecting the
temporal and prefrontal lobes), which led to “disrupted frontotemporal processing”
(2004, p.666). Friston ( 1998 , p.122) similarly notes that the schizophrenic brain shows an
absence of positive correlations between prefrontal and superior temporal regions that he
traces to abnormal glutamate transmission, which “is the mainstay for the excitatory
cortico-cortical interactions that integrate different brain areas.”
 Recall that Crick and Koch used the analogy that the front of the brain is looking at the
back of the brain. Apparently the fasciculi are the means by which they do this, as well as
the means through which they influence processing in the temporal and parietal lobes.
Crick and Koch speculate that what they call the neural correlate of consciousness is
“expressed by only a small set of neurons, in particular those that project from the back
of the cortex to those parts of the front of the cortex that are not purely motor and that
receive feedback from there” (2003, p.124). More specifically, Koch ( 2004 ) says that some
projection neurons from inferior temporal areas to the principal sulcus in the dorsolateral prefrontal cortex may form part of neural correlate of (subject) consciousness."

This would, according to Hirstein, allow one person to experience the other person's consciousness:

"There are certain patterns of activity moving along
the fiber bundles. We attach the fiber bundle from the brain of the bearer of the qualia (below)
to the right places in the brain of the sharer (above). The owner of the brain on top can
experience the conscious representations of the owner of the brain on the bottom. What the
person on top experiences cannot be his own conscious perceptual representations, which
reside in his temporal and parietal lobes, since the connections to those are broken."

Hirstein postulates that Mindmelding can answer a variety of long-standing philosophcial questions and is of great practical use. Most of these have already been answered here through pure reasoning. For example, Hirstein writes:

1. On philosophical zombies: "If we put the zombie into a mindmelding experiment, the person joined to him would not experience anything."
2. On inverted qualia: "If two people really could have inverted spectra while we could find no differences in their
brains to account for this, that would be an explanatory gap. In cases in which two people
do experience different colors, I have argued that mindmelding could allow them to
know this."
3. On merging selfs: The type of mindmelding I am mainly speaking about is partitioned mindmelding.
Partitioned mindmelding occurs when each A and B experience the conscious state of B
as if it were their own state. But A and B maintain their senses of self, by maintaining
more or less normal operation of their prefrontal lobes. A much more radical type of
experiment (that we may or may not want to call mindmelding) would involve also
attempting (somehow) to merge A’s and B’s sense of self, i.e., attempting to merge their
entire minds. As it is described on the television show Star Trek , the Vulcan mindmeld
sounds like a variety of this sort of complete mindmelding. Open, unpartitioned, mindmelding would be a new experience for both participants. It would somehow involve two
selves operating at the same time. It is not at all clear what this would be like. No wonder
Mr Spock finds it so draining. In this case, there is perhaps a type of normal state that
provides a clue as to what such a mindmelding experience might be like. One possibility
would be that this type of experiment would produce something like mutual knowledge,
or common knowledge, which occurs when one is knowingly experiencing something
together with someone. A first question here is, how does mindmelding compare to joint
attention to an external object? Mindmelding where the bearer is looking at a tree might
be similar to joint attention where two people are standing together looking at a tree." If universalism is true, this experience will be entirely different from what Hirstein has described here.
4. One the mind-body and binding problem: "the heart of the idea of privacy: the idea that conscious mental states are unbreakable metaphysical atoms, intrinsically
containing a subject who is aware of them. If mindmelding is possible this conception
is mistaken, and the mind can be divided into parts: a conscious state, and a subject. The
mind–body problem itself can likewise be divided into two parts, which can be treated
separately. The first part is the problem of consciousness. How does the brain produce
unified conscious states containing bound qualia from all those different sources? The
answer to this, [Hirstein contends], is that it uses carefully controlled synchronized
oscillations to link all and only thalamocortical components of a given conscious state. The second part of the
mind–body problem is the problem of the self, or subject of our conscious states. How
do we explain the presence of a sense of self in consciousness? How do we explain the
intimate relation the self bears to the conscious state? How do we explain what appears to
be a permanent asymmetry between your access to your conscious states and my access to
them? In short, how do we explain privacy?"

About the practical use of mindmelding, he notes:

"Mindmelding would be of great use in allowing us to accurately diagnose sensory
problems. Tinnitus, color blindness, and the like could be diagnosed quickly and
accurately by physicians who were able to mindmeld. Diagnosing tinnitus, for instance,
currently involves asking the patient about the exact quality of the sound he is aware of.
With mindmelding, the doctor would have the option of actually hearing the sound itself,
rather than relying on our limited ability to describe sounds. Mindmelding would allow
us to understand sensory experience of people with mental illness, such as the hallucinations of schizophrenics. [...] It would allow us to understand what the sensory experience of autistic people is like.
Mindmelding would also allow a much deeper understanding of dreaming."


### Nick Bostrom's Counter-Arguments

The most famous counterarguments against universalism—or more specifically, the unification hypothesis, which is a sufficient condition for universalism to be true but not an unavoidably necessary one—were published by Nick Bostrom. 
Bostrom’s first counterarguments both rely on the assumption of a quasi-infinite universe. While they ultimately fail to refute unification, they might still provide new insights into the nature of the universe and experience.

His first argument can be summarized as follows: 

P1: If unification is true, then the precise duplication of an experience cannot increase the total amount of experience (being experienced by the universal subject). 

P2: If the universe is quasi-infinite, then any physically possible experience, including any instance of suffering, is instantiated somewhere.

C: It is impossible to cause any additional suffering, no matter what we do. 

Bostrom argues that this conclusion would undermine our ethical theories, which should lead us to reject P1 and instead accept that each new instantiation of a precise duplicate constitutes a numerically distinct experience, thereby increasing the total amount of experience. 
This does not refute the sameness of subjecthood, only the sameness of duplicated experiences.
More importantly, it is unclear how one could infer a truth claim from this argument when it is based purely on intuitive merit considerations, namely, that our decisions should affect the total amount of suffering. This is no more problematic for unificationists than determinism is for those who accept that our actions are ultimately not under our control and therefore cannot alter the amount of suffering either way. 
Bostrom’s second argument is more significant and presents a genuine paradox. Zuboff formulates it as follows:
“If the experience-producing world is too big, a balance would have been tipped so that the most frequently occurring experience types, which are reflective of the reality that produces and shapes them, would have been flooded out by less frequently arising but far-greater-in-their-possible-number chaotic experience types. And if unification were true, since precise duplication could not increase the amount of experience, the greater frequency of reality-reflective experiences in such a world could not increase the probability of one's current experience being of the reflective type rather than chaotic. 
Thus, it would have to be grossly improbable that this experience you are having is not chaotic. To relieve this improbability, one would need to let go of either unification or the excessive largeness of experience-producing reality.” 
Since both a quasi-infinite universe and unification seem to be true, this creates a paradox. Rejecting unification would require refuting the argument for it, which Bostrom does not do. The argument can be summarized as follows: 
Imagine two precise duplicates of a brain, both experiencing the same experience. If we instantaneously swapped a small part of each brain with the other, the experience in each would not change, and the experiencer would not notice any difference. 
If we continued swapping parts step by step, the location of the experience would remain unaffected. However, if we swapped the brains in their entirety, it would be clear that the location of the experience had changed. 
This is only possible if there is ultimately just one experience instantiated in both brains.

One could counter that swapping a part of the brain actually swaps the experiences associated with that part, but that the subject of experience can't tell and hence remains the same. 
This would allow one to reject unification while still maintaining universalism.

If one does not wish to abandon unification, then one must instead reject the idea of a quasi-infinite universe or find an explanation for why we do not experience chaos. 
Resolving this paradox might provide new insights into the nature of consciousness. Abandoning the multiverse would also undermine Bostrom’s first (moral) argument. 
Since the multiverse hypothesis is very attractive, I would prefer to let go of unification without letting go of universalism. However, for now, it is probably best to remain agnostic, while I do want to acknowledge that universalism increases the probability of the multiverse hypothesis being false. Nevertheless, I will try to argue for a framework that's able to incorporate both universalism and the multiverse hypothesis.
Within this hypothesis, if unification and universalism were true, the universal subject would be the experience itself without any connection to its instantiations. If duplication and universalism were true, one would be a certain instantiation of the experience. 
One would in both cases not know which instantiation one is experiencing, but in the first case it is ontologically unknowable because the question doesn't even make sense, in the 2nd case it is only epistemologically unknowable, but one is in reality one specific instantiation. 
The problem with the second case is that there is no answer to the question what may happen to me if suddenly half my brain was swapped with the corresponding half of a precise duplication. 
If I was one specific instantiation, wouldn't then half my experience be altered by altering the instantiation? However, by arguing that the experience is dependent on the instantiation, while the subject is not, we are evading the issue, because even if we swap an experience, which alters the instantiation, the other instantiation's experience was also mine, so both are still mine after the swap, even if they are numerically distinct. Bostrom concludes similarly: 
"It is an interesting question what happens to personal identity in Zuboff’s scenario. For present purposes, however, we need merely note that the scenario does not work as an argument against Duplication. 
According to Duplication, what happens in this case is simply that there are two qualitatively identical but numerically distinct streams of phenomenal experience. At any given time, there is, for each of the lumps of brain-matter, a phenomenal experience that supervenes on it. 
Whether we regard the situation as one in which ultimately two brains have changed places or as one in which two brains that remain on opposite sides of the table have exchanged all their matter, is of no consequence as far as Duplication is concerned." 
The only remaining question is, how the subjecthood can be a universal, when all the other experiences are not. I cannot find a good explanation for this, which leads me to conclude that unification should in the end be true. The paradox remains.

Universalism, as based on the universal of immediacy, since experience is always here, now, and mine, rest on a type theory of identity rather than a token theory.
One contended counterargument to type theory is the slicing problem. Let's see why this assertion is false. 
Imagine replacing an entire human brain with a functionally equivalent computer. Arnold Zuboff’s proof of functionalism allows us to infer that this preserves qualia and consciousness. 
Nick Bostrom now imagines a thought experiment, in which such a computer is sliced into two identical halves by splitting each wire and electrical component in two, inserting an insulator material between them. 
Bostrom suggests that the insertion of the insulator leads to the duplication of qualia and that insulating more of the computer gradually leads to more qualia being experienced. He writes: 
"Duplication of experience happens when a segment of added parallel circuitry that is insulated from the original circuitry is comprehensive enough that, if this segment had existed on its own, the computation being implemented by it would have generated experience." 
This of course is only true, if duplication is true, but upon further reflection, it doesn't make sense. Suppose we begin slicing just the left visual system while leaving the rest of the brain unchanged. 
If visual qualia were truly being duplicated, we should recognize the difference, since both slices are still connected to an otherwise intact brain. But functionally, the system has not changed. Therefore, according to functionalism, qualia cannot change either. 
One can of course doubt the truth of functionalism, but I would refer to Zuboff's proof to convince you otherwise. This means we can continue slicing more of the system without any effect on experience. This is the slicing problem for the token theory. 
The question really is: Where is this duplicated experience experienced? One can of course argue that there is a new experience, but that it has the quality of immediacy and is therefore also mine, which makes it indistinguishable from my other experience. 

But there's another variant of the argument opposing the type theory of identity. Suppose we have fully sliced and isolated two identical halves of a brain. 
Now, if even an infinitesimal functional difference is introduced in one of the halves, this could, under a naive interpretation, create two separate conscious entities. 
This would mean that we now have two non-identical computations, C and C′, corresponding to two distinct conscious states, S and S′. Such an outcome would necessitate two separate conscious experiencers. 
Just as in the token theory case, we could then arbitrarily slice the system many times and introduce minuscule perturbations, yielding an exponential explosion of conscious experiences. But does this rule out the type theory? 
No, because altering one of the slices has changed the overall functional structure of the system, thereby altering the unified conscious experience rather than duplicating it. More fundamentally, this demonstrates that experience only exists as a whole. 

The function (which we are experiencing) itself can be conceptually decomposed into parts, but any change to one part necessarily alters the entire function. 
There can be no system in which only a single functional unit is different while every other computational process in the experience remains unchanged. Each functional unit is defined by its input-output relationships. 
If a unit is only internally different while still producing the same outputs, then no functional change has occurred, and thus no difference in experience can arise.
This reasoning extends to larger functional units such as the visual system. 
If a functional difference exists only within the visual system without propagating to the rest of cognition, then it can't influence behavior. But only that which can in principle influence behavior can be recognized, and only what's recognizable can be experienced. 
Therefore, only functional changes that affect the system as a whole are recognizable and experienced.
If we were to really change only the left visual system functionally, it would result in a difference in all the other brain regions as well. 
If both slices are still receiving inputs and sending outputs to the rest of the brain, the experience would include a superposition of both outputs, resulting in incoherence and probably just confusion. Type theory is still standing strong. 

### The Binding Problem and the Spread Brain Experiment

One possible resolution to Bostrom's probability paradox of unification (which I explained previously) may lie in solving the binding problem of consciousness, since its solution might reveal duplication to be true. 
The binding problem arises from the observation that our experience is unified, despite being composed of many functional elements. 
If we conceptualize the functional units of a brain as independent particles, each possessing its own reference frame, it seems paradoxical that these separate units collectively generate a singular, unified conscious experience. 
Since each unit only processes its own input-output relationships, it should only "see" its own local perspective, forming a local order of events, while other functional units might have a different order, which would make the whole system incoherent and unable to unify. 
An interesting thought experiment in the context of binding is the spread brain. Imagine we could activate and modify neurons through certain machines precisely as they are physically activated and modified in a functioning brain (maybe optogenetically). 
If we were to spatially disperse these neurons across the world (while preserving their connectivity and input-output relationships) the phenomenal experience should remain unchanged, since functional integrity is maintained. 
Taking this further, suppose we entirely sever the connections between neurons while still ensuring that each one is activated at the correct time with the correct inputs and outputs. 
In this case, temporal coordination within the connectome no longer plays a role, meaning we could activate the neurons in any arbitrary order without affecting experience. However, this leads to an absurd implication: 
since individual neurons are not functionally distinct from each other in isolation, we could theoretically reduce the brain to a single neuron being activated randomly, while still somehow preserving the same phenomenal experience. 
This conclusion is obviously untenable, and Arnold Zuboff arrived at a similar insight in The Story of a Brain in 1981. The question then arises: At what point in this process does experience disappear? 
The answer is that when neurons are fully disconnected, they no longer form a unified system. If one of the independent neuron-activating machines were to fail, the other neurons would have no way of detecting the failure. 
In effect, they would not “notice” each other at all, which means that they cannot have formed a singular, unified experience in the first place.

A first hypothesis might be that causality is the key factor in binding experiences. 
However, this does not fully explain the problem, as there are differences in the strength of binding across different levels of experience. 

- Local binding is very strong. E.g. we cannot willfully separate the colour green from the structural appearance of grass.

- Global binding is weaker and can be modulated by attention. E.g. we can choose to focus on smell or taste and suppress one from conscious awareness.

If causality alone determined the unity of conscious experience, we would expect a gradual fading of experience with increasing causal distance. However, this is not what we observe. 
Instead, we find a hard boundary between our own conscious experience and that of another person, even though two individuals can be causally linked to some degree.
Furthermore, causality does not solve the problem of reference frames. 
Every neuron appears to function independently in terms of its local input-output relationships, yet a thought is composed of an intricate pattern of active and inactive neurons, a phenomenon that cannot be accounted for by looking at neurons in isolation. 
As previously argued, a functional difference that cannot influence behavior cannot be recognized in experience. 
This implies that conscious experience requires a system in which functional units are not independent but rather form a holistic structure, such that any change in one unit necessarily alters the entire functional state of the system. 
Even inactive neurons contribute to experience by differentiating between potential patterns of activation. This leads to a crucial conclusion: for experience to arise, every functional unit within the system must contribute to the overall function of the system. 
This mechanism manifests itself through field-behavior, as seen by the evidence that certain electromagnetic fields of the brain have been shown to correlate with consciousness and synchronization and resonance across brain regions are indicators of unified experience. 
Physical fields provide a natural solution to the problem of reference frames:

- Holism: The field offers a unified description of the function that generates experience, integrating all functional elements into a coherent whole.

- Boundary problem: The physical properties of electromagnetic fields restrict conscious experience to an organism, as different brains have independent fields. Since every functional unit must depend on every other for experience, this forms the boundary of consciousness.

Within this framework, we are the function from its inside perspective, but it is incorrect to think that each consciousness has its own function. Instead, there is only one function across the entire multiverse, instantiated by physical fields. 
This function can be described in terms of input-output relations or topology. Functionalism is still true in this regard (descriptively). But ontologically, we would be the emergent field-function resulting from all electromagnetic interactions in the universe. 
The boundary of our personal experience results from being physically cut-off from the rest of the function, making us unable to perceive the rest of ourselves, even though we are also the rest of the function. 
This view (if coherent) resolves the probability paradox and allows for a multiverse while preserving both universalism and functionalism. If two identical experiences were created in separate brains, they would constitute duplications. 
Though identical from the perspective of each part of the function, they would be distinct parts within the total (universal) function. Since they share the same (universal) function, they are the same subject, but their experiences would not be experienced only once but twice. 
Thus, unification would be incorrect, and the multiverse vindicated. I’m not saying this is the final solution, but it could be a solution to Bostrom’s probability paradox.

A much simpler (but similar to my previous) answer to Bostrom's probability paradox follows directly from functionalism. Functionalism implies that only the input-output relations of a system (its causal relations) determine its phenomenal experience. 
We are the function. If this is true, it immediately implies the existence of a universal function, since everything within a given light cone is causally dependent. This means that any system (including us) is ultimately part of the same function. 
However, conscious experience is a special kind of system, one that can perceive itself, bind coherent information into a unified experience, and maintain a hard boundary separating itself from the rest of the bigger system. 
However, we do not need to explain these properties to establish that conscious experience remains part of the larger universal function, since this is a wholly separate claim that only relies on functionalism being true.

This leads directly to duplication: 

While identical experiences may be epistemically indistinguishable, they are still (as in my previous solution attempt) ontologically separate as distinct parts of the larger function. 
Since they exist within the same universal function, they are part of the same subject, which supports universalism. So, this view abandons unification, meaning that functionally independent experiences remain distinct, and still allows for the possibility of a multiverse. 
One might interject and criticize this explanation on the same grounds I criticized causality as experience-binding in my previous solution attempt to the probability paradox: How can causality account for the hard boundary of conscious experience? 
But this explanation doesn't claim that causality could account for the boundary of consciousness. There might be many more prerequisites for conscious experience, like coherence, consensus, integration, feedback loops etc. These are more likely to account for the boundary.


### In Denfense of the Type Theory of Identity

There are further thought experiments uncovering that the type theory of identity must be true and that duplication is metaphysically incoherent. If the argument here is successful, which I think it is, then we should actually abandon the infinite multiverse-hypothesis. This would have drastic implications for the theory of everything, and in particular Wolfram's model, which I believe to be the most probable candidate for delivering such a theory. It would entail that we, as conscious observers, have the property of producing causal invariance in such a way that all branches that would result in our experience continuing, ultimately always converge into a single branch. Such a model would possibly be explainable under Superdeterminsim, which I believe to be the most likely explanation. Other models would include Relational Quantum Mechanics, QBism or the Transactional Interpretation of Quantum Mechanics. All such models would then deserve to receive more attention.

Zuboff argues as follows:

"A 'token' of a 'type' is a specific instance of it. Thus tokens of the abstract type 
novel would include the more specific types French novel and American novel. 
One token of American novel would be the particular novel, the detailed type, 
The Adventures of Huckleberry Finn. And tokens of Huckleberry Finn would be 
the individual copies of it (which would at the same time be tokens also of the 
more abstract type American novel and the still more abstract type novel). 
 
We once visited a planet where, until we arrived, there had always been only 
one copy of any book. The inhabitants, consequently, had had a hard time 
discovering the distinction between a novel, as a type, and a copy of it, as its 
token. They sloppily thought, for example, that the literary merit of a novel 
depended on the particular identity of their one copy, which to them just was the 
novel.  
 
But even before we, with our great powers of duplication, had arrived on that 
planet, one of their own philosophers had invited them to engage in thought 
experiments about hypothetical duplications; and this alone was enough to 
remedy their confusion. He would ask them to think about a hypothetical 
duplication of one of their books. Couldn’t they then see a distinction they hadn’t 
noticed before, between a copy and a novel? This would be like the already 
familiar distinction between a word and its instances. The novel would number 
just one despite there being two copies of it. The novel would continue to exist 
when a copy was destroyed, so long as another copy still existed. Whether or 
not a book was exciting would depend on the novel rather than on the copy. 
And although in our own real exercise brains have been actually duplicated, it 
could have been just as effective if one of your Earth philosophers had simply 
asked you to engage in the corresponding thought experiments.  
Anyway, the suggestion we are considering, with the aid of our brain duplication 
exercise, is that the experience of being the particular person reading this 
particular discussion may be a detailed type, like a particular novel, and remain 
numerically one if instantiated more than once, as in the case of the duplicate 
brain activities in our exercise. And it is part of the view we are considering that 
the subject of the experience also remains numerically one in both instances, 
just as Huckleberry Finn, the character, remains numerically one in all the 
copies of his adventures.  
Perhaps the identity of an episode of experience and its subject, like that of a 
novel and its character, depends merely on a pattern and is therefore indifferent 
to changes in the particularity of the medium in which the pattern is maintained. 
And perhaps the particularity felt in experience, the this and here and mine in it, 
is merely a subjective impression that exists equally well in every occurrence of 
the pattern of it, like the setting and perspective of a novel. 
What I have just been describing might be called the 'type view' of the identity of 
experience and its subject, as opposed to the 'token view' that would tie the 
particularity of subject and experience to the particularity of a brain and its 
activity. And a variation of our exercise with the distant brains can, I believe, 
help you in deciding between these views. 
But first I must tell you that in a few moments my colleagues, in preparation for 
the next exercise, will be shutting down one of the brains in the pair that is 
currently communicating with your body. And this is something worth 
thinking about. For, if you believe the token view, you must hope, if you want to 
remain conscious for the next exercise, that it will happen not to be your brain 
that is shut down but the other one. If, on the other hand, you believe the type 
view, you won’t be bothered, because you then believe that you will continue 
being conscious whichever brain remains working, much as Huck Finn would 
continue in his adventures if any one of a number of copies of the novel 
happened to be destroyed. 

So, by the way, it should be clear that substantive matters like self-interest are 
at issue in these metaphysical considerations. On the answer to this 
metaphysical question of whether your particular experience is a type like a 
particular novel or a token like a particular copy depends how you will be 
spending the time following the shutdown. Thus I hope it is clear that we are 
not, as some might like to charge, simply indulging in linguistic decisions as to 
whether to call either the type or the token 'a particular experience'. To which, 
type or token, properly attaches the self-interested concern about remaining 
conscious is a substantive question not open to linguistic choice, any more than 
would be the question to which attaches literary merit, the copy or the novel.  
 
And now that you have reached this point my colleagues are about to do that 
shutting down of one brain in the pair. If the token view is right I must bid 
farewell to one of two readers. Five... four... three... two... one.... It’s been done. 
Since on the token view the reader I am now addressing is the lucky survivor, 
perhaps congratulations are in order. On the type view a shrug of the shoulders 
is in order. And now for an exercise that decides between these views.  
 
We have just cut apart the hemispheres of the remaining brain and put each 
hemisphere into its own nutrient bath. But we have all along continued by radio 
communication the pattern of impulses across the severed corpus callosum, so 
that the activity of each hemisphere has all along been integrated with that of 
the other as though they had never been parted. And each still communicates 
with the body in the same pattern it normally would. This maintenance of 
the normal pattern of brain activity and of interaction with the body explains why 
your experience and behavior is, even in this odd case, of a normal sort. 

Now we are going to embarrass the token view. You see, we have just turned 
on a duplicate we made of the right hemisphere, which will communicate with 
the left hemisphere and the body in the same pattern as the still operating 
original right hemisphere does. So each of the two right hemispheres is now 
interacting in a precisely similar fashion with the single left hemisphere and the 
body.
If you take the token view, that a subject and its experience are to be 
distinguished by the particularity of a brain and its activity, you are now faced 
with a puzzle. 
 
For the subject and the experience of either of the two right hemispheres is 
equally well joined with the subject and experience of the single left 
hemisphere. For example, the side of your visual field being processed in your 
left hemisphere is joined just as seamlessly with the other side of the visual field 
whether this is processed in one or the other of the two right hemispheres. But 
the token view must distinguish the subject and the experience in one of the two 
right hemispheres from the subject and experience in the other. According to 
the type view, however, there is now just the one subject, you, with the same 
experience you would be having if there had been no doubling of the right 
hemisphere. 
 
The type view, then, treats this case like that of a two volume novel when we 
have got two copies of the first volume but only one of the second. There is no 
doubling of the adventures of the first volume, somehow with only a single 
continuation in the second volume. Two central characters, one in each copy of 
the first volume, do not strangely become one character in the single copy of 
the second volume. Similarly, we should not try to say that the experience of 
one of the halves of your visual field is now doubled because of the doubled 
hemisphere and belongs to two distinct subjects, you and somebody else—but 
only half somebody else, because both subjects somehow are the same one 
subject in the activities of the single left hemisphere. This token view is 
incoherent."


To see this more clearly, just imagine what we've done before. Imagine, we shut down one of the right hemispheres. Would you need to be worried that you might not survive this shutdown? Well, on the token view you would only worry that half of your experience would not survive. But the problem is that there is no remaining left half of the experience on its own, because the remaining experience is still a unified experience of both the left and right hemisphere. Consequently, you cannot have experienced a shutdown, because the part of you that survives after the shutdown doesn't experience a shutdown. Therefore, you survive as a whole. 

If you are still sceptical, you might argue that I have cheated by limiting the thought experiment to one hemisphere, while we can only really prove the type theory if my entire identity would be on the line. Well, this is quite easily done. Imagine, instead of one left hemisphere and two right hemispheres a scenario with three duplications of a brain, 1, 2 and 3. Each duplication has a right and left hemisphere, only connected by our radio communication device. Both hemispheres in brain 1 communicate with each other. Both hemispheres in brain 3 communicate with each other. But the left hemisphere of brain 2 also communicates with the right hemisphere of brain 3 and the right hemisphere of brain 2 also communicates with the left hemisphere of brain 1. Now, if someone were to shut down brain 2 entirely, person 2 cannot have died. The experiences of person 2's left hemisphere would shut down, but the corresponding right hemisphere experience would survive in person 3. Similarly, person 2's right hemisphere would shut down, but the corresponding left hemisphere experience would survive in person 1. And since both 1 and 3 have identical overall experiences to person 2, person 2 must survive entirely in each of them.

By the way, this thought experiment also closes our loophole solution that the experience could have been duplicated while only the subject was universal. If duplication were correct, even if both of the right hemispheres were us, we would need to fear a 50% chance of half of our experience being eliminated in these thought experiments. But that's just incoherent. Either we die or we survive. There is no half survival. And since we do not die, we survive as a whole. Duplication is refuted, leaving us no choice other than accepting the non-existence of a quasi-infinite universe.



## Consciousness


### The Problem of Dualism

Dualism is the thesis that the mind is non-physical and that the mind and brain are two distinct things. I think that much of the powerful illusion of personal identity, absolute qualia and free will can be attributed to this dualistic belief, because it deceives us to believe in some form of soul, separate from the physical realm. Such a soul, as a supernatural phenomenon, cannot be part of of a natural world, for if it were, it wouldn't be supernatural anymore; it would just become part of what nature is. The common view of personal identity is that we are distinct souls, living inside a physical body, being born into and torn out of existence by the body's physical death. The common view about qualia is that they are somehow part of this soul-realm and are therefore wholly unexplainable by science. And the common view about free will is that this soul of ours can somehow itself cause some effects, independent of the physically determined chain of causation. The last view is particularly interesting, because it assumes a bidirectional communication between the physical and the mind-realm. How else would we even know about both of them? 

Daniel Dennett writes: "If mind and body are distinct things or substances, they nevertheless must interact; the bodily sense organs, via the brain, must inform the mind, must send to it or present it with perceptions or ideas or data of some sort, and then the mind, having thought things over, must direct the body in appropriate action (including speech). Hence the view is often called Cartesian interactionism or interactionist dualism. In Descarte's formulation, the locus of interaction in the brain was the pineal gland, or epiphysis. [...] Since we don't have the faintest idea what properties mind stuff has, we can't even guess how it might be affected by physical processes emanating somehow from the brain, so let's ignore those upbound signals for the time being, and concentrate on the return signals, the directives from mind to brain. These, ex hypothesi, are not physical; they are not light waves or sound waves or cosmic rays or streams of subatomic particles. No physical energy or mass is associated with them. How then, do they get to make a difference to what happens in the brain cells they must affect? [...] Dualism's embarrassment here is really simpler than the citation of presumed laws of physics suggests. It is the same incoherence that children notice - but tolerate happily in fantasy - in such fare as Casper the Friendly Ghost. How can Casper both glide through walls and grab a falling towel? How can mind stuff both elude all physical measurement and control the body? A ghost in the machine is of no help in our theories unless it is a ghost that can move things around." The soul realm could have no power of the physical realm and must fail to explain consciousness. 

These old arguments against dualism might not refute all types of dualistic approaches, but this wasn't my goal here. The goal was just to show how this one idea has really stiffled philosophical progress, even though it's so easily shown to be a fantasy. Instead of refuting all typs of dualism, it will suffice to prove that functionalism must be true, which is a non-dualst account of consciousness and mind. If functionalism is true, then dualism is false.


This, in turn, means that the merely intrinsic characterisics and states of any parts of the brain are wholly irrelevant to consciousness. As long as the input/output-relations are the same, the experience is the same. This implies that philosophical zombies, as imagined by David Chalmers, are impossible. It also implies that mind-uploading is indeed possible and that cryonics/cryopreservation can actually work in principle. While these topics are interesting, they are not discussed in further detail here.


### Functionalism
Functionalism claims that the character of our mental experience is wholly produced by the function it plays in the system of our brain. The identity of a mental state is then determined by its causal relations to sensory stimulations, other mental states, and behavior. According to functionalism the whole mental character of vision—the whole of how things look—is fixed purely in the pattern of responses to vision and not in any of the initial processing of vision in the visual cortex. 
To prove this view, let us imagine an operation performed on the brain of a patient. In this operation, the left side of the patient’s visual cortex is replaced by a non-organic machine that performs exactly the same function as the visual cortex would have. 
The machine preserves exactly the same causal input/output relationship with the rest of the brain. The only differences are internal to the machine, not in the external relationships with other brain regions. 
We can imagine two worlds: one in which the patient retains their organic visual cortex, and one where they have the non-organic implant. In both worlds, the person behaves exactly the same, as all causal relations are unaltered. 
Now, there are only three possibilities for what happens to the character of the patient’s mental experience: 

1) There is a change in the character of mental experience, and the person recognizes this change. This is impossible because the person would act differently if their mental experience changed, which contradicts the premise that the machine preserves all causal relationships.

2) There is a change in the character of mental experience, but the person does not recognize this change. This is also impossible because any change would only occur in the left visual cortex. If qualia were inverted or absent, the right visual cortex would remain unaffected, leading to an obvious discrepancy in the character of qualia between the left and right visual fields. Such a difference would be impossible to ignore.

3) There is no change in the character of mental experience. This is the only viable option.
So, functionalism is true.

Zuboff writes about this thought-experiment: 
"Of course one and the same pattern of speech and behaviour, as described from the
outside, might be produced by very different psychological states, as when sincerity is
replaced by pretending. It is this consideration that seems to defeat the behaviourist attempt
to define the mind purely in terms of behavioural dispositions. But in our case we know that
the pattern of psychological responses to vision remains the same; it is impossible that
anything like pretending be introduced by the gadget as stipulated. For the parts of the brain
that would be involved in pretending, or in any other psychological complication that could
have produced the same speech and behaviour despite a difference in the experience, are
necessarily unchanged by the gadget and therefore unadjusted to any change in experience.
So the speech and behaviour, it seems, must simply be responsive to an unchanged
experience.
But there may be a way we can think of even the psychological pattern remaining the
same despite important differences in the quality of experience. It seems I can easily
imagine myself as experiencing red objects with the same phenomenal quality with which I
now experience blue objects and vice versa. And it seems I can also easily imagine myself
as having experienced colour differently in this way from birth (which is essentially, of
course, the tale often told by philosophers about such a difference of experience between
two people). It seems I could further imagine that, despite that private difference in
experience, I was still taught to say a fire was “red” and the sky was “blue” and, whichever
qualia I regularly experienced with them, I still formed the same patterns of practical,
intellectual, emotional and irrational associations and reactions regarding the colours of fire
and sky. It seems that the pattern of my psychological responses, not just my behavioural
dispositions, could have been the same as now in such an imagined case of qualia inversion,
while my experience of red and blue was different. Moreover, it seems I can imagine an
automaton, with something like my pattern of mental functioning in its mechanism, but with
no qualia, no experience, at all.
But just as it is impossible that anything like pretending be introduced by the gadget,
it is impossible also that anything be introduced like the sort of sweeping qualia inversion or
absence that might be imagined to allow experience to change while the pattern of mental
functioning stayed the same. For, once again, it is impossible for the remainder of the mental
system to adjust in any way to any change in the experience processed by the gadget, since
the remainder of the mental system is, due to the stipulation, necessarily unchanged.
An inversion that might seem to be without functional implications would have to be
both systematic and total; it would have to occur consistently across the whole of the
experience involving the relevant qualia, and therefore, in this case, in the qualia of visual
memories and imaginings as well as in those of all of immediate vision. (And an absence of
qualia that might seem to be without functional implications would have to be a total
absence of all the qualia, an absence, that is, of all consciousness.) But our gadget
replacement was of only the left half of the visual cortex. So if this replacement somehow
resulted in an inversion or absence of qualia in the vision processed by the gadget, this
inversion or absence on the right side of vision would clash with the necessarily unchanged
qualia of visual memories and associations, as well as with the unchanged qualia of the other
side of the visual field. Such a clash would make it absurd, in the now familiar way, that the
pattern of mental functioning could not be reflecting a clash. So qualia inversion or absence
cannot be what is happening in the gadget replacement. The experience must simply be the
same.
[...]
It misleadingly seems to us that the intrinsic nature of our quale of red cannot be
determined by something as extrinsic to our visual processing as the external pattern of the
gadget’s causal relations with the surrounding brain. And this seems aptly illustrated by the
ease with which one can imagine colour qualia reversing their roles in one’s psychology, as
we earlier did with red and blue. Impressed by the apparent non-relational immediacy of
qualia and their seeming interchangeability, we want to link them intimately to nonrelational, interchangeable intrinsic properties of the brain. And we can then seem to
understand the possibility of qualia inversion as the possibility of a role reversal in brain
activity, between, say, “chemical x” and “chemical y”.
But what can prevent our imagining a reversal of the roles of such functionally
interchangeable chemicals between the right and left visual cortex of the same person? And
then, according to this assignment of qualia to such intrinsic determinations, red and blue
things on one side of the visual field would look the other way around from how they looked
on the other side; yet they would be treated and thought of, in all the activities of the rest of
the brain in which they were compared, as looking the same on both sides. And this would
be absurd.
If we are to imagine that qualia can be totally inverted or absent without functional
implications, this requires that we think that qualia depend on interchangeable nonfunctional properties of a sort that could thus be inverted or absent without functional
implications. But that means it must also be possible that these non-functional properties,
and therein qualia, could be changed unsystematically, and that there could be merely partial
inversions or absences of qualia, without any functional implications. And this, of course, is
absurd.
[...]
But, one might think, there may yet be a way to resist this conclusion. What about
the very aphasia case I described earlier? We would naturally regard it as absurd that a
person could at once be moving based on vision and honestly reporting he is not seeing that
scene in which he is moving. This case of mental inconsistency seems impossible, yet it
exists. So maybe what we rejected, a gadget replacement in which experience changed with
no effect on mental functioning, is like aphasia, and only seemingly impossible. But the
cases are crucially different.
We try to imagine what it’s like for the aphasia patient to move around and deal with
objects based on vision that he honestly says he’s not having. We then realize that the state
of vision on which the movement is based and the very different state of vision on which the
speech is based must each be experienced as though it did not belong to the same person
who is in the other state. But surely both states must really belong equally to the patient. If
only one of them had existed, it would undoubtedly have been his experience. How can the
mere existence of the other have changed that ownership? The solution to this puzzle, then,
is that it must just be seeming to the patient that he is in only one of these visual states. He is
really in both states at once, but with an illusion in each that he isn’t in the other. Being in
both is not the same as knowing that one is. This confusion between metaphysical and
merely epistemic boundaries is what makes such a break between the functions of a single
person’s mind seem impossible though it really is not.
Notice that this case of aphasia, unlike that of the gadget replacement, has functional
implications, which are essential to it. It consists in a disharmony between functions, those
of speech and movement, which corresponds, we must suppose, to an alienation between the
visual experiences on which these functions are based."



### A Functional Analysis of Experience

In his treatise of functional analysis of experience Zuboff encourages us to look at a so-called Necker cube. The Necker cube is an ambiguous drawing, resulting in an optical illusion.
Each part of the drawing is ambiguous by itself, yet the human visual system picks an interpretation of each part that makes the whole cube consistent, by choosing an orientation it might have, if it was a 3D object.
We can shift this orientation ourselves, even though, of course, the drawing of the cube does not change.

"A naive view is that the shift is an event in the object, in the picture on the paper, and
you are simply open to this in your perception of it. If it were not for the special obviousness
in this case of a reason for rejecting it, this naive view would be by far the most natural one.
After all, that view agrees with how things look. The phenomenal properties of the picture
are changing, and it can only look to us as though the picture in itself is changing.
What makes it so obvious in this case that the natural view is wrong is that you are
controlling the shift. Which way the picture looks is decided by you, not by anything in the
object. So you know that what changes is not the object but something in you, your way of
seeing it. As philosophers we are lucky here. Since in the struggle for survival either way of
seeing such a cube (called a “Necker cube” in perceptual psychology) could be useful, in
this special sort of case nature has allowed us not only a rare power over our own perception
but with it also a rare opportunity to see beyond our usual naivety. In most cases we simply
take our perception as a passive openness to the object, which is thought of as in itself just as
we see it. But now we may learn that seeing anything requires an activity in us, of seeing it
as something.
The Necker cube can, I think, suggest strongly to us the secret of what this visual
experience is. Is it not plausible that the shift in your experience of the cube be nothing but a
shift between two systematic patterns of potential psychological and behavioural responses,
the responses that you would make if called on to describe the cube, trace its front side,
imagine objects resting upon it, etc.? When you are shifting from one way of seeing to the
other, and the object is looking as though it is changing in its properties, the various ways
you would function regarding that object are shifting. The suggestion is that the change of
look in the object is nothing but the shift in the functions, from an appropriateness of
function to one orientation of the cube to an appropriateness of it to the other orientation. In
that change in the functions, and logically inseparable from it, the look of the object is
changing. The object couldn’t now look like that without such a shift in the functions and
there couldn’t be such a shift in the functions without it now looking like that. The shift in
functions and in look are necessary and sufficient conditions of each other. And this that the
Necker cube so strongly suggests is that which our consideration of the gadget replacement
had already proved. Experience is logically determined by function.
Someone who straightforwardly saw an object would be therein fixed to say, do and
think things appropriate to it having a look; that would be its having that look for him. A
lesion between the visual cortex and the speech centre could, in robbing him of some but not
all of that appropriateness of function, make us say he both does and does not see. Further
such lesions, between the visual cortex and more functions, would rob him of more ways in
which he was seeing till finally, cut off from having any functional character, the visual
cortex would have lost its significance for vision and he would be straightforwardly
sightless.
The appropriateness of potential responses to an orientation of the cube would not be
a consciousness of the potential responses themselves. No, rather the existence of the pattern
of potential responses would be the consciousness of the orientation of the cube. And one
mustn’t imagine lots of distinct structures and determinations in the brain, each designed to
deal explicitly with another circumstance in which another appropriate response would be
implied by the existence of the mental state. Thus if an appropriate response is that I would,
with the right motivation, describe cube A as seeming to have the orientation of cube B, one
mustn’t expect the function of speech to include an explicit, spelled-out determination that I
would be so describing it to a circus clown named Bozo to whom I was honestly reporting
my experience or that I would be ready to say the appropriate thing in some particular
ancient lost language if I had learned to speak it. These implications would be there, in my
function of speech, if it was fixed in a way appropriate to my seeing that orientation of the
cube; but they would be there only implicitly.
Let’s now examine more closely what happens when we see. Light from the object
strikes the retinas. In the retinas light-sensitive chemical reactions trigger an immediate
pattern of impulses in associated neurons. Specialized cells are so connected with this
immediate stimulation that they are stimulated only by various abstract features of it. For
example, one specialized neuron will receive impulses when, and only when, lines lie at a
certain angle, no matter where on the retinas they are registered. This will finally be
translated into functional responses to that angle of line as an abstraction. Much of the
processing in the visual cortex carries on like this, with more and more general features of
the scene being registered in specialized neurons. But none of this is vision if it is considered
only at this stage. None of it ever would be vision if it did not issue in functional effects that
fixed the look of the object of vision by their appropriateness to its being that way.
A pawn in a game of chess may be a piece of stone or a piece of wood. Its being
stone or wood is an intrinsic property not importantly related to its being a pawn. That the
piece of stone or wood is also a pawn is logically determined by its extrinsic property of
possessing a role in a game. Without the context of the game it makes no sense to call this a
pawn. Yet the pawn still is the piece of wood or stone. It has intrinsic properties too,
intrinsic properties which must be consistent with its role if it is to be a pawn. It would be a
crude mistake to want to identify the pawn somehow with the wider game and not the piece
of stone or wood simply because it is the wider game that makes the stone or wood also a
pawn.
Seeing the angle of a line is something caused by the relevant stimulation of the
eyes, something that in its turn then causes the various appropriate functional determinations
of talking, thinking and behaving, the ones that go with the line being seen as at that angle. I
do not want to say that seeing the angle is the determinations of the functions any more than
I would want to say that a pawn is a game of chess. It is the stimulation of the specialized
cell in the visual cortex that is the seeing of the angle, but only because of its effects in the
functions. Without those effects the seeing would be bereft of its phenomenal character, as
well as its behavioural and internal psychological implications, and then it would be no
seeing. I am here endorsing a view called “the causal role identity thesis”; I am endorsing it,
that is, if it includes my understanding of phenomenology. A mental item has its mental
nature purely through playing the causal role of that item; but its playing that role, I would
insist, is to be understood, as we proved through our gadget replacement case, to be fixing
any phenomenal character the mental item may have. This is crucial because a phenomenal
character is essential to much of the mental. What would a pain be without it?
And, speaking of pain, let me reinforce this analysis of vision with a consideration of
another mental item, a pain in my right hand. The naive view that is directly suggested by
the way the thing seems is that an unpleasant substance or quality is actually occupying an
area of my physical hand, just as some injected material would be doing. But no physical
investigation of my hand will turn up that substance or quality. Furthermore, someone who
had lost an arm could be genuinely encountering an object just like this one, as a phantom
limb pain, without the hand even being there.
The naive view is that if the area of the pain then shrinks two causally connected
events are occurring. The pain itself is shrinking, and this shrinking of the pain is causing
my experience of it to change accordingly. But there are not here two causally connected
events. It would be absurd, a contradiction, for the pain to shrink while my experience failed
to reflect this. My experience of the pain logically determines the entire existence and nature
of the pain itself. This is not to say that the pain as mental object somehow is the experience
of it. This also would be absurd. The pain and the experience of it are correlatives. For a
purely phenomenal object like a pain, to be is to be perceived. Its whole existence and nature
is logically determined by the existence and nature of the experience of it. In a sense only
the experience exists, since in its existence alone does the pain exist; but then in that other
sense, of course, that it is perceived, the pain does exist. And all its phenomenal properties,
of location, intensity, unpleasantness, are determined utterly in the experience of them.
But what makes the experience an experience of just such a pain? Try to think of my
feeling that pain in my right hand while I was in all ways fixed to respond to it as a tickle on
the sole of my left foot. How could it have the properties of a pain in my right hand then?
How could it fail to be instead a sensation of a tickle on the sole of my left foot?
The pain or the tickle, the phenomenal object, exists only in there being an
experience of it. This experience is the brain process, or whatever else, that plays the
psychological causal role of the experience of the pain or the tickle. And this experience
possesses its phenomenological character, and therein the pain or tickle possesses its
apparent character, solely through the experience’s functional character, through the way it
affects the mental functions that would deal with the pain or the tickle.
Yet much of the mental is partly or wholly unconscious. The conscious mental
activities, speech, imagination, voluntary movement and so on, are, as it happens, in the
upper part of the brain. They typically are highly integrated with and reflective of each
other, though, as we saw in the case of aphasia, they are not necessarily so. When a child
learns to use a spoon, at first it is the conscious mind that awkwardly deals with it. As the
child practices, however, neurons lower down in the brain are firing sympathetically with
the conscious brain activity and also averaging out the mistakes and clumsiness. Such
actions, once learned, are far better performed unconsciously. Soon it will not be the
conscious mind that deals with the spoon. The conscious mental activities will be free for
those things done better by them.
In the view I am urging consciousness is not an occult quality but rather a certain
area and style of activity. Consider for a moment some things that lie on the borderline of
consciousness. Some detail of your peripheral vision or the sensations in your toes, were you
really conscious of these before I mentioned them? That may be hard to say. But they were
brought very definitely into consciousness when I did mention them. In your becoming more
conscious of the toes they became much more influential in the conscious mental activities;
you then were ready to talk about them, think about them, perform voluntary actions in
response to them. My view, of course, is that this much fuller occupation of your conscious
functions with the state of your toes just was your more definite consciousness of them."



### A Functional Analysis of Colour Qualia

"So what should a functionalist say about qualia inversion? It is not surprising that
colours are experienced as systematically interchangeable on the most obvious level of
functioning. For colours serve, quite literally, as mere placeholders in our spatial experience.
It must be that one colour could easily appear in the place of another. Yet colours must be
distinguishable; something in how we experience them makes red look different from blue.
Anyone who embraces functionalism because of the a priori reasoning of this paper must
say that, since only functional properties determine the experience of qualia, when we
imagine an inversion of that experience without any obvious functional change we are
actually imagining an inversion of certain subtler functional properties that give the colours
their particular looks. But what could these subtler functional properties be?
At one time it was popular to talk about the “red-green paradox”, that two people
might be experiencing red and green qualia that were inverted between them with no sign of
this difference in their speech or behaviour. But philosophers have largely been won over
instead to talking about an inversion of the whole spectrum (with which my earlier inversion
of red and blue is roughly in line). This is because the qualia of red and green are not in
every way interchangeable after all; they are differently related to the qualia of the other
colours.
For example, green will be produced by a blending of blue and yellow. This looks
right to us. But if your quale of green was replaced by that of red, the unchanged blue and
yellow qualia would be rather unconvincing in producing the former red quale instead of the
green when they blended. This would look wrong, even if experienced that way from birth.
It was thought that standing the qualia of the rainbow upside down, an inverted spectrum,
would preserve the pattern of blending. But this has been challenged. I think the truth is
rather that each colour quale is in a unique relationship with all the others. For then we may
be able to understand a quale as logically determined by the set, appropriate only to that
quale, of one’s potential responses to its potential blendings and contrasts.
There is a feature of colour experience that perceptual psychologists call “colour
constancy”. I once looked at my familiar brown briefcase lying on a couch and was
surprised to discover that a spot on its surface had turned pink. My first thought was that
perhaps some strong bleach had been splashed on it. But after a while I realized that a circle
of bright, unusually focused sunlight was shining on the briefcase through a small opening
in a curtain across the room. As soon as I recognized this fact the colour constancy in my
perception changed the look of the colour of that spot from pink-in-the-shade to a lighted-up
brown. I was able then to make myself move back and forth between seeing one colour and
the other, just as you were able to do between the two orientations of the Necker cube.
I think that when my experience of that colour changed what was crucially changing
was a set of implicit potential responses to relationships of colours and light, to how this
colour would blend and contrast with others, in sunlight or out of sunlight. To see this colour
as pink-in-the-shade was to be ready, implicitly, with all those responses appropriate to its
looking pink-in-the-shade; and the change to seeing the colour as lighted-up brown was a
change to an alternative state of readiness and appropriateness that was dramatically
different. If a colour has any look for us we must be in a state of appropriate response to that
look. And the a priori assurance that functions preserve all experience requires that the
colour having its look just is our being in such a functional state.
The functions of speech and behaviour that we are tempted to think would remain
unaffected in a systematic qualia inversion are those based on extrinsic relationships of
qualia, such as being associated with certain names or objects. The functions that I am
arguing logically determine the qualia, are based rather on qualitative relationships that are
inherent in the qualia and inseparable from their natures.
The number nine has extrinsic relationships with its name and with the planets of the
solar system, which happen to be nine in number but might not have been. But its inherent
relation to three, considered as its square root, logically determines that the number is nine.
Nothing that has nine’s relationships with three, or with five hundred and four, could fail to
be nine. Just so, nothing could fail to be a certain quale that had its inherent relationships
with the other qualia. And an expression of those same inherent relationships in the higher
functions, in the implicit potential responses to the look of a colour, could not fail to be an
experience of that quale.
But the main point for me is not that such an account of our experience of qualia is in
its own right persuasive. It’s rather that something like this account must be correct because
functionalism can be established as necessarily true by the reasoning of our replacement
argument.
I confess that the combination of the replacement argument for functionalism and the
inverted spectrum argument against it has sometimes appeared to me to represent an
unresolvable paradox at the heart of the mind-body problem. On the one hand it seemed
easy to conceive of total systematic inversions or absences of qualia that were functionally
irrelevant. On the other hand the proposition that such functionally irrelevant changes in
qualia were possible would have had to imply that the qualia depended on non-functional
properties; and this in turn would have had to imply that there could also be partial,
unsystematic changes in qualia that were functionally irrelevant. We have seen that such
non-functional disturbances of experience would be absurd. They would be far more clearly
absurd, I have been arguing, than the determination of qualia by purely functional
properties.
If I find myself insisting that my experience of red is uncapturable by any functional
analysis, I can stop myself by reflecting that if God were playing a trick on me, of fiddling
with the character of whatever was causing me to speak, as long as God preserved just its
functional implications for speech, I would go on talking in exactly this way about the
absurdity of functionalism. Until, of course, this reflection stopped me. Functionalism seems
to me by far the lesser of two apparent absurdities."


Another interesting topic on the subject of qualia, which I will only discuss briefly, is the problem of aphantasia. Aphantasia is a condition where a person is unable to visualize images in their mind. Most people can picture objects in their "mind’s eye." For example, if someone asks you to close your eyes and imagine an apple, you could likely describe its color and any unique details. However, someone with aphantasia wouldn’t be able to do this. To them, the idea of an apple exists only conceptually, not as a mental image. While they understand that apples can be green, yellow, or red, they can’t visualize a specific one in their mind. Imagination exists on a spectrum between hyperphantasia and aphantasia. 
Both types of imagination seem to be closely connected to memory. People with hyperphantasia can recall and describe memories in much greater detail, while those with aphantasia tend to forget autobiographical information more easily. This suggests that memory is strongly linked to the ability to summon visual details.
Researchers believe aphantasia may be caused by a weak connection between the frontal and occipital lobes. The frontal lobe generates the intention to visualize, while the occipital lobe creates the mental image.
What makes aphantasia especially interesting in the study of consciousness is the idea that it may involve unconscious mental visualization. It seems that people with aphantasia can visualize 3D objects almost as well as people without aphantasia, since they can solve 3D rotation puzzles almost as quickly, but people with aphantasia are not consciously aware of their mental processes involved in solving these puzzles. But this is exactly what we would expect on our view of functionalism. As we've already found out, any functional unit in our brain can only be recognized in terms of its input-output relationships with any other functional unit. It the frontal and occipital lobes are only weakly connected, it is completely expected that we will not be able to create mental visualizations. The Multiple Drafts Model of consciousness will also make it clear that these visualization need not be send to any other place in the brain to become conscious. It is completely sufficient to posit that the occipital lobe is a bad competitor in the competition of clout in the brain. 


### Multiple Drafts Model and the Thousand Brains Theory

Now, after finding out that functionalism is the right description of consciousness, experience and mind, we can next devote ourselves to figuring out how it all works.

Daniel Dennett's Multiple Drafts Model (MDM) presents a functionalist approach to understanding consciousness. MDM posits that consciousness arises from parallel, distributed processes within the brain, leading to multiple, simultaneous interpretations—or "drafts"—of sensory inputs and cognitive events. These drafts compete for influence over behavior and cognitive functions, with no single draft holding privileged status as the definitive conscious experience. Dennett's model draws from the Global Workspace Theory in positing that consciousness arises when information is broadcast across a neural workspace "where just about everything can get in touch with just about everything else", allowing disparate cognitive modules to access and integrate information. 

"The basic idea is that consciousness is more like fame than 
television; it is not a special “medium of representation” in the
brain into which content-bearing events must be transduced in
order to become conscious. As Kanwisher (2001) aptly emphasizes: “the neural correlates of awareness of a given perceptual
attribute are found in the very neural structure that perceptually analyzes that attribute.” Instead of switching media or
going somewhere in order to become conscious, heretofore
unconscious contents, staying right where they are, can achieve something rather like fame in competition with other fame
seeking (or just potentially fame-finding) contents. And, according to this view, that is what consciousness is." 

According to this view, this competition is what defines consciousness. Just as one can be on TV and seen by millions without becoming famous, because the TV appearance doesn’t have the right impact, there is no special area in the brain where representation alone is enough for consciousness. It’s always the consequences that make the difference.

"So consciousness is not so much fame, then, as political influence—a good slang term is clout. When processes compete for 
ongoing control of the body, the one with the greatest clout 
dominates the scene until a process with even greater clout displaces it."

I think one of the strongest arguments against the Cartesian Theater resides in a very short passage in Dennett's book Consciousness Explained. The Cartesian Theater would require a certain place where all the information comes together to become conscious. But if this were the case, we could never become conscious of information, which we were unconscious of at the moment of it being received. An often cited example is the autopilot-driver: "You have probably experienced the phenomenon of driving for miles while engrossed in conversation and then discovering that you have utterly no memory of the road, the traffic, your car-driving activities. It is as if someone else had been driving." This seems to be plausible if consciousness was a Cartesian Theater, because we think we were unconscious of the road-information we were receiving while driving and because it hadn't entered our Cartesian Theater, is is forever lost in unconsciousness. But we can think of counter-examples: "An even more striking case is the pehnomenon of being able to count, retrospectively in experience memory, the chimes of the clock which you only noticed was striking after four or five chimes. But how could you so clearly remember hearing something you hadn't been conscious of in the first place? The question betrays a commitment to the Cartesian model; there are no fixed facts about the stream of consciousness independent of particular probes."

In Dennett's model, consciousness is not a static state but a dynamic process, continuously updated and revised as new information becomes available. The big open question is how the brain decides what process becomes famous. This is where the Thousand Brain Theory, proposed by Jeff Hawkins et al, comes into play. Numenta's Thousand Brains Theory (TBT) complements the Multiple Drafts Model by proposing a consensus/voting mechanism underlying the brain's processing capabilities. According to TBT, the neocortex comprises numerous cortical columns, each functioning as an independent "brain" capable of learning complete models of objects through sensory inputs and movement. Each column creates its own predictions and interpretations, analogous to the multiple drafts in Dennett's model. Cortical columns operate in parallel, processing sensory information independently. However, to achieve coherent perception and action, the columns engage in a voting process, communicating their interpretations to reach a consensus. This consensus mechanism ensures that the brain integrates the diverse outputs of individual columns into a unified experience, aligning with the distributed processing and competition among drafts proposed by the Multiple Drafts Model. The person as whole only ever becomes conscious of the results (the consequences) of this consensus, which is easily shown to be true, because at any given moment our eyes receive completely different images, in turn causing our V1 neurons to receive rapidly changing signals. But even though our neurons constantly receive completely different signals, we are only aware of a stable image representation of our environment. 

Daniel Dennett wrote: "According to the Multiple Drafts model, all variaties of perception—indeed, all varieties of thought and mental activity—are accomplished in the brain by parallel, multitrack processes of interpretation and elabroation of sensory inputs. Information entering the nervous system is under continuous "editalorial revision." For instance, since your head moves a bit and your eyes move a lot, the images on your retina swim about constantly, rather like the images of home movies taken by people who can't keep the camera from jiggling. But that is not how it seems to us. People are often surprised to learn that under normal conditions, their eyes dart about in rapid saccades, about five quick fixations a second, and that this motion, like the motion of their heads, is edited out early in the processing from eyeball to ... consciousness. [...] The editorial processes occur over large fractions of a second during which time various additions, incorporations, emendations, and overwritings of content can occur, in various orders. We don't directly experience what happens on our retinas, in our ears, on the surface of our skin. What we actually experience is a product of many processes of interpretation—editorial processes, in effect. They take in relatively raw and one-sided representations, and yield collated, revised, enhanced representations, and they take place in the streams of activity occuring in various parts of the brain. This much is recognized by virtually all theories of perception, but now we are poised for the novel feature of the Multiple Drafts model: Feature detections or discriminations only have to be made once. That is, once a particular "observation" of some feature has been made,by as specialized localized portion of the brain, the information content thus fixed does not have to be sent somewhere else to be rediscriminated by some "master" discriminator. In other words, discrimination does not lead to a representation of the already discriminated feature for the benefit of the audience in the Cartesian Theater—for there is no Cartesian Theater."

Even though I'm not going into more details about the model, it can convincingly explain many more, and I would even claim, all of the known psychological diseases. It is particularly insightful to consider the following phenomena in light of the MDM: dissociative identity disorder, schizophrenia, dissociative amnesia, self-talk, cognitive dissonance, metacontrast, the colour phi pehnomenon, blindsight, red-green colorblindness, alien hand syndrome, phantom limb syndrome, depersonalization or derealization disorders, aphasia, Capgras syndrome, Anton-Babinski syndrome, prosopagnosia, reduplicative paramnesia, anosognosia, Tourette syndrome, Déjà vus, ADHD, synesthesia or palinopsia. 


### The Left Brain Interpreter

Studies, pioneered by Michael Gazzaniga, on split-brain patients showed that each hemisphere of the brain could independently answer perceptual questions in multiple-choice tests. These rare patients resulted from a medical procedure for epilepsy, where severing the corpus callosum prevented seizures by stopping feedback loops between the two hemispheres. However, this also isolated the right and left hemispheres, creating two separate streams of consciousness, though they were barely noticeable in daily life.

Experiments revealed this split, for when the left hemisphere (via the right eye) saw a chicken foot, the right hand chose a chicken. Meanwhile, when the right hemisphere (via the left eye) saw a snowy scene, the left hand selected a snow shovel. When asked why, the patient—who could only speak from the left hemisphere, where Broca’s area and Wernicke’s area process language—knew why the right hand picked the chicken but had no access to why the left hand chose the shovel. Yet, the left hemisphere immediately fabricated a reason: "The chicken foot goes with the chicken, and you need a shovel to clean the coop."

Years of research confirm that our brain constructs narratives to explain our actions, even though our behavior results from a highly modular and automatic system. Our quick reactions, emotional responses, and learned behaviors provide material for an “interpreter” that integrates these inputs, asks, "Who is responsible?" and inevitably concludes, "I am." Though an illusion, this mechanism shapes our perception of self. To really get this point, let's look at some more reasons to believe that such an interpreter is actually deceiving us into thinking that we have free will. For example, experiments show that when people are shown a screen with a cursor and asked to select images, they tend to identify with the cursor's movement, even if it's controlled by another participant. Under hypnosis, some people perform actions and then fabricate stories to explain them, often creating plausible reasons their brain invents. Extreme examples of this confabulation include anosognosia, where patients with impairments like paralysis or blindness deny their condition and invent reasons for their behavior. The Anton-Babinski syndrome is even more extreme, where blind patients believe they can see, fabricating visual experiences despite no signals from the visual cortex. This suggests that most of our perception of reality may be generated in our language center, which uses inputs from all senses to create a plausible narrative. This is also supported by the fact that there is no evidence that, thousands of years ago, people were able to perceive the colour blue because there was no word for it. Once the word existed, people identified the sky and sea as blue, while they had previously been assigned other colours or colourlessness. While consciousness is made up of many processes, the language center seems essential to conscious perception. 

While these experiments also support my claims about the non-existence of free will and true responsibility, they are here meant to show that we really do fabricate drafts of narratives in out mind. Our brain synthesizes various drafts into a coherent narrative, even when some information is inaccessible or incomplete. This narrative construction is a functional process, enabling individuals to maintain a consistent sense of self and understanding of the world, despite the underlying fragmentation of neural processes. This, it seems to me, is the mechanism for probing drafts, which Daniel Dennett describes:
"Probing this stream [of drafts] at different places and times produces different effects, precipitates different narratives from the subject. If one delays the probe too long (overnight, say), the result is apt to be no narrative left at all—or else a narrative that has been digested or "rationally reconstructed" until it has no integrity. If one probes "too early," one may gather data on how early a particular discrimination is achieved by the brain, but at the cost of diverting what would otherwise have been the normal progression of the multiple stream."

This probing mechanism operates separately from the consensus mechanism mentioned earlier, but it plays a crucial role in prioritizing inputs for the consensus process. At any given moment, multiple drafts interpret our sensory inputs. Consensus is first established at lower levels of mental processing—such as what we see, hear, or where we are—each forming its own internal agreement. These lower-level consensuses then compete for dominance at higher levels of cognition. The winning drafts shape our conscious awareness, influencing our behavior and feedback loops, since their selection reinforces certain functional elements of our mind while weakening others. This effectively prioritizes certain processes to be able to influence our behavior. Our interpreter forms a narrative around our priorities, deceiving us into believing that all of our them are our doing, even though we only get to experience the result retrospectively, when it has already been decided. This narrative around priorities is what Michael Graziano calls the attention schema. Such a model of our priorities ehances the brains ability for self-control.


### Grid Solution to the Binding Problem

Daniel Dennet writes in his book Consciousness explained: "Visual stimuli evoke trains of events in the cortex that gradually yield discriminations of greater and greater specificity. At different times and didfferent places, various "decisions" or "judgements" are made; more literally, parts of the brain are caused to go into states that discriminate different features, e.g., firest mere onset of stimulus, then location, then shape, later color (in different pythways), later still (apparent) motion, and eventually object recognition. These loclaized discriminative states transmit effects to other places, contributing to further discriminations, and so forth. The natural but naive question to ask is: Where does it all come together? The answer is: Nowhere. Some of these distributed contentful states soon die out, leaving no further traces. Others do leave traces, on subsequent verbal reports of experience and memory, on "semantic readiness" and other varieties of perceptual set, on emotional state, behavioral proclivities, and so forth. Some of these effects—for instance, influences on verbal reports—are at least symptomatic of consciousness. But there is no one place in the brain through which all these causal trains must pass in order to deposit their content "in consciousness". [...] Contents arise, get revised, contribute to the interpretation of other contents or to the modulation of behavior (verbal or otherwise), and in the process leave their traces in memory, which then eventually decay or get incorporated into or overwritten by later contets, wholly or in part. This sekin of contents is only rather like a narrative because of its multiplicity; at any point in time there are multiple drafts of narrative fragments at various stages of editing in various places in the brain. While some of the contents in these drafts will make their brief contributions and fade without further effect—and some will make no contribution at all—others will persist to play a variety of roles in the further modulation of internal sate and behavior and a few will even persist to the point of making their presence known through press releases issued in the form of verbal behavior."

An important idea in AI research is that neural networks can represent information in high-dimensional spaces, where different properties of stimuli are encoded along distinct dimensions or hyperplanes. For instance, the colour "red" and the concept of an "apple" might be represented in separate hyperplanes. James Garson proposed that the integration of these properties—perceiving a red apple—occurs when processing trajectories align across these hyperplanes, allowing for the coherent binding of features. This approach circumvents the classical binding problem of how two types of information interact. There is no interaction. One difficulty in this might be that two units of information seem to either be bound or not bound together, but cannot be somewhat or partially bound. Separate hyperplanes would allow for such partial binding, since the trajectory in activation space might not be precisely along the dimensions of such property-hyperplanes. This solution could also only ever be a partial solution, since it might explain local binding, but doesn't explain global binding. It has been a critique that causality is too broad a term to explain global binding, since input-output relationships are so numerous in our brain that everything would have to be bound together. 

I believe that the solution to the binding problem consists of two parts. The first part involves grid cells, and the second part is our attention/narrative mechanism. While local binding is quite similar to the proposed hyperplane solution, it is better explained through the known neural mechanisms of grid cells. Global binding, on the other hand, is best understood in terms of small-world networks, a long-range consensus mechanism, and an model of attention. Importantly, the binding problem does not require us to abandon functionalism or computationalism.

Place cells and grid cells are well-researched types of neurons located in the hippocampus and entorhinal cortex, respectively. They are believed to play a crucial role in the entire neocortex as well. Essentially, a particular grid cell is activated when we occupy specific locations in physical space. For example, if we were playing football on a field, certain grid cells would activate only when we were at particular points on that field. There might be ten such points where the same grid cell would be activated whenever we were in these points vicinity. Research has shown that these activation points form a hexagonal pattern. One can visualize this as ten large circles drawn on the football field, each surrounded by six neighboring circles, which explains the hexagonal-shaped activation regions.
However, a single grid cell alone cannot differentiate between these ten distinct circles. If we relied on just one of these cells, we would be unable to differentiate between the ten possible locations. The solution to this lies in grid cell modules, which utilize different grid patterns with varying circle sizes and orientations. The overlapping activations of multiple grid cells within these modules allow for the reliable identification of a specific location on the field. With increasing number of grid cells in a grid cell module it becomes increasingly improbable to have two locations on the field where all the activations are the same.

Movement from one point on the field to another corresponds to a particular transformation in grid cell activations, which can be described as a "distance" in activation space. It is postulated that these distances are tracked by what Numenta refers to as displacement cells. Displacement cells function similarly to grid cells in that they cannot, on their own, represent a unique displacement. However, "the cell activity in multiple displacement cell modules represents a unique displacement in much the same way as the cell activity in multiple grid cell modules represents a unique location. [...] Note, a displacement vector not only represents the relative position of two objects, it also is unique to the two objects. Complex objects can be represented by a set of displacement vectors which define the components of an object and how they are arranged relative to each other. This is a highly efficient means of representing and storing the structure of objects. This method of representing objects allows for hierarchical composition."

In this way, grid cells or similar mechanisms can form representations of different object properties, effectively solving the local binding problem. The global binding problem is solved by the consensus mechanism between different functional modules.
Hawkins et al write:
"One of the classic questions about perception is how does the neocortex fuse different sensory inputs into a unified model of a perceived object. We propose that the neocortex implements a decentralized model of sensor fusion. For example, there is no single model of a coffee cup that includes what a cup feels like and looks like. Instead there are 100s of models of a cup. Each model is based on a unique subset of sensory input within different sensory modalities. There will be multiple models based on visual input and multiple models based on somatosensory input. Each model can infer the cup on its own by observing input over movements of its associated sensors. However, long-range non-hierarchical connections allow the models to rapidly reach a consensus of the identity of the underlying object, often in a single sensation.
Just because each region learns complete models of objects does not preclude hierarchical flow. The main idea is that the neocortex has 100s, likely 1000s, of models of each object in the world. The integration of observed features does not just occur at the top of the hierarchy, it occurs in every column at all levels of the hierarchy."

On attention, they write: "One of the key elements of a location-based framework for cortical processing is the ability of an area of cortex to rapidly switch between object spaces. To learn there is a logo on the coffee cup we need to alternate our attention between the cup and the logo. With each shift of attention, the cortical grid cells re-anchor to the location space of the newly attended object. This shift to a new object space is necessary to represent the displacement between two objects, such as the logo and the cup. It is normal to continuously shift our attention between the objects around us. With each newly attended object the cortical grid cells re-anchor in the space of the new object, and displacement cells represent where the new object is relative to the previously attended object."

It can then be conjectured that our overall attention is also a shift between object spaces, where the object is, connected to the MDM, the winner in the competition of fame.



### Attention Schema Theory

I contend that Graziano's attention schema theory is subsumed by Gazzaniga's interpreter. Graziano proposed that an attention schema is like the body schema. Just like the brain constructs a simplified model of the body to help monitor and control movements of the body, so the brain constructs a simplified model of attention to help monitor and control attention. The information in that model, portraying an imperfect and simplified version of attention, leads the brain to conclude that it has a non-physical essence of awareness. The construct of subjective awareness is the brain's efficient but imperfect model of its own attention. This model of attention is practically identical to the interpreter described by Gazzaniga, which is (as I've said) identical to the probing mechanism described by Dennett i.e., the fabricated narrative is the narrative about our attention. In this view, consciousness is not a direct reflection of sensory inputs but a model- or narrative-based construct that provides the organism with an understanding of its own focus and mental states. The attention schema enables the brain to attribute awareness to certain stimuli, creating the subjective experience of consciousness, which aligns with the functionalist emphasis on the roles and interactions of mental processes. 

In the greater scheme of things the voting mechanism which decides about what drafts of narrative become famous and can therefore influence our behavior, is itself made into a narrative of prioritization by an interpreter in our mind. This narrative is what Graziano calls the attention schema, which is related to our memories in such a way that we would call these memories our experiences (that we were aware of). The primary adaptive function of the attention schema is to enable a better, more flexible control of attention. In the theory of dynamical systems control, a control system works better and more flexibly if it constructs an internal model of the item it controls. An automatic airplane-piloting system will work better if it incorporates a model of the dynamics of the airplane. An air and temperature controller for a building works better if it incorporates a rich, predictive model of the building's airflow and temperature dynamics. The brain's controller of attention works better by constructing a rich, internal model of what attention is, how it changes over time, what its consequences are, and what state it is in at any moment.

This model is essentially the narrative we perceive as that which we are aware of. The model itself might to a great extend reside in our brain's language-processing modules, but it is highly connected to every part of our cortex. It only becomes aware of the consensus within our cortex, but might influence the voting process itself, for it might influence the shifting of attention within the cortical object spaces.



## Morality

Why is consciousness so tightly linked to morality? In questions regarding animal ethics, abortion ethics, AI ethics, or medical ethics, consciousness is often a deciding factor for how to treat any being. 
Kant thought that humans had some kind of inherent dignity. But where does that come from? Philosophers have tried to answer this question, but it there doesn't seem to be an actual answer. It somehow just has to be there, in order to make Kantian ethics work. 
The answer to both questions is really simple in hindsight, given our knowledge about Universalism. Parfits argument, that personhood is based on psychological connectedness (relation R) is overcome and instead is refined by the argument that personhood is defined by what makes an experience mine, this, here and now, which is nothing else than its style of immediacy. 
If this is accepted, then every being with this pattern of immediacy in its experience is the same person. By that logic, consciousness, as a necessary condition for immediate experience, tells you, when another being is you. 
That is why consciousness is so important, because it distinguishes what is you and what not. And by doing that, it distinguishes what is of moral consideration and what not. There is no inherent dignity to humans, but it emerges from consciousness, self-interest and Universalism.

### Hume's Law
How to get an 'ought' from an 'is':

P1: One should only desire, what one really desires.

P2: There is only one subject of experience, every being with immediate experiences is the same subject.

C: One should only desire, what everyone really desires. 

Here we evade the is-ought problem by seemingly already including an 'ought'-statement in the premises, that's actually a necessarily true statement. Now one would of course ask, how this ought-statement can be justified, but it is justified, because desire and beliefs are intimately connected. 
Arnold Zuboff gives the example of a cup of hot mud. Looking at the cup, I could believe it to be hot chocolate and may think that I desire it. But in reality my believe is wrong, which is why my real desire is to not drink from the cup. 
Applying this principle consequently to all my desires, requires for me to have a perfect grasp of reality. My only real desires are those I would have if I had a perfect grasp of everything involved. 
The principle going along with this that governs my actions must tell me to act, as far as possible, as I would want myself to be acting with a perfect grasp of everything involved. 
It would include within it, then, all the motivation of all of the various systems of desire, but it would also have the correction of all that motivation in light of the perfect grasp. The overall result must be a desire for the reconciliation of all systems of desire. 
This is not to say, that a perfect grasp of reality is desirable let alone achievable, but it describes the process by which we at least try to live by, because our mind automatically simulates and auto-corrects our desire in accordance with our beliefs. 
But is premise 1 really a factual statement? In this regard I think Sam Harris was right, when he said that 'ought' and 'should' are verbal traps. They presuppose an objective perspective, from which morality can be evaluated. Such an 'ought', presupposing an objective will of reality, must fail as long as reality is not teleological.
But if I just ask, 'should I drink from the cup?' I am not asking about a god-given rule, I'm only asking if I really desire to drink from the cup. It is not that deep. 'Should' is just a short-hand for 'real desire'. Therefore premise 1 is just a tautology: One really desires only what one really desires. 

### Normative Fundamentals

A desire or will is always a call for change toward a desired future or more accurately against certain undesired futures. In a control system, these would be set points, but we can call them goals. The cause of our desire, whatever its origin, is called a motivation. 
The root of our motivations lies in our preferences and their root in well-being. Preferences allow us to rank different experiences based on their quality of well-being—that is, how much we desire them relative to one another. An ethical theory must take this into account. 
It is a famously unsolved problem that one cannot from is-premises derive an ought-conclusion. An ought-statement is a certain kind of want-statement or desire, because if I ought to do something, I would obviously want to do it, since it would be the right thing to do. 
Thus, what I ought to do must be what I would want to do if I knew what it was. The real question, then, is how to determine what I would truly want to be doing in a given situation. There is an answer to this question, but the answer is out of our reach: 
I would always want to do what I would want to do if I had a perfect grasp of everything involved. Since what I ought to do must be something I can actually do, there would be no further possible understanding beyond such a perfect grasp. 
I would take into account every perspective and information to reach the conclusion of what I would then actually desire and could be sure that there is nothing to change this desire anymore, because there is no further grasp to be gained about the situation. 
Since this desire could not be changed, it must be what we would ultimately want to do. It follows that I ought to do what I would want to do if I had a perfect grasp of everything involved. 
As noted above, any desire is ultimately motivated by well-being. If ought-statements are driven by desires, they are motivated by well-being and preferences. So I could also say that what I ought to do is what I would prefer doing if I had a perfect grasp of everything involved. 
With this in mind, universalism completes the ethical framework. If I say, "I ought to do what I would really prefer if I had a perfect grasp of everything involved", I can substitute "the universal subject" for "I", since everyone is the same subject according to universalism. 
This yields:

"I ought to do what the universal subject would really prefer if the universal subject had a perfect grasp of everything involved."

Notably, we cannot substitute "everyone" for the other "I" in that sentence, because it does not refer to the universal subject, but rather to the specific agent with its unique thoughts and influence on the world.

This principle shares similarities with Rawls' "veil of ignorance," but instead employs what Daniel Kolak calls the "veil of wisdom". 
Instead of ignorance it employs a perfect grasp (perfect knowledge and understanding) of everything. Instead of judging as if I was to be born into one, I need to be judging as if I was to be born into every conscious being. 
The result is an ethical framework that is fundamentally consequentialist-utilitarian in its pursuit of well-being, specifically aligning with preference utilitarianism, as it seeks to reconcile everyone's preferences. 
Even more specific would be the recommendation of "practical (non-ideological) negative preference utilitarianism" or "Prioritarianism", because, as indicated in the beginning, a desire is a call for change, which would not arise if one is content with one's mental state. 
We do not desire some maximally specific future, but we prefer a future state that avoids what we want to avoid experiencing, which often results in the same overall goal (i.e. when there are only two options), but is more broadly applicable than utility-maximization. 
For example, if the only options are winning or losing, then avoiding to lose is the same as wanting to win. But in general, because we cannot precisely control or even predict the future, we can merely hope to evoke change in the right direction away from what we want to avoid. 
This is in line with Poppers views on science, where, since we cannot know the future, we can only ever hope to find theories that will not get falsified, but we can never actually obtain truth. 


### Ideal Observer Theory

The principle of perfect grasp is a type of ideal observer theory, making it cognitivist and subjectivist in nature. This means that ethical statements express propositions, which can be true or false, and where truth or falsity depends on people's preferences. 
This stands in direct opposition to non-cognitivism (such as expressivism), which asserts that ethical statements are not propositions and can only be honest or dishonest, but not true or false. To strengthen the principle of perfect grasp, we need to refute non-cognitivism. 
Expressivism is trivially false because ethical statements do not depend on emotion, if universalism is true. Any rational conscious being, even without emotions, or a psychopath, would necessarily need to agree with ethical statements derived from the principle of perfect grasp, 
because they will experience whatever they might otherwise inflict upon other conscious beings. If emotions are not necessary for ethical discourse, expressivism is obviously false, as it relies on emotional attitudes being the foundation of moral statements. 
Non-cognitivism is furthermore false because emotions are always functional states of interpretations of beliefs. Emotional expressions depend on the underlying beliefs that interpret all sensations. 
Even pain is not belief-independent, but rests on the belief that physical damage is harmful. This belief is so deeply ingrained in us (since even babies have this belief) that it seem more visceral and not like a usual belief. 
However, it is revealed to be one by people who have managed to overcome pain through meditation or otherwise. In psychology, it is a well-established fact (e.g. in the ABC model) that emotional reactions are responses to our beliefs. 
One could not experience specific emotions without corresponding specific beliefs. The claim that "moral statements are expressions of emotions" becomes (since emotions are expressions of beliefs) "moral statements are expressions of expressions of beliefs." 
Since beliefs are propositional (truth-apt) and can be right or wrong, emotions can be right or wrong as well, in the sense, that one would not feel them with a better grasp about what's involved.

So, even if the expressivist/emotivist claim were true, that ethical statements were just expressions of emotions of approval or disapproval, 
these emotions can still be right or wrong, depending on whether their underlying beliefs are right or wrong. For example, consider the desire to drink from a cup seemingly filled with hot chocolate, which actually contains hot mud. 
Your emotional expression of desire can be true or false, depending on what is actually in that cup, because your emotional expression is based on your beliefs about what the cup contains. Or think about your emotional expression of disgust upon seeing a pineapple pizza. 
This expression is only possible iff you believe there is something disgusting about pineapple pizza.

Beliefs can exist prior to emotions, but emotions cannot exist prior to the beliefs they express. 
One might ask, if one hasn't fully understood my claim, what could be right or wrong or true or false about an expression of pain. But I'm not claiming that the expression of pain is a belief in itself and could be right or wrong propositionally (because emotions aren't propositional), but that it is an expression of a belief. 
A change in beliefs necessitates a change in expression of beliefs, which means a change in emotional expression. The pain in itself isn't right or wrong, but the beliefs underlying that pain can be, so I might not truly want to express pain, if my beliefs were different. In that sense, the pain can be right or wrong (non-propositionally).
This means that beliefs are critical to any moral statement. Emotions cannot be explained without them and cannot be dealt with separately. Even though emotions are not propositions in themselves, they can still be right or wrong, depending on what beliefs they express. 

Zuboff summarizes: "I only ever desire a thing because of what I believe it to be. And since beliefs are correctable, so are desires. [...] My only real desires are those I would have if I had a perfect grasp of everything involved."


### The Intricacies of the Principle of Perfect Grasp

There are many intricacies about the principle of perfect grasp or what Arnold Zuboff calls the principle of best action. Let's explore a few of them to get a better understanding of what this view actually entails. 

1) "There could be a real desire to have another desire that was not itself real. For example, a child’s desire for an actual visit to the moon (as opposed to a fantasy visit) might be based on an ignorance of the visit’s difficulties and nastiness but might also, 
as an unrealizable desire, be harmless and fun to have. Thus having the desire might be really desirable. But being an object of a real desire would not turn the moon desire itself into a real desire. 
For the object of the real desire would be the moon desire and not the moon desire’s own object, the moon visit. The point of real desires is not at all that they would have been desirable to feel, as the moon desire would be. 
Real desires would anyway only be felt in a hypothetical perfect grasp of reality. The point of real desires is rather that they, and only they, as based on that perfect grasp of reality, have power to define for us which objects are really desirable 
(including perhaps a mistaken felt desire to visit the moon). From nothing more than such necessary truths about agency and desire springs the overriding motive of each and every agent to conform to the principle of best action, 
to do, as far as possible, what he would want if he had a perfect grasp of what he was doing."

2) "Real desires are those that would be had with a perfect grasp of reality. There are therefore real desires belonging to a rock, namely those it would have had 
with a perfect grasp of reality. [...] The perfect grasp contains all the angles. [...] The principle of best action requires the sort of reconciliation of all wills that would be formed in a perfect, and therefore merged, grasp by everyone of everyone and everything. 
Doing what is right for oneself turns out to be doing what is right for all those affected by one’s actions. Self-interest, transmuted through a principle that simply makes it consistent within itself, turns out also to be morality. [...] 
Morality and prudence have been revealed to be two characterizations of a single set of real desires. Morality is in their horizontal dimension, their responsiveness to others; and prudence is in their vertical dimension, their responsiveness to the future. 
Prudence is usually taken to be a self-interested farsightedness that will often conflict with morality. But morality and real prudence can never disagree, because morality, like real prudence, must agree with real self-interest." 
3) "Notice, by the way, that it is not rationality as such that is here being recommended. It might be a good thing in some respects not to be rational from time to time. But if we are irrational in assessing what is best, if our motivation is incoherent, 
that can prevent us from doing what is actually best for ourselves, what we would want ourselves to be doing grasping perfectly what it was we were doing. Rationality determines coherence, and what is important here is coherent motivation. 
That is the primary value of rationality for agents, though it has many secondary uses too. But rationality is not being worshipped here."

4) "A perfect grasp would have to comprehend at once, and perfectly, states of consciousness that essentially exclude one another. 
Perhaps this means that our hypothetical perfect grasp of reality is logically impossible. But, possible or not, omniscience is the inevitable ideal of our knowledge and the perfect grasp of reality is the inevitable hypothetical basis of an appropriate responsiveness to reality, 
which is the whole point of action. The perfect grasp need not be logically consistent to have this significance. [...] Let me mention a further feature of the perfect grasp that may make it impossible. [...] 
The perfect grasp must appreciate fully that what is important in the actual world is that the better possible experience become actual. In other words, although each potential experience must be fully grasped with all its compelling character, 
it must still be valued with regard to whether it should be made actual through action, as though the experience has not yet been had, as, indeed, it has not."

5) "It would be ridiculous to regard the possession of the perfect grasp as a particularly desirable state to be in. In the actual, limited state of consciousness I happily have the option of avoiding any close acquaintance with either 
the frustration of not scratching or else the pain that follows scratching. The perfect grasp, however, must comprehend fully all the things I might want to avoid, combined, perhaps impossibly, with a full appreciation of all the escapes from them that might be had. 
So the principle of best action is badly misunderstood if it is taken to be recommending an actual perfect grasp or an actual feeling of one’s real desires, which would anyway be unattainable for us and perhaps contradictory. 
Neither does the principle, considered in itself, recommend truth, knowledge or an increased perception, appreciation or grasp of things in any degree whatsoever. Though an increased grasp of things will often be useful in deciding which actions are endorsed by the principle, 
and in carrying them out, and some increases in grasp would be discovered in a perfect grasp to be inherently desirable, the purely hypothetical perfect grasp that is mentioned in the principle is merely employed for its aptness 
in defining the best course of action for an actual, limited consciousness. [...] In fact, though a perfect grasp is necessary in the hypothetical state that would define best action, 
knowledge that is merely propositional will often be far preferable as a basis for the actual carrying out of such action. [...] A genuine perfect grasp of a pain, since it would contain within it the pain itself, would have all the motivational force of that pain. 
By contrast, the mere entertaining of a proposition, which only refers to or describes the pain with its words, will not in itself carry such motivational force. Yet there is a way I could try to bring to bear on my actions at least some of the absent force in the perfect grasp. 
[...] I would be creating an image of a missing content of the perfect grasp of what was relevant to my choice so that this missing content might yet throw some of its weight against the actual temptation in my
current perspective. [...] 
And perhaps, because I am rational, I can behave as though motivated by a pain that I do not have."

6) "A perfect propositional knowledge of everything that could be relevant to your choices, as opposed to the perfect grasp we have been describing, 
would be inadequate for defining your real desires and best action. As we have seen in the case of the itch, you can act against even the best of judgments when they
are in the form of propositional knowledge; 
this form does not in itself have the motivational force of the reality it represents. Only the sort of perfect grasp I have required could be certain to render you perfectly responsive to the reality of your actions."

7) "What if I sought a little pleasure [...] in attempting to solve a crossword puzzle? Such pleasure must depend on initially not knowing the puzzle’s solution. 
Since that solution would be fully grasped somewhere in a perfect grasp of reality, a person with the perfect grasp might seem incapable not only of valuing sadistic pleasure but also of valuing the rather innocent pleasure of filling in a crossword puzzle. 
Must the attempting of a crossword puzzle be condemned because it seems in this way inappropriate to a truly comprehensive state of consciousness? But the perfect grasp must be a perfect grasp of all the values within the limited grasps. [...] 
On the one hand, a perfect appreciation of reality must include a perfect empathetic grasp of actual and potential states of ignorance and distorted appreciation; and so it must include the pleasures of both sadism and crossword puzzles. 
Yet, on the other hand, the perfect grasp must also include an over-arching final assessment of these states in the light of full reality, including the reality of victims’ pains and the crossword puzzle solutions. [...] 
Here it is important to recall a point I made earlier, that a merely hypothetical grasp of reality must include a full grasp of its own actual unreality. If the person’s perfect grasp is merely hypothetical, 
the stress must be on the reconciliation of the only real states of consciousness, the limited ones, rather than on any hypothetical gratification of that person
in his non-existent synthesis of these states. [...] 
Imagine that you with a perfect grasp could shrink back to possessing only a limited consciousness, perhaps taking with you just the information you would need to make your life a satisfied one within that narrow compass. 
Well, after the shrinking your best action would still have to be defined by what you would have been deciding with a hypothetical perfect grasp of everything affected by your choices. The knowledge brought back to your limited consciousness could be a dangerous thing 
without reference to the perfect grasp of what it really was you were doing with that knowledge. Your real desires, the things you really want to be doing, are what that perfect grasp, and only it, represents. 
The point is neither to have that grasp nor to escape from it, but rather, as far as possible, to conform to what it would have had you wanting."

8) "Another mistake about morality could be a fanaticism in the application of the principle of best action itself, 
having us always concerned about distant others and a distant future. If no special weight were given to oneself and those close to one, or to the present moment, much of human life, including that of similarly concerned others and future moments, would be poisoned, or starved.
Thus a moral concern for others may best be tempered by a particular concern with one’s own affairs. A prudential concern for the future may best be tempered by a respect for spontaneity and living in the present. 
Anyway, we often know more about ourselves and the present and can deal better with them. But none of what I been saying should be seen as a qualification of the principle of best action; it is what the principle itself must recommend. 
The principle has the power to absorb like a sponge all criticisms according to which it would be recommending anything undesirable, such as the self-defeating fanatical concern for others and the future that we have just rejected. 
The hypothetical perfect grasp itself must contain an appreciation of all such problems and any solutions. And it would include, as part of this, a full recognition of the limits and special needs of particular agents."

A moral system that demanded absolute, unrelenting concern for distant others and the distant future might, only at first glance, seem to follow from the principle of perfect grasp. But a perfect grasp would necessarily include a perfect grasp about one's own mental and physical limitations. Humans cannot function effectively under unbearable moral burdens. Excessive moral demands lead to burnout, inefficiency, and sometimes paradoxically worse moral outcomes (e.g., failing to care adequately for those closest to oneself or one’s immediate responsibilities). Moral demands must be sustainable; if an approach causes widespread exhaustion, resentment, or disengagement, it fails to produce the best outcomes. Furthermore, spreading resources too thinly diminishes effectiveness—if one tries to maximize distant concerns at all costs, one risks achieving less than if one had balanced priorities. In this way, fanaticism paradoxically harms the very values it seeks to promote. Extreme self-sacrifice discourages others from acting morally because people confronted with an impossible standard may give up entirely. Additionally, moral motivation depends on intrinsic satisfaction and meaning, which fanaticism often erodes. A perfectly informed agent would therefore conclude that an ethical approach must be practicable and motivating, not just theoretically demanding. If everyone were to prioritize distant concerns at the expense of their immediate responsibilities, then the entire moral structure of human interaction might collapse. A perfect grasp would include an awareness that local and immediate responsibilities are crucial for maintaining long-term moral effectiveness. Stoicism teaches that inner tranquility is essential for moral clarity and effective action. We must focus, for it is in our own best interest and best for our mental health, on what we can control, and accept what cannot be controlled. I believe that stoicism is the most important philosophy for finding out what a perfect grasp would entail.


### The Value of Life

Universalism leads to some new perspectives on ethics. One such perspective concerns the ethical value of life. Since the death of an individual is no longer synonymous with the death of their subjective experience, it follows that death is not inherently something to fear. While this was recognized in other ways by other philosophers, death becomes an entirely different story for universalists.

Thomas Nagel was one of the philosophers who argued against the common view that death is always evil. It is easy to see that death itself, as it is non-experiential and is therefore never experienced, cannot have a moral status in itself. Rather, one must argue that death deprives us of potential experiences. Nagel wrote: "The death of Keats at 24 is generally regarded as tragic; that of Tolstoy at 82 is not. Although they will both be dead forever, Keats's death deprived him of many years of life which were allowed to Tolstoy; so in a clear sense Keats's loss was greater, (though not in the sense standardly employed in mathematical comparison between infinite quantities.) However, this does not prove that Tolstoy's loss was insignificant. Perhaps we record an objection only to evils which are gratuitiously added to the inevitable; the fact that it is worse to die at 24 than at 82 does not imply that it is not a terrible thing to die at 82, or even at 806. The question is whether we can regard as a misfortune any limitation, like mortality, that is normal to the species. Blindness is not a misfortune for a mole, nor would it be for a man, if that were the natural condition of the human race. The trouble is that life familiarizes us with the goods of which death deprives us. If we put aside doubts about their status as goods and grant that their quantity is in part a function of their duration, the question remains whether death, no matter when it occurs, can be said to deprive its victim of what is in the relevant sense a possible continuation of life. The situation is an ambiguous one."

Under universalism the question of continuation becomes entirely irrelevant, because experience will just continue for universal subject not matter what. 

Instead, death must be evaluated on a different basis than usual. The fear of death can no longer be understood as the fear of non-experience. Nor is it simply the fear of losing one’s memories, as storing those memories as a computer file would not alleviate this fear. 
To think clearly about this, we must consider the will to live and the will to die, each of which has its own reasons that can be ethically addressed. 
The will to live is, in reality, the desire for one’s thoughts to continue having an impact on the world. The will to die is, in reality, the desire to avoid certain experiences. To evaluate these desires ethically, we need to consider the principle of perfect grasp: 

1) Would one truly desire for one’s thoughts to continue having an impact on the world if one had a perfect grasp of everything involved?

2) Would one truly desire to avoid certain experiences if one had a perfect grasp of everything involved?


Almost everyone would answer the first question affirmatively, as most people believe in the positive value of their thoughts. However, opinions about the value of other people's thoughts are often more sceptical. 
The second question can also be answered affirmatively. A perfect grasp of the relevant factors would include a comprehensive understanding of our opinions, beliefs, and attitudes, as well as how to change them to eliminate suffering. 
Since the desire to die is fundamentally tied to avoiding suffering, one would not truly desire death with a perfect grasp of all factors, as suffering could always be avoided, which one would always want to do, since suffering is inherently what one wants to avoid. 
That said, this argument is not entirely practical, as avoiding suffering is not always straightforward. Ultimately, both questions must be evaluated in the context of specific circumstances and cannot have universally applicable answers. 
Notice that the question of whether one's thoughts should continue to have an impact is also a question about suffering, as one's thoughts are only valuable insofar as they contribute to moments of well-being. I'd argue that universalism aligns best with negative utilitarianism. 
This also aligns well with Popper's view, who made the observation that suffering obliges us to help, whereas there is no similar call to increase the happiness of someone who is already doing well, creating an asymmetry in favor of negative utilitarianism. 
The asymmetry originates from the fact that there are moments of experience one might want to change (those being moments of suffering) and moments one doesn't want to change (moments of well-being). 
One can be content with a moment and not wish to alter it, even if it isn't the best possible experience. As long as one is content with their experience, there is, by definition, no need for change and no call for action. 

In Nagels example, when comparing Keats and Tolstoy, the question about whose loss was greater, is in reality a question about whose thoughts should have had a continuing impact. From this perspective, it can be seen why old people might be seen not to be as big of a loss as younger people, because younger people tend to have more new thoughts and consequently more potential to be a positive influence on the larger superorganism, while old people tend to have already had their best thoughts and their life continuing will not change their already existing impact as much. For these reasons, it is justified to declare Keats' early death as the higher loss. 

Nagels second argument, about potential experiences (like blindness for the mole) not being a misfortune if they are not part of our normal condition, is tightly linked to my second question regarding the avoidance of experiences. There might be kinds of experiences we could have had, which would have been much better than our actual experiences, and we could say that we have been deprived of them. There might be experiences we might not actally want to avoid but have avoided because we didn't have a perfect grasp. Suppose, for example, that you are a huge football fan and have won tickets to your favorit game. But you don't know that you have been gifted these tickets. Before you find out, another person steals your tickets and goes to the game, without you ever finding out that you had won. You have been deprived of a great experience. Or imagine you avoided going to party, because you preferred to watch a movie at home. But unbeknownst to you, you would have met the love of your life there and would have lived a much better life overall if you had gone. You have deprived yourself of a better experience.

Missing out on experiences might not always be a misfortune, but it definitely can be. If you had a perfect grasp of everything involved, you would want to act in such a way as to actualize those better experiences. 

But again, note that there is nothing tragic about death itself. The tragedy comes from the universal subject not being able to either have some experiences it might have wanted to experience, or not being able to have certain thoughts influence its other instantiations, which it might have wanted to keep on having an influence. Within the framework of universalism, when someone takes their own life, they deprive themselves of certain future experiences and the potential impact they could have had. However, they do not deprive themselves of experience itself or the ability to have an impact altogether; rather, they forgo the experiences tied to that specific organism and the unique influence the thoughts of that organism could have had. When you kill another person, you deprive yourself of the experiences you might have had living as that individual, as well as the potential future influence that person could have had. In this sense, killing is wrong, not because it results in annihilation, but because it robs you of potential future experiences and impacts on the world, which you might have wanted to have. Since we cannot fully predict what those future experiences or influences might be, the most prudent course of action is to preserve life in every incarnation.
There are very few exceptions. For instance, one might justify killing Hitler, but even in such cases, we cannot truly know if the counterfactual world—where Hitler dies before World War II—would result in less suffering in the long term. It might even lead to greater suffering over time. Since we do not have access to the perfect graps, it is unhelpful to think about other possible worlds. We should just do whatever we think, would be in our self-interest if we had a perfect grasp.


### Population Ethics

Universalism has far-reaching consequences for population ethics. One specific problem in population ethics is the non-identity problem. This problem says that even a change that would, on the surface, seem to represent a clear improvement for a future person will often fail to make that person better off. Instead, it often serves only to bring another person, a "better-off person," but still a nonidentical person, into existence in place of the one we intended to help.
Under universalism, this view no longer makes any sense. 
Any future person is the same subject because their experience is immediate in first-person style. Therefore, universalism solves the non-identity problem, removing a key issue people have with longtermism. The distant future does matter! 
Another issue in population ethics concerns how to evaluate collective well-being. Even if there was a reliable measure for well-being, simply adding everyone's well-being leads to Parfits repugnant conclusion, while averaging everyone's well-being leads to his absurd conclusion. 
Both approaches depend on the assumption of separate identities, which falls flat under universalism.
This is not to say that averaging or adding well-being is inherently wrong. 
However, giving each being the same scale of well-being (e.g., from -100 to 100) purely because of their physical separation is flawed. In averagism, for instance, we divide total well-being by the number of people, but under universalism, this number is totally arbitrary. 
We could hypothetically split or merge brains however we wanted, changing the "number of people" without affecting the number of actual subjects, which always remains one.
Total utilitarianism seems more consistent with universalism, but it comes with its own challenges. 
E.g. Robert Nozick, raised the problem of the utility monster, where one being (perhaps an AI or a merged brain) is capable of experiencing such enormous happiness that it justifies the mistreatment of others to satisfy it. This makes utilitarianism non-egalitarian. 
Additionally, the repugnant conclusion states: "For any possible population of at least ten billion people, all with a very high quality of life, there must be some much larger imaginable population whose existence, if other things are equal, would be better, even though its members have lives barely worth living".

Both of these problems arise from trying to frame population ethics in terms of mathematical points of well-being or suffering per life. 
Instead, we should think in terms of moments of experience being added to the universal subject's experience. These moments cannot simply be reduced to discrete points of well-being, but they can be understood in terms of their functions, if functionalism is true. 
Happiness and suffering, therefore, are functions and ultimately functions of behavior. This means that the limits of behavior are the limits of well-being and suffering, making it impossible for a significantly deviating utility monster to exist. 
The repugnant conclusion, on the other hand, can be evaluated in the same way an individual might evaluate their own life: the question becomes whether one would prefer a short life with high well-being or a long life with low but still positive well-being. 
Viewed this way, the repugnant conclusion isn't even truly repugnant. If experiencing the world has intrinsic value for the person, as Nozick suggested in his experience machine thought experiment, then the repugnant conclusion might actually be favored by the universal subject. 
Total utilitarianism, therefore, seems to be correct under universalism. Ultimately, if the number of experienced moments is important, is up the population under examination and cannot be answered objectively. 

### Longtermism

Consider the thesis that "we should not aim for ethical principles". If this thesis is wrong, then we should aim for ethical principles. 
If it is true, then "we should not aim for ethical principles" would itself be an ethical principle to aim for, thereby undermining its own claim that we should not aim for them. This contradiction means that there are ethical principles we should aim for. 

To determine what those principles might be, it is necessary to apply the scientific method to test and investigate which principles work best. Most of our moral discourse on such principles has been shaped by neartermism. 
By this I mean a focus on goals that primarily benefit people who are currently alive. Such collective goals include equality, freedom, stability (which encompasses security and peace), solidarity, truth, or fairness. 
Such ideals have become central to many societies, yet upon deeper reflection, they are largely products of neartermist discourse, even though some can also be applied to future generations. 
In contrast, longtermism has emerged as an ethical perspective that advocates for additional goals that civilization should pursue. These include sustainability, progress, and civilizational safeguarding (which encompasses civilizational immune systems and expansion). 
Without aiming for these goals, civilization will sooner or later inevitably encounter various crises. It may self-destruct due to an unsustainable standard of living, the degradation of its own ecosystem, or a lack of moral progress, leading to value lock-in and stagnation. 
Stagnation, in turn, correlates with moral regression and creates the risk of civilization being unprepared to handle critical future risks, what some longtermists refer to as S-risks (suffering risks of enormous scope and severity). 
To mitigate these risks, longtermists call for civilizational safeguarding to prevent such catastrophic scenarios from actualization. One can conceive of various civilizational immune systems designed to avert foreseeable disasters, 
such as AI enslavement, superviruses, immortal dictatorships, grey goo, natural disasters (asteroids, solar flares, supervolcanoes etc.), nuclear war, and so on. 
Arguably, the only way for civilization to ensure its long-term survival is through expansion to other star systems, populating as much of the universe as possible to minimize the risk of self-destruction, external catastrophes or a dark forest scenario. 

There are many arguments against longtermism, and while they might succeed in challenging the idea that uncertain future lives should be valued as highly as certain present lives, they do not refute the fundamental longterm ethical principles outlined here. 
One might claim that future generations should be responsible for solving their own problems while we focus on ours. But in the case of these longtermist principles, the issue is precisely that future people may no longer be capable of solving their problems 
if we are too complacent in accepting the lock-in of certain values, stagnation, or unsustainable technologies. While addressing these challenges may be relatively easy now, they may become significantly more difficult over time, much like what has happened with global warming. 
Longtermism is further strengthened by universalism, which clarifies that the suffering of future beings is just as much yours as the suffering you experience in your present life. 
According to the principle of perfect grasp, if one were to fully grasp everything involved, one would naturally seek to prevent futures of immense suffering, for one will otherwise unavoidably experience all that suffering. So this is what we should strive to do.


### Practical Application

There are many possible analogies to approximate the previously derived ethical rule:

"I ought to do what the universal subject would want to be doing if the universal subject had a perfect grasp of everything involved." 

I will discuss the two most helpful analogies and how they address ethical questions.

1. The Time Traveler Analogy:

Imagine that every other person in a given ethical situation is actually yourself, who has time-traveled back, erased your memories, switched bodies, and adopted the personalities of the people involved. If you fully grasp this mental exercise, you would naturally try to help others in need to the best of your ability because you'd recognize that you will one day be in their shoes. 
Peter Singer’s argument in Famine, Affluence, and Morality becomes obvious under this perspective: helping others is rational because their suffering is ultimately your suffering. But it will also be obvious that Singer's conclusion that such actions are obligatory is mistaken. 
Obligations require that others are truly other identities. You cannot have obligations to yourself. Instead, failing to help would simply be irrational, like refusing to treat your own wound. You don’t have to treat it, but ignoring it would only cause you more suffering. 
The time-traveler analogy can be used in many different ethical situations: helping people in need, helping the lower class, helping migrants, arguing for conditional pacifism or human rights, explaining the wrongness of racism or eugenics etc., creating rational empathy. 

2. The Reincarnation Analogy:

In this analogy, you imagine dying and having the opportunity to choose your next life, similar to Rawls "original position". Suppose you are presented with the option of reincarnating into an available body or skipping to the next one. 
Some lives, particularly those filled with extreme unnecessary pointless suffering, would be lives not worth living and you would probably skip them, if you could.
This analogy helps clarify ethical questions regarding e.g. abortion or antinatalism. 
Since I am ultimately the same subject in all lives, I do not have an obligation not to abort another being. I would simply be ending another potential instantiation of myself, which cannot harm me in any way. 
However, if a given reincarnation would lead to unbearable suffering, it would be irrational not to skip it. Just as I would choose to avoid a deeply painful situation in my current life if given the option, it would be rational to prevent such lives when possible. 
This reasoning extends directly to veganism. If I am to be reborn as every conscious being, then I will also live through the lives of factory-farmed animals, experiencing their suffering firsthand. 
Given the horrific conditions in which these animals are raised and slaughtered, I would want to skip those lives whenever possible. The rational choice, then, is to end the industries that create such suffering. 
Given the cultural hurdles, the most impactful option would be to contribute to the development of in-vitro meat and precision fermentation.

Given all this and given the fact that we should prioritize reducing suffering, these analogies might seem to favor antinatalism. 
After all, one could reduce suffering by skipping all incarnations ending conscious experience entirely. But 1st of all, the principle of perfect grasp is a maximization principle seeking the best outcome (positive utilitarianism), not avoiding the worst (negativ utilitarianism). 
Prioritizing suffering is therefore not a dogmatic principle, but a practical suggestion.
Second of all, antinatalism, if it were desirable, would only work inside the light-cone a certain civilization occupies. We can never influence the suffering outside of our light-cone. 
Therefore, destroying life is not a rational objective, because the universal subject would experience all the other lives outside our light-cone anyways. Instead, the best we can do is to contribute to the positive experiences of the universal subject as much as we can. 


### Veganism

Given our universalistic considerations it is a horrible attrocity and what some philosophers might call a transparent (albeit not really transparent anymore) tragedy that animals around the world must endure unimaginable suffering. Since animals cannot even rationalize suffering, like humans can, and since animals have to rely on their senses more than we do, it stands to reason that their pain is even more intense than ours. It is blatantly obvious that we should not allow animal suffering since all of it is our own suffering. The killing of animals is a separate issue, which, according to universalism, is not as reprehensable as is culturally believed. As I have laid out in my evaluation of life, the question of killing another being is a question about whether I would want that being to keep having an impact on the world and whether one would want to have their experience or rather skip it. I don't think there is a general answer to this even in the case of animals, but their impact on the world might seem to be negligable and their experience rather worthy of avoiding. Another separate issue is the relative value of animal and human lives. It can be argued that human lives are, from a consequentialist point of view, more important than animal lives, for we are the only known species that's able to expand beyond our current planet to colonize the universe. Humanities long-term value for conscious life could therefore justify our generally higher relative moral value. But I will spare any more details on these issues. Instead, I want to concentrate on a few arguments for vegeterianism/veganism and then argue what the right action might be.

1. Jeremy Bentham already made one of the most important points about animal welfare: "The	day	may	come	when	the	rest	of	the	animal	creation	may	acquire	those	rights	which	never	could
 have	been	withholden	from	them	but	by	the	hand	of	tyranny.	The	French	have	already	discovered
 that	the	blackness	of	the	skin	is	no	reason	why	a	human	being	should	be	abandoned	without	redress
 to	the	caprice	of	a	tormentor.	It	may	one	day	come	to	be	recognized	that	the	number	of	the	legs,	the
 villosity	of	the	skin,	or	the	termination	of	the	os	sacrum	are	reasons	equally	insufficient	for
 abandoning	a	sensitive	being	to	the	same	fate.	What	else	is	it	that	should	trace	the	insuperable	line?
 Is	it	the	faculty	of	reason,	or	perhaps	the	faculty	of	discourse?	But	a	full-grown	horse	or	dog	is
 beyond	comparison	a	more	rational,	as	well	as	a	more	conversable	animal,	than	an	infant	of	a	day
 or	a	week	or	even	a	month,	old.	But	suppose	they	were	otherwise,	what	would	it	avail?	The
 question	is	not,	Can	they	reason?	nor	Can	they	talk?	but,	Can	they	suffer?"
Bentham asks us to imagine a human being transformed into an animal. Imagine an individual, a human, subjected to a series of transformations by an evil scientist. First, the evil scientist administers hormone therapy, causing the human to grow fur until their body is entirely covered. Then, the scientist amputates the individual's human legs and replaces them with dog-like legs. Finally, the scientist replaces the entire human's body with the body of dog, except the human's brain.
This process leaves us to question: Is this transformed being still morally valuable? Has the person lost their inherent moral worth simply because they now exist in the body of a dog? One might argue that the individual still holds moral value because their brain—the seat of intelligence, reasoning, and self-awareness—remains human. After all, if their reasoning capabilities remain intact, might we not still regard them as having moral value comparable to that of a human? However, there are obvious counter-examples to this kind of argument, since we do not just kill and eat cognitively underdeveloped humans. For instance, what if, instead of a fully developed human brain, an infant’s brain were taken and implanted into a dog? In this scenario, the child’s brain would still possess the potential for intelligence and reasoning, but it would be an underdeveloped brain, incapable of the reasoning capacities that are often used to justify moral worth. And yet, if this child were implanted into a dog’s body, would we now regard them as having less moral value, or even no moral value at all, because their intellectual development was stunted by the transformation? Wouldn't we have even more empathy for such a human, who has to suffer such a horrible fate? We can think of all these reprehensible scenarios and will find no characteristic, Bentham says, other than the ability to suffer, that makes us morally valuable. Scientific research has demonstrated that many animals, including cows, pigs, and chickens, form deep social bonds, experience grief, and even show signs of self-awareness. If we base moral worth on emotional depth and the capacity for suffering, then these animals should be granted ethical consideration.
3. Animal agriculture is one of the leading causes of deforestation, greenhouse gas emissions, and biodiversity loss. Producing animal products requires significantly more land, water, and energy than plant-based food. By consuming animals, we directly contribute to our own environmental destruction, which causes suffering not only for animals but also for humans, particularly vulnerable populations affected by climate change. Changing one's diet in line with animal welfare might not just be a healthy decision and a relatively easy moral decision, but it can also impact and protect the environmental security of other humans and humanity's sustainability as a whole.
4. Most people accept that it is wrong to cause unnecessary suffering. However, modern animal agriculture subjects billions of animals to extreme confinement, painful mutilations, and violent deaths—all for products that are not necessary for human survival. "If our own health depended on eating animals, then there could be an argument for violence against animals (serving nutritional purposes) being necessary. But that’s not the case. We’re not inflicting horrible suffering on animals in order to preserve our own health and thus prevent our own suffering. We’re inflicting suffering on billions of animals in order to get a little more culinary pleasure at most. And very likely not even that: In an experiment at the University of Bochum, 90% of the students didn’t notice that their “beef goulash” was vegan. The availability of vegan gourmet food is increasing rapidly too. Last but not least, it’s largely a matter of culinary socialization anyway: Nobody craves exotic foods (such as dog, dolphin or chimp meat) that don’t exist and are taboo in our society. The same would be true in a vegan society (providing plenty tasty cruelty-free meats) with regard to all meat that requires violence against any sentient animal."
5. Once you adopt a speciesist position, you accept that at some point in our evolution, a human suddenly becomes a morally valuable human, while previously being an ape, and that a specific evolutional shift grants humans moral consideration, while the ape could simply be killed. However, a new species never directly emerges from another, so you’re left deciding between two nearly identical creatures, one of which is morally valuable and the other not. Where does this sudden morality come from? It's entirely arbitrary and unjustified. One might try to create a gray area to avoid this dilemma, but that only shifts the problem. The gray area must start and end somewhere, such as between a child and a parent from the previous generation. These differ only marginally as well, so why are the boundaries of the gray area defined there? Given this evolutionary continuity, drawing a moral line between humans and animals is arbitrary. If we value humans based on their sentience and emotional complexity, we must extend similar consideration to nonhuman animals.

I don't think that this problem can be solved by just convincing other people, protesting for animal welfare and maintaining a vegan diet. The solution is already under way and will be adopted merely by market forces and disruption. Different substitutes for animal products are rapidly falling in cost and prices. This disruption will take place in the first half of the 21st century and is probably much better than the more radical solution of banning animal products. The latter would likely destabilize our culture and not stop animals from being harmed and killed but instead lead to people slaughtering them illegally—much like what happened with alcohol consumption during Prohibition in the United States in the early 20th century. Supporting the emerging industries of substitute products is probably the best we can do today to accelerate this transformation. 



### Negative Utilitarianism and the Trolley Problem

I argued that negative utilitarianism is best understood as a practical guideline rather than a fundamental ethical principle. The principle it proposes is:

An act is right if and only if it minimizes the total sum of suffering.

The corresponding positive principle would be:

An act is right if and only if it maximizes the total sum of well-being.

But the trolley problem and the more extreme transplant problem demonstrate why negative utilitarianism cannot serve as the ultimate ethical foundation. 
Here's the original Trolley Problem:

"Edward is the driver of a trolley, whose brakes have just failed. On the track ahead of him are five people; the banks are so steep that they will not be able to get off the track in time. 
The track has a spur leading off to the right, and Edward can turn the trolley onto it. Unfortunately there is one person on the right-hand track. Edward can turn the trolley, killing the one; or he can refrain from turning the trolley, killing the five." (Thomson 1976, 206) 

And here's the original transplant problem:

"Donald is a great diagnostician. Five of his patients are dying. By chance Donald learns of a healthy specimen such that if Donald cuts him up into bits, a peculiar physiological process will be initiated in the five, curing them. 
Donald can cut his healthy specimen up into bits, killing him, thereby saving his patients. Or he can refrain from doing this, letting his patients die." (Thomson 1976, 214) 

At first glance, negative utilitarianism might suggest that we should minimize the number of deaths, since one person dying results in less immediate suffering than five people dying. However, this view fails to account for the total suffering involved. 
The five survivors in these scenarios would likely experience suffering—both from the traumatic experience and from ongoing difficulties in their continuing lives. Their collective suffering could easily exceed that of the single individual, if its life continued. 
The problem becomes even more pronounced when we scale up the numbers. If a thousand people are in danger, negative utilitarianism would suggest that their combined suffering would far outweigh that of a single individual and should be eliminated. 
This logic leads to an obviously unethical conclusion: that systematically eliminating as many people as possible would be the best action to take. Clearly, this contradicts what we would truly want to do if we had a perfect grasp of everything involved. 
This conclusion arises because it completely neglects the fact that there is a desirable state of mind, we call well-being and not just an undesirable state of suffering. To optimize the experience of the universal subjects, we need to think about the desirable and undesirable.

### Team Formation

Generally, team formation is seen as a positive development, as it unites people behind a common cause. The mindset associated with this is called the 'soldier mindset' by Julia Galef. However, team formation could just as easily be seen as creating division and exclusion. 
Beyond practical purposes, it leads to self-deception through motivated reasoning. People begin to integrate these causes into their personalities, so that any attack on these feels like an attack on their entire personality, leading to polarization and radicalization. 
Galef therefore recommends adopting the 'scout mindset' to benefit from practical team formation while embracing truth as the team’s guiding cause, countering motivated reasoning. If we align ourselves with seeking truth, there are no enemies to our team. 
Any other team formation necessarily ignores that what one does to the enemy, one ultimately does to oneself, since we are ultimately the same subject. Political, religious, or economic team formation, beyond its practical uses, should therefore always be seen as unhelpful.



### Moral Decision-Making under Uncertainty

If there are genuine moral principles, their prescription will inescapably clash with the uncertainty of an actions future consequences:
"If it is always wrong to kill an innocent person (even to save more innocent people), how do we evaluate actions that carry a risk (perhaps minuscule) of killing an innocent person? If there is a moral obligation (all else being equal) not to bring bad lives into existence, but no obligation to bring good lives into existence, what do we say in situations where it is uncertain whether some potential future life will be bad or good? And what if we are uncertain about the relevant moral principles themselves?"

Every action we take carries some moral risk. Driving a car risks harming others, speaking risks influencing others negatively, going outside risks injury, while staying inside risks misery. Moral risk is unavoidable. If strict moral obligations existed, we would never be able to determine the right action—any nonzero probability of violating an obligation would necessitate avoiding that action, leading to total paralysis. If we were obligated to prevent the death of any innocent person, we would also be obligated to avoid any action that could possibly contribute to such a death—which includes any action at all.

Fortunately, universalism frees us from rigid moral obligations. Ethical frameworks based on obligation have practical utility but lack true objectivity. Under universalism, moral failure is not a crime but rather a form of foolishness. Universalism resolves the paralysis problem by replacing moral duties with an evaluation of actions as wise or unwise. Acting wisely means making justified decisions based on the best available reasoning, approximating what we would do if we had a perfect grasp of all relevant factors. However, if an action ultimately results in suffering, then—even if it was justified—it was still a mistake.


A greater challenge for a consequentialist ethical framework is handling uncertainty in expected well-being. While we reject deontological moral principles, we still hold that well-being is an objective goal to aim for. Since we cannot achieve an actual perfect grasp, nor would we want to achieve it, we can only hope to approximate what we would want to be doing if we had it, by reasoning through any given situation.
"Orthodox decision theory advises expected utility maximization as the rational response to uncertainty. But expected utility maximization may seem infeasible in ethical contexts due to the complexity of the morally significant effects of even ordinary choices (Lenman 2000; Greaves 2016). It may give implausible weight to small probabilities of astronomically good or bad outcomes, like human extinction (Balfour 2021)."

If we had a perfect grasp, we would simply act in accordance with the best possible outcome because we would have complete knowledge of all consequences and all possible worlds. Lacking this, we approximate by reasoning under uncertainty.  But what’s the best approximation? Since we cannot really know the probabilities of future consequences, any action could also have the opposite of our intended effect. We must account for the butterfly-effect and therefore cannot choose specific actions because of probability considerations. Even if the probability of events of astronomical suffering isn't zero, we cannot prescribe any actions on that basis, because we cannot know their consequences. Another confusion about probability reasoning is that our decision today will inevitably determine the probability of its consequences. This omits the fact that, if my decision can influence the consequences, all my future decisions will also influence the consequences. But then, my decision today will not determine the probability of its consequences, it will only slightly influence them. My next action could completely change those probabilities. 

Since the perfect grasp would not be paralyzed by tiny probabilities, it would likely advise us to act in ways that are robustly good across multiple reasonable models. By focusing on actions that are robustly good, rather than trying to optimize all of your actions, you will increase your probability of alignment with the perfect grasp. Obviously you should still avoid actions that predictably lead to large suffering or risk catastrophic failure. Be sceptical of overfitting decisions to extreme, unlikely scenarios, because this often comes at a cost of more probable and pressing concerns, which, if neglected, could cause the extreme scenarios to become more likely. Probability considerations beyond a certain improbability must be discarded as absurd, because otherwise I can always justify immoral actions by appealing to hypotheticals about a 10^-1000 % probability of 10^1100 beings in extreme suffering, seemingly making this a big deal to think about such scenarios. But the probability here is best approximated to be 0, because the probability itself must have a certain probability distribution about its correctness, which must also have a high probability that 0% could be the correct probability for such an event happening.


## Control


Universalism and incompatibilism share many counter-intuitive ethical consequences, making it highly improbable for these concepts to have any role in a hypothetical science of ethics. As a result, consequentialist-utilitarian ethics is way more persuasive. 
If both incompatibilism and universalism are true, then concepts such as blame, retribution, hate, obligation, and responsibility are rendered incoherent or unethical.
Under universalism, obligation is untenable because obligation presupposes a relation between distinct individuals. 
If I inflict harm upon myself, I do not have an obligation to myself to cease my own suffering. But given that I am everyone, it follows that I cannot have obligations toward others, since there are no true "others" distinct from myself. 
But all the other listed concepts, blame, retribution, and responsibility, ultimately depend on the coherence of obligation. If no one has obligations, then it is impossible to blame anyone for harming another in the retributive sense, for the blamer and blamed are the same. 
To blame another would be equivalent to blaming myself for harming myself. While I may regret my past actions, I would not, on that basis, intentionally inflict further suffering upon myself as a form of retributive response. 
Thus, for the universalist, both blame and retribution are irrational.
If obligation is incoherent, then responsibility is equally untenable. Responsibility presupposes that an agent is accountable for their actions in a way that distinguishes them from the actions of others. 
But if I am everyone, then I would necessarily be accountable for all actions universally. This kind of responsibility, one that depends on the attribution of actions to the personhood of persons, is nonsensical. 
The only entity to which an action can meaningfully be attributed is a particular intention corresponding to a particular conscious brain.

Incompatibilism leads to the same conclusion through another line of reasoning. 
Responsibility is only possible in a substantive sense if libertarian free will exists, that is, if responsibility entails causal sourcehood or control, rather than mere conscious identification. The latter is insufficient for blameworthiness. 
If I consciously identify with the intentions that led me to harm another, I am still nothing more than an observer of those intentions. My identification with them is simply a consequence of the physical processes within my brain, over which the observer has no ultimate control. 
But a helpless observer cannot be held morally responsible for anything. The agent who performed the action—like a falling tree, following physical laws, causes harm—may be causally responsible for the resulting suffering, but cannot be subject to blame or punishment. 
If one would not punish a baby who unintentionally causes harm, or a person whose violent actions are the consequence of a brain tumor and who ceases to act violently upon its removal, then no one can justifiably be punished, since everyone is equally powerless in their actions. 
If responsibility collapses, then all associated deontological concepts collapse as well. Retribution becomes incoherent because punishment becomes arbitrary. We do not punish a tree for falling, for it can't do anything against its descent; 
likewise, no person has true control over their actions, and thus retribution cannot be meaningfully applied.

Certain additional moral concepts become incoherent from the standpoint of universalism, even though they remain compatible with incompatibilism. 
For instance, the concept of justice as fairness is flawed. Fairness presupposes a demand for equality between distinct persons. If two players engage in a game, the game is fair only if both abide by the same rules. 
However, for a universalist, every game is ultimately a game in which I play against myself. If I play a game where I pretend to be two separate players, A and B, it is ultimately irrelevant which player wins, because I am both. 
Similarly, concepts such as ownership and theft become incoherent, for one cannot steal from oneself. This can be illustrated through the time-traveler analogy: Suppose my future self from tomorrow travels back in time and claims ownership over my present possessions. 
Who rightfully owns them, the present self or the future self? Since both are the same person, both own the possessions. But, under universalism, because everyone is the same person, it follows that everyone would own all possessions, including mine. 
Consequently, ownership must be attributed to something else about the agent other than the person, if it wants to remain a coherent concept. Yet, as we have established, the agent was never free in any meaningful sense, bringing us back to incompatibilism. 
Here, the concept of deservingness is rendered incoherent, though not necessarily under universalism. We cannot deserve anything, for we've never been in control of our actions. While ownership may have pragmatic utility, it can't be justified on the basis of desert either. And it cannot be justified on the basis of personhood. The way I would frame practical ownership is in terms of dibs, making it a special game with specific rules for what happens to agents who break the rules. There isn't anything inherently immoral to break the rules of a made-up game, as long as no suffering is caused and well-being is increased, but it must be noted that breaking agreed upon rules can be detrimental to one's integrity and might reinforce further unhealthy and immoral behavior. 

By the same reasoning, meritocracy can at best be pragmatically, but never morally, justified, if we define it as: "If initial chances are equal, the winner deserves the gain", since deservingness isn't real. Being rewarded or punished for our talents can never be ethically justified, as we never had the ability to decide what our talents would be, we just have to accept them. In practical terms, rewarding and punishing talent can certainly be justified. However, in doing so, one no longer refers to the intrinsic value of talent but rather to its external consequences—namely, that this talent will achieve the best possible results in relation to a specific goal.
A naturally gifted marathon runner may put significantly less effort into improving their endurance than a biologically disadvantaged runner, yet in a meritocracy, they would still deserve the victory if they achieved the better result. This reveals that meritocracy operates on a different principle than meritocrats initially assume. In reality, they are not concerned with rewarding hard work itself but with achieving the best possible results. This entirely excludes the notion that meritocrats see work, effort, or dedication as intrinsically valuable.

The truth of these claims is easier to recognize, if we think about the long-term future of humanity. We are already moving in this direction of giving up ownership and we'll soon need to rethink meritocracy, when the merit of AI outpaces all of us.


### The Compatibilist Confusion

I think the compatibilist confusion originates from two distinct concepts of control as foundations for free will. To distinguish these, I will refer to the first type as regulatory control and the second as ultimate or true control. 
Regulatory control is the kind of control central to control theory in engineering. For example, if a thermostat regulates temperature, it can be said to have regulatory control—there is a causal dependency of temperature on the thermostat. 
It has a given control loop and a certain response function to a given temperature input. The output causally depends on the input.

Ultimate control, on the other hand, is equivalent to determination, where "to determine an action" means to be the ultimate cause of that action. 
The Consequence Argument easily demonstrates that this kind of control is impossible. If determinism is true, then events from a million years ago already determined my actions, meaning I cannot have been their ultimate cause and therefore lack true control. 
Peter van Inwagen's argument is even more precise:

"If determinism is true, then our acts are the consequence of laws of nature and events in the remote past. But it's not up to us what went on before we were born, and neither is it up to us what the laws of nature are. Therefore, the consequences of these things (including our present acts) are not up to us" (p. 56).

Van Inwagen makes the great point that there are seemlingly touchable and untouchable facts, that is, facts in our control and facts we cannot do anything about. The latter would include that 1+1=2 or that 317 is a prime number or that dinosaurs existed or that opposite charges attract each other. Seemingly tochable facts would be the decision what to eat on a particular day or what to say in a conversation. But if there is some proposition P, which is untouchable, and Q follows from P, then Q must also be an untouchable fact. Since determinism holds that for any state Q there is a preceding state P that causally determined Q, then, since whatever happened millions of years ago is an untouchabel fact, any state of the universe after that is an untouchable fact, without any ultimate control on our part.

What would be required for such ultimate control though, is not an indeterministic process (as some might claim), since I would not be the cause of that either. 
Rather, what's needed is complete independence of my functional outputs from the inputs I receive. To be truly in control, I must be able to decide independently of my inputs what to do. If my outputs are determined by my inputs, then I am not the ultimate cause of my actions. 
Only if I could originate an event uncaused by the laws of physics would I truly be the ultimate cause. But since this is impossible, free will of this kind is likewise impossible.
Compatibilists can attempt to salvage free will by weakening the requirement of independence. 
However, in doing so, they recover only regulatory control, which is not what free will is truly about. Daniel Dennett, for instance, argues that if our complex control loops function well, then we have free will. 
But this is no different from a thermostat functioning well and claiming that it, too, has free will.
The primary motivation for compatibilism is to preserve moral responsibility, as it may seem that without free will, responsibility collapses. So, are thermostats blame-worthy? 
Compatibilists might also argue that “if the agent had wanted to do otherwise, then he would have done otherwise” is sufficient for free will. However, this is clearly inadequate, since—if determinism is true—the agent could not have wanted to do otherwise. 

Then there's the Principle of Alternative Possibilities (PAP):

P1: An agent is responsible for an action only if the agent could have done otherwise.

P2: An agent could have done otherwise only if causal determinism is false. 

Conclusion: Therefore, an agent is responsible for an action only if causal determinism is false.

Harry Frankfurt challenges the first premise by arguing that alternative possibilities are not necessary for moral responsibility. 
His Frankfurt cases have their own problem (e.g. the two-horned dilemma). But even if we reject PAP, this is not sufficient as an attack on the actual free will principle: An agent is responsible for an action if and only if they are its ultimate cause. 
Since Frankfurt cases do not establish ultimate control, they fail to preserve genuine responsibility.

Does this mean that responsibility does not exist? Yes. However, this does not contradict preventionist punishment, and accountability remains intact, too. 
We can still assign actions to agents and hold them accountable, even if they are not ultimately responsible for them.
The condition for accountability can simply be that the agent consciously identifies with their actions at the moment for which they are held accountable. 
If I were asleep or hypnotized when performing an action, I would not be truly accountable because I was not conscious of it. In essence, an agent is accountable for an action if, at the moment of action, they recognize themselves as accountable for that action. 
The moment-condition is necessary to prevent cases where external manipulation, like implanting a chip to alter an agent’s sense of responsibility afterward, could falsely absolve them of accountability. To be accountable, one must have felt accountable at the moment of interest.

### Accountability

I argue that compatibilists are not actually debating free will or what it truly means to be responsible for an action, but rather the concept of accountability.

If we frame the discourse this way, there are actually interesting arguments about what qualifies as morally accountable. 
For instance, John Martin Fischer subscribes to the idea that moral responsibility requires what he terms guidance control, as opposed to what he terms regulative control (not to confuse with what I termed regulatory control). 
Regulative control would necessitate genuine metaphysical access to alternative possibilities, which implies causal independence between inputs and outputs. If determinism is true, such causal independence is impossible. 
(The reasoning is that causal independence necessitates alternative possibilities, but alternative possibilities do not necessitate causal independence. A truly indeterministic process would also enable an agent to have done otherwise, without the agent being the ultimate cause.) 
Fischer’s concept of guidance control is then essentially what I have termed regulatory control, and his argument for moral responsibility is, in reality, an argument for moral accountability. Even though he refers to it as responsibility, I will now call it accountability. 
His concept of accountability requires "reasons-sensitivity of the appropriate sort and mechanism ownership." What he calls mechanism ownership corresponds to what I describe as consciously identifying with one's actions in order to be accountable for them.
Fischer explains: 
"One’s mechanism becomes one’s own in virtue of one having certain beliefs about one’s own agency and its effects in the world, that is, in virtue of seeing oneself in a certain way. 
(Of course, it is not simply a matter of saying certain things—one actually has to have the relevant constellation of beliefs.) In my view, an individual becomes morally responsible in part at least by taking responsibility." 
The requirement of reasons-sensitivity is a crucial addition, as it prevents accountability in cases of brainwashing/manipulation.

Alfred Mele critiques the Fischer-Ravizza approach with several counterexamples: 
For instance, a person who believes they had no choice, yet still sees themselves as responsible, would not actually be responsible according to the approach. However, if we are talking about accountability, this is not a problem. 
One only needs to identify with one's actions to be accountable, not necessarily perceive oneself as responsible or as the source of those actions. Even an addict who does not truly want to act on their addiction must still, in some way, identify with their actions. 
Only dissociative disorders could pose a challenge to accountability of this sort.
Another counter from Mele involves a person with a phobia, where one fear overrides another and leads to an immoral decision—one that would not have occurred if events had unfolded differently. 
Such a person is still reasons-responsive but might not necessarily be considered morally accountable due to their phobia. 
Fischer responds by arguing that reasons-responsiveness can be divided into different spheres of responsiveness, where phobia-driven actions would be placed in an outer sphere, potentially lacking sufficient responsiveness for moral accountability. 
I do not think this distinction is necessary and would maintain that phobias do not negate accountability. Like the Stoics, I believe that yielding to one's emotions is a choice rather than an inevitability, as Dennett might have also pointed out. 
Ultimately, I'd conclude that Fischer’s approach to accountability is essentially correct:
An agent is accountable for their actions if and only if they consciously identify with their actions and are appropriately reasons-responsive at the moment of action. 


### Stoicism and Compatibilism

Stoicism is not often framed this way, but it is fundamentally a philosophy based on compatibilism. To the extent that this is possible, the Stoic view largely aligns with my view. The Stoics fully embraced determinism. 
Stoicism holds that one can only genuinely control one’s own thoughts, and that it is our thoughts, not external circumstances that we should focus our attention on. While we do not have true control over our thoughts, we possess a crucial form of regulatory control: 
the ability to anticipate, reflect, and reason through different possibilities. Unlike a simple thermostat, which passively responds to inputs, the human brain can reason through and weigh different outcomes as well as regulate its own regulatory processes. 
This meta-control grants us a degree of independence or agency that a thermostat cannot achieve. It doesn't give us the independence needed for true control, but it can be seen as approaching such independence in the form of autonomy. 
Imagine a scenario in which your brain is placed in a vat and stimulated to experience a virtual reality where you exist in a white room with only two values displayed on a screen: the outside temperature and the inside temperature. 
You can regulate only the inside temperature, essentially making you a thermostat. In this case, your brain no longer receives direct input from the real world and is, in a sense, independent of it. 
While external conditions (the outside temperature) still influence your decisions, most of your reasoning about how to adjust the inside temperature arises from internal cognitive loops, rather than direct external stimuli. 
Your behavior is still physical and therefore causally determined without any true control on your part, but you still possess significantly more independent autonomy in your immediate environment than a thermostat. 
Stoicism is fundamentally a philosophy of self-regulation within a deterministic universe.

The stoics recognized that value and control are nearly indistinguishable. One is the the motivation, the other is the action of desire. 
In other words, we attempt to control whatever we value, because valuing something inevitably leads to desire, and we cannot help but act on our desires, unless a stronger competing desire restrains us. 
The search for free will or for control becomes, in essence, the search for what one truly values. The Stoics recognized this and built their ethics around the idea that eudaimonia comes from properly aligning our values with what is actually within our regulatory control. 
This is why Stoicism emphasizes the cardinal virtues, wisdom, justice, courage, and self-control. These virtues are not arbitrarily chosen; they are precisely those qualities that help refine and optimize our self-regulation. 
As Albert Ellis said, mistakes are almost always based on ignorance, stupidity, or emotional disturbances. 
The value of wisdom is intended to shield us from ignorance and stupidity, justice is meant to guard us against the selective thinking driven by our intrinsic selfishness that disregards the fate of others (a similar suitable virtue of this category would be empathy). 
Courage protects us from making mistakes out of fear, and self-control shields us from mistakes driven by irrational passions. 
In other words, the Stoic ideal is not about having "free will" in the metaphysical sense but about achieving optimal self-regulation (mental health), controlling what can be controlled and ceasing to desire control over what cannot.

### Against Retributivism

Arnold Zuboff has also argued against retributivism and for incompatibilism. 

He writes: "Think of an evil agent, one who deserves punishment. Think of the man who put pieces of glass in baby food in order to extort money from the baby food company. 
Now imagine you become convinced that this evil man had been assembled, the night before he launched his disgusting scheme, by an advanced sort of Frankenstein, a mad scientist capable of constructing from the raw materials of living things not a mere automaton, 
but a completely human-like being, with a brain supporting a mind that has in it the same pattern of conscious and unconscious mental life that we would have assumed this man had when we started thinking about him as our example of an evil agent. 
Since we have stipulated the internal sameness of this man with the man as he would be in the normal case, he himself will not know he was manufactured by the scientist. He has a complete set of memory impressions of having been a child, of a bank balance, etc., 
as well as all the same motivation he would have had in the natural case. So he does the awful deed. Now what do we think of him? My reaction is to think he is bad and ought to be stopped and, if this is possible, changed. 
What I no longer seem to feel is that he deserves to suffer, great pangs of guilt as well as what we decent people will do to him. I no longer feel that there’s an intrinsic good, a good apart from such possible consequentialist goods as reform or deterrence, 
in his feeling the pain of punishment because in some deep sense he deserves the pain. I see now too clearly for that that he is merely a product, with a nature chosen not by him but his manufacturer. [...] 
When I consider my reaction to thinking of this agent as a product of the mad scientist, it seems to me that my usual retributive feeling has drained out of me. Someone might be tempted to feel that the scientist deserves the punishment for choosing this evil nature 
for his creature. And, just for the fun of it, I could next ask the one who thought of this to imagine that the scientist too had been, without his knowledge, similarly created the night before he did his work by yet another such scientist. 
But this just delays our coming to the real point: what possible moral relevance can there be in the difference between the normal, actual case and this science fiction one? In contrast to our artificial man, the natural man in the actual case was produced 
by a less organized set of causes--heredity, environment, maybe initially God. But he, just like our artificial man, simply must act according to what he is--according to a nature which is internally indistinguishable between the two cases. 
If we try thinking that, be he natural or artificial, some of his actions may have been somehow uncaused, this would only threaten to remove from him even the sort of responsibility that bad weather may have for a bad harvest. 
Yet we are all vulnerable to the illusion that a thinking being cannot really be limited by his own nature--that he is always free to rise above it and improve upon it. 
Of course, thank God, a person can resolve to improve himself and can do so, but surely this will only happen when that was in his nature, in that very self-reflection that must seem to the agent to be indeterminate. 
I contend that no agent can be conceived of that could ever be responsible in the way required by retributivism. First, no agent could be responsible in any sense for his own beginning. In particular, he could not before he existed have chosen his own nature. 
[...] And even if, like God in the ontological argument, he existed with necessity from his own nature alone, he would have had no choice in either his existence or his nature. Next, only as far as he himself shapes his later nature can he be thought responsible in any sense 
for that, but such self-improvement or self-corruption cannot make him responsible for his nature in the way required for deontological desert. For, apart from either external causes or a metaphysical spontaneity for which nothing could be responsible, 
this self-determining can only depend on his original unchosen nature or later developments of that. Finally, the agent cannot be held responsible in the required sense for any of his actions, whether these are thought of as determined by the nature he essentially did not choose 
or as bubbling forth somehow undetermined and thus with nothing responsible for them in any sense. I conclude we should cure ourselves of this mode of judgment that has caused such great suffering in the bitterness of the judge as well as in the pain of the needlessly punished. 
But what of the Kantian view, already mentioned, that the agent when he acts morally, since he is then purely rational, can be free of all the particular, contingent conditions that seem to have formed him? 
Kant certainly thought that this sort of autonomy invested an agent with the responsibility required for deontological desert (although it seems to me this could authorize only reward, not punishment, since to be rational and autonomous was to be good). 
I think this fails, because if the agent’s motivation is somehow fixed timelessly by the forms of rationality, this is not freedom but necessity--it involves no choice. 
Neither could the agent, in the formation of his nature, have chosen either whether he was to be capable of rationality or whether he was to be disposed much to use it. 
(These same points apply to the idea that what we are entitled to punish retributively is the evil nature of the agent. Anyway, we can’t either punish or sensibly blame for being evil the property of being evil.)"


### Moral Luck

In Kantian tradition, Thomas Nagel introduced the moral Control Principle: "We are morally assessable only to the extent that what we are assessed for depends on factors under our control." This principle is introduced to prevent our moral judgement from praising or blaming a person for something they were just lucky or unlucky to be involved in.

"For example, if we find out that a woman who has just stepped on your toes was simply pushed, then our temptation to blame her is likely to evaporate. It seems that the reason for this is our unwillingness to hold someone responsible for what is not in her control. Similarly, if two drivers have taken all precautions, and are abiding by all the rules of the road, and in one case, a dog runs in front of the car and is killed, and not in the other, then, given that the dog’s running out was not something over which either driver had control, it seems that we are reluctant to blame one driver more than the other. Although we might expect different reactions from the two drivers, it does not seem that one is deserving of a worse moral assessment than the other."

But, as Nagel says: "Where a significant aspect of what someone does depends on factors beyond his control, yet we continue to treat him in that respect as an object of moral judgment, it can be called moral luck” (Nagel 1979, 59). Nagel establishes four kinds of luck in opposition to control, but for our discussion it suffices to talk about causal luck. Nagel himself acknowledges that the discussion about causal luck is equivalent to the debate about free will. As I have argued, there is no free will of the sort needed for moral responsibility. Nagel agrees:

"I believe that in a sense the problem has no solution, because something in the idea of agency is incompatible with actions being events, or people being things. But as the external determinants of what someone has done are gradually exposed, in their effect on consequences, character, and choice itself, it becomes gradually clear that actions are events and people things. Eventually nothing remains which can be ascribed to the responsible self, and we are left with nothing but a portion of the larger sequence of events, which can be deplored or celebrated, but not blamed or praised" (1979, 68).

We must ultimately reject the Control Principle and the Kantian conception of morality that embraces it. We must accept that every action and its consequences are ultimately a matter of luck, even if it falsely appears that we have ultimate control over some of our choices. There are, of course, other definitions of luck, for instance not based on control but on coincidence (probability of coinciding events). Me, expecting heads, in a fair coin-flip, coinciding with the coin actually landing heads can be considered luck. If some moral action would depend on this coincidence, this would count as moral luck, quite apart from control, because even though the coin flip might be deterministic, it will still land heads only 50% of the times it is tossed. The kind of moral luck going along with this, is defined by the probability of some matching patterns of events. One may say to have moral luck, if there was such a coincidence involved. Since the probability that a person acts immoral without reason is very low, it is not seen as moral luck if the person behaves morally, but it is morally unlucky if the person happens to act immoral without reason. The woman, in the example above, stepping on your toes, makes you morally unlucky, because it is an improbable coincidence that the woman acts this way (without reason, only because she was pushed). This kind of luck remains, but the opposite of this kind of luck is not control anymore, since a high probability of an event occuring doesn't entail control over that event. True control plays no role in genuine moral assessment. Only conscious states of well-being and suffering have moral significance, and actions can be evaluated based on whether they contribute to one or the other. It is entirely irrelevant whether an action was free, lucky, controlled, or accidental—what matters is its outcome. If an action produces well-being, it is good; if it causes suffering, it is bad. Beyond this, we cannot make further moral judgments about the person performing the action, other than acknowledging their accountability. A person who causes suffering is not evil but merely accountable for a harmful action, much like a thermostat can be said to be responsible (only in a regulatory sense) for maintaining a comfortable or uncomfortable temperature. The key difference is that a person is conscious and responsive to reasons, whereas a thermostat operates mechanistically. This distinction makes the person morally accountable for an effect, while the thermostat can only be attributable to an effect.


### Superdeterminism

First, let me cast some doubt on the Many-Worlds Interpretation of quantum mechanics. The Many-Worlds Interpretation (MWI) posits that measurements cause the universal wavefunction to branch into distinct, non-interacting worlds. While this explains why observers perceive a single outcome, it does not account for why outcomes follow the Born rule’s probability distribution.
One attempt to derive probabilities in MWI appeals to self-location. The reasoning behind it is known as the Epistemic Separability Principle (ESP), defined as follows:

"Suppose that universe U contains within it a set of subsystems S such that every agent in an internally qualitatively identical state to agent A is located in some subsystem which is an element of S. The probability that A ought to assign to being located in a particular subsystem X ∈ S given that they are in U is identical in any possible universe which also contains subsystems S in the same exact states (and does not contain any copies of the agent in an internally qualitatively identical state that are not located in S)."

In other words, if two different universes contain the exact same set of possible places where you could be (and don't have extra copies of you elsewhere), then your probability of being in any particular place should be the same in both universes. Nothing outside of those places should influence your probability judgment. For example, suppose we played in game in which you were awakened in one of ten identical rooms. Your probability of being in room 7 is 10%. ESP would tell us that our probability of being in room 7 must then be 10% in any universe in which the exact same game has been played. Regardless of how many worlds the game is being played in, the probabilities do not reflect that because the probabilities of being in a particular room are the same in each world. This implicitly endorses the type view of personal identity across branches, while it endorses a token view of personal identity inside branches. The latter follows from Sbens and Carrolls following statement:

"In classical physics, you experience self-locating uncertainty when, for example, the universe
is so large that you should expect there to exist a distant planet where someone is having
the exact same immediate experiences that you are, or when the universe survives so long
that you should expect short-lived Boltzmann brains to pop out of the vacuum in the exact
subjective state you are in now, or when, as in Duplicating Dr. Evil, you have reason to
think someone has purposefully created a duplicate of you. Such phenomena can also occur
in quantum mechanical contexts, leading to within-branch uncertainty."

Self-locating uncertainty assumes that, even though the experience of you and your duplication might be the same, they are somehow distinct, even before they are actually differentiated.

"Sebens and Carroll argue their case for ESP-QM based on a specific example of a quantum process, which they call the ‘once-or-twice example’. In this example, Alice and Bob each have a spin-1/2 particle both of which have been prepared in the x-spin up eigenstate. Alice measures spin in z-direction of her particle without looking at the outcomes yet, but Bob does look at the outcome and measures spin in x-direction of his particle if the outcome of Alice’s measurement was +1/2. On Sebens and Carroll’s view of Everettian QM, branching occurs across the entire wave function if and when Bob makes his measurement. Therefore, inasmuch as branches are precisely defined, after Bob’s measurement there are two branches in which Alice’s outcome is +1/2, but only one branch in which it is −1/2. Branch counting thus recommends that Alice ascribe a probability of 2/3 to being in a +1/2-branch, whereas the Born rule recommends ascribing a probability of 1/2.
Branch counting thus is at variance with the Born rule in the given case."

The ESP would instead recommend to view these experiments as distinct. Bobs measurement cannot influence the probability of Alice's outcome, because they are in different subsystems. The state of the Alice + detector system is unchanged when Bob performs his measurement on the branch in which the spin in x-direction of Alice’s particle is +1/2. The probability that Alice ought to assign to being located in subsystem X (which is a particular subsystem where her measurement is +1/2) is identical in any possible universe, which contains the subsystems S in the exact same state (the +1/2 state). Therefore, it must also be identical when Bob created further branches. It will always remain 1/2. This principle, according to Sebens and Carroll, enables us to derive the born-rule, which we'll not retrace here. Instead, I will critique this principle, based on its incoherence. Apart from the fact that you cannot endorse the type- and token-view of personal identity at the same time, endorsing the type-theory across branches doesn't even work on its own.

If we adhere to the correct understanding of personal identity and apply the type view of identity consistently to the MWI, the hypothesis becomes incoherent.
Since an observer’s experience is identical across all worlds where, for example, a particle was reflected by a beam-splitter (and thus detected at detector 1) and across all worlds where it was transmitted (and thus detected at detector 2), these worlds form two indistinguishable “bundles” of branches, over which the observer is now distributed, meaning that there are two different observers with their different experiences, regardless of how many branches are contained in each bundle. This suggests that the observer, upon measurement, must assign equal probability (50%) to each bundle, because he can either find himself in bundle 1 or bundle 2. The EPS advises us to do the same, because it relies on qualitative differences in experience to differentiate which bundle of branches someone is located in (that is, it endorses the type-view of personal identity across branches).

However, this approach fails in asymmetric cases. For example, an imperfect beam splitter might reflect a photon 33% of the time and transmit it 67%. One must still assigns equal probabilities to each resulting bundle, getting a 50-50 split, because there are still only two differentiated observers created by this measurement. But this cannot explain our observing reflection 1/3 and observing transmission 2/3 of the time. This demonstrates that self-location reasoning does not track quantum probabilities.

Another problem for the MWI is the measurement problem. The measurement process in quantum mechanics is a non-linear process, while the Schrödinger equation is a linear equation. The MWI must invoke an equivalent assumption to explain the observation of a definitive state. The Schrödinger equation itself will only ever describe probability amplitudes, without producing observables. A superposition of initial states will only result in a superposition of eigenstates. Choosing one of the eigenstates just reintroduces the measurement problem. The measurement process is completely incompatible with the Schrödinger equation. Sabine Hossenfelder writes:

"In the many worlds interpretation, if you set up a detector for a measurement, then the detector will also split into several universes. Therefore, if you just ask “what will the detector measure”, then the answer is “The detector will measure anything that’s possible with probability 1.” This, of course, is not what we observe. We observe only one measurement outcome. The many worlds people explain this as follows. Of course you are not supposed to calculate the probability for each branch of the detector. Because when we say detector, we don’t mean all detector branches together. You should only evaluate the probability relative to the detector in one specific branch at a time.
That sounds reasonable. Indeed, it is reasonable. It is just as reasonable as the measurement postulate. In fact, it is logically entirely equivalent to the measurement postulate. The measurement postulate says: Update probability at measurement to 100%. The detector definition in many worlds says: The “Detector” is by definition only the thing in one branch. Now evaluate probabilities relative to this, which gives you 100% in each branch."

Probably the greatest issue with the Schrödinger equation is its inherently complex nature. It is often suggested to quantum mechanics students to shut up and calculate, and to not even try understanding the actual nature of quantum mechanics, because the Schrödinger equation is nothing that could be measured. Such a process, it is assumed, determines what is physically actualized. The statistical nature of quantum mechanics should have pointed us toward the obvious conclusion that the Schrödinger equation is not fundamental, but must be a result of a more fundamental deterministic process. 

Sabine Hossenfelder similarly concludes: 

"In its density matrix
form, the Schrödinger equation is remarkably similar to the classical Liouville equation. So
much that, in this form, the Schrödinger equation is sometimes referred to as the Schrödinger-
Liouville or Quantum-Liouville equation, though the historically more correct term is the von
Neumann-Dirac equation [...] The classical Liouville equation is linear in the probability density due to conservation
of probability. But this linearity says nothing whatsoever about whether the dynamics of the
underlying system from which the probability density derives is also linear. Hence, for example,
chaotic dynamical systems, despite their nonlinear dynamics, obey the same linear equation for
probability density. To us, this close formal similarity between the two equations strongly
suggests that quantum physics, too, is only the linear probabilistic description of an underlying
nonlinear deterministic system.
From this point of view, pursuing an Everettian approach to quantum physics is not the
right thing to do, because this idea is founded on the belief that the Schrödinger equation is
fundamental; that nothing underpins it. Moreover, it does not make sense to just append nonlinear dynamics to the Schrödinger equation in situations when state decoherence becomes
non-negligible, because it is not the Schrodinger equation itself that needs to become nonlinear."

One further problem of the MWI is its incompatibility with the theory of relativity, because it violates local causality. Sabine Hossenfelder explains:

"Remember that the reason the Copenhagen Model violates Local Causality is that a measurement in one location can reveal information about a measurement in another, space-like
separated region, and that this information could not be obtained from the wave-function.
Well, the only information we have in the Many Worlds Interpretation is also that in the
wave-function, and for what our observations are concerned, it gives the same predictions as the
Copenhagen Model. Hence, it also violates Local Causality. You can believe in as many other
universes as you wish, making a measurement on one end of the wave-function will de facto
reveal something about the outcome on the other end. If you, for example, know that a particle
was measured in one place, you know it wasn’t measured in another place. This, combined with
the impossibility of predicting the measurement outcome from the wave-function alone, makes
the Copenhagen Model non-local, and the Many Worlds Interpretation changes nothing about
that.
I believe the reason that Many World supporters are confused about this point is the following. If you think that the origin of violations of Local Causality in the Copenhagen Model is the
Collapse Postulate, and you discard of the Collapse Postulate, then what’s left should be local. However, the non-locality of the Copenhagen Model does not come from the Collapse Postulate per se, it comes from the lack of information about measurement outcomes in the wavefunction, combined with the fact that making a measurement in one place reveals something
about another place. The Collapse Postulate is just how this increase in information is taken
into account in the Copenhagen Model. The Many Worlds Interpretation doesn’t change anything about the origin of the violation of Local Causality, hence it violates Local Causality the
same way. It does not help that [...] the word “locality” in the context of
Many Worlds Interpretations has been used with different meanings.
Neither does it help to add more complicated stories about how the branching occurs. The
local branching idea has it that the
splitting of the worlds obeys the speed-of-light limit. For this reason, a measurement in B in does not split the world in A instantaneously. It is only when the two come into causal
contact and observers from both places can compare their measurements that the split must have
been completed.
But for us instrumentalist this story is irrelevant. We take the measurement results from A
and B and apply our definition of Local Causality. It is violated, and that’s that. Whether you
think there were multiple different universes with different outcomes at A before we compared
both records is irrelevant for the fact that the records which we do have in our universe show
these correlations. And since there is not enough information in the wave-function to predict
A without the information from B, Local Causality is violated. Stories about local branchings
do not matter. (Unless the branching contains additional information about the measurements
that will be made, in which case Local Causality can be restored by violating Measurement
Independence instead.)
One way to proceed is to just argue that Local Causality is not a relevant or even meaningful
criterion to require of a theory with many worlds. This is the avenue pursued in ideas like the
Parallel Lives Interpretation. This is a very valid approach to pursue because indeed
giving up Local Causality is one of the logical options. It still leaves one with the question of
how to reconcile the measurements that we observe with the locality requirements in Einstein’s
General Relativity, but then some might not find this as important as I do.
If one, however, makes the mistake of thinking that the Many Worlds Interpretation is
locally causal, then one has another problem, which is Bell’s theorem. According to Bell’s
theorem, a theory can only be locally causal if it introduces hidden variables which violate
Measurement Independence. [...]
It is indeed possible to formulate some variants of Many Worlds models that are local, but
these violate Measurement Independence. This should not be surprising because this follows
from Bell’s theorem." We will see what this entails in the following summary of superdeterminism.


Since we now have good reason to believe that there aren't infinitely many branches of history, but probably only one, we must, as I've suggested elsewhere, take superdeterminism more seriously. Superdeterminism is the hypothesis that local realism can be saved by hidden variables, if statistical independence isn't satisfied. More specifically, "a deterministic hidden-variable model is said to be superdeterministic if the so-called Measurement Independence assumption (sometimes referred to as the Statistical Independence assumption or the λ-independence assumption),

ρ(λ|θ) = ρ(λ)

is violated.

In 1964, John Stewart Bell published the Bell inequality, delivering an unwarranted setback to Einstein’s hidden variable theories about quantum mechanics. Einstein had introduced the concept of local realism to address apparent violations of two fundamental physical principles by quantum mechanics: locality and realism. Einstein was concerned about the apparent superluminal "spooky action at a distance", which is not compatible with the theory of relativity.
Locality asserts that no information or influence can travel faster than the speed of light, while realism holds that particles possess well-defined properties even when not being measured. Many physicists interpret Bell’s inequality as proof that hidden variables cannot exist, arguing that its violation implies the rejection of local realism—a conclusion that has been experimentally confirmed.

However, this interpretation is not entirely accurate. Bell’s theorem is based on several key assumptions, and abandoning the notion of hidden variables is not strictly necessary to account for the experimental results. Specifically, Bell’s theorem rests on three fundamental premises:

1. The existence of hidden variables
2. Locality
3. Statistical independence

Statistical independence states that experimental outcomes are not influenced by the choice of measurement—that is, they do not depend on the experimental setup or the decisions made by the experimenters. For this reason, it is also referred to as the assumption of free will.
Bell noted himself:

”There is a way to escape the inference of superluminal speeds and spooky action at a distance. But it involves absolute determinism in the universe, the complete absence of free will. Suppose the world is super-deterministic, with not just inanimate nature running on behind-the-scenes clockwork, but with our behavior, including our belief that we are free to choose to do one experiment rather than another, absolutely predetermined, including the "decision" by the experimenter to carry out one set of measurements rather than another, the difficulty disappears. There is no need for a faster than light signal to tell particle A what measurement has been carried out on particle B, because the universe, including particle A, already "knows" what that measurement, and its outcome, will be.” 

Superdeterminism should be taken more seriously because of its compatibility with Einstein's theory of relativity. For Bell's theorem to work, one must assume non-overlapping past-lightcones for the settings of instruments:

"It has been assumed that the settings of instruments are in some sense free variables — say at the whim of the experimenters — or in any case not determined in the overlap of the backward lightcones. Indeed without such freedom I would not know how to formulate any idea of local causality, even the modest human one."

But this is not a given:

"After all, the backward light cones of those two acts do eventually overlap, and one can imagine one region which controls the decision of the two experimenters who chose a and b."

The mystery of hidden variables appears to grow deeper the further one looks back in time and the farther one moves apart in space. Imagine an experiment involving two entangled photons, conducted at opposite ends of the Milky Way, after two spacecraft have traveled for thousands of years to reach their respective destinations. By the time the measurements take place, the original experimenters are long dead, and their distant descendants must decide how to configure the measurement apparatus to determine the state of one of the photons.
In the standard interpretation of quantum mechanics, this first measurement collapses the quantum state of the photon—and instantaneously, across the vast distances, also determines the state of its entangled counterpart. This apparent faster-than-light influence is one of the key features of quantum entanglement, though it does not allow for faster-than-light communication.

If superdeterminism is correct, however, the states of both photons were predetermined from the moment the spacecraft launched, though they remained unmeasured. In this view, the universe would have had to evolve in such a way that the experimenters (or their descendants) were inevitably led to choose the exact measurement settings that align with the states the particles had possessed since the beginning of the journey.
To many physicists, this idea resembles a kind of cosmic conspiracy—a scenario in which the fundamental laws of the universe conspire to prevent truly free experimental choices, challenging the assumptions underlying conventional scientific methodology.
The key critique is this: "In any deterministic theory one can take a measurement outcome and, by using the law of time-evolution, calculate the initial state that would have given rise to this outcome. One can then postulate that since this initial state gave rise to the observation, we have somehow “explained” the observation. If one were to accept this as a valid argument, this would seemingly invalidate the science method in general. For then, whenever we observe any kind of regularity—say a correlation between X-ray exposure and cancer—we could say it can be explained simply because the initial state happened to be what it was."

Sabine Hossenfelder retorts: "In a superdeterministic theory, the measurement
setting, θ, can be chosen in the same way you can chose it in normal quantum mechanics. It’s
just that what the prepared state will evolve into depends on those measurement settings.
Part of this confusion comes from mistaking a correlation for a causation. Violations of
Statistical Independence, ρ(λ|θ) ≠ ρ(λ), do not tell us that λ influences θ. They merely tell us
that λ and θ are correlated. Another part of this confusion stems from forgetting that Bell’s theorem makes statements 
about the outcomes of measurements. It does not have a time-dependence. So really all the
variables which appear in the theorem are the variables at the time of measurement. This also
immediately clears up the confusion about what happens if one changes the detector settings
between preparation and detection. The answer is: nothing. Because the setting at preparation
was always irrelevant, and so was the exact way in which these settings came about. In a
superdeterministic theory, the evolution of the prepared state depends on what the detector
setting is at the time of measurement. [...]
ρ(λ|θ) ≠ ρ(λ), put into words, means that the
hidden variables which determine the measurement outcome are correlated with the detector
settings at the time of measurement. This does not necessarily mean the hidden variables were
correlated with the measurement settings at the time of preparation. This correlation may instead have come about dynamically.
However, if the theory is deterministic, one can replace the state of the detector at the time
of measurement with the state of the detector at the time of preparation plus the evolution law.
This makes the situation dramatically more complicated because to find out what the setting will
be at the time of detection one now has to calculate what goes on in the experimenter’s brain.
If one in this way insists on describing the measurement setting at the time of detection by the
state of the system at an earlier time plus a hideously complicated evolution of a macroscopic
system, then the violation of Statistical Independence begins to appear conspiratorial. It seems
that, somehow, something must have made the experimenter change the settings so that they
“fit” to the evolution of the prepared state, when really the only thing that matters was the
setting at the time of measurement all along.
One can make the situation even more complicated by rolling back the initial condition
further in time by some billions of years so that now one has to take into account each and everything that happened in the backward lightcone of the actual measurement. It then seems like
one needs to arrange the hidden variables in “just the right way” so that any possible method
for choosing the detector settings comes out correctly. This is one of the reasons that superdetermism is widely believed to be finetuned."

"A “finetuned” theory or a “conspiracy theory” is a theory which lacks explanatory power. A
lack of explanatory power means that the theory requires more information as input than just
collecting the data. This notion of finetuning makes sense because a theory that is finetuned in
this way is unscientific. We should prefer just collecting the data over such a theory.
[...]
Consider this common example of finetuning: A pen balanced on its tip. Should we encounter such a situation, we would suspect a hidden mechanism because otherwise the balance
seems “unnatural”, a “conspiracy”, or “finetuned”. The reason is that we know the system is
unstable under perturbations that (here comes the relevant point) are likely to occur (during the
time of observation) given the initial conditions we are likely to encounter. This means we have
an empirically supported expectation for the state the system is likely in, and the perturbations
it is likely to be exposed to, and these experiences have themselves been obtained by drawing
on theories which have explanatory power.
For this reason, seeing a precariously balanced pen leads us to discard the possibility that it
just happens to have had the right initial condition as possible but implausible. It is much more
likely the pen is held in place by something. Any mechanism that did that would remove the
need to finetune and thus have much higher explanatory power.
The balanced pen is an example for a good, scientific finetuning argument. As example
for an unscientific finetuning argument take the supposed issue that certain parameters of the
standard model of particle physics are finetuned. In this case, we have no way to quantify the
probability of having a different parameter in this model because that would describe a universe
we do not inhabit. Neither can these parameters be perturbed in the real world. The consequence is that this notion of finetuning used to criticize the standard model has no empirical
basis. And unsurprisingly so: The standard model is in some sense the scientifically most successful theory that humans have ever conceived of. It explains Terabytes of data with merely
some dozen parameters. It would be hugely absurd to complain it lacks explanatory power.8
This is not to say that finetuning arguments are always unscientific. Finetuning arguments
can be used when one has a statistical distribution over an allowed set of parameters and a
dynamical law acting on this distribution. If the initial distribution requires us to use detailed
information to get the observed outcome then the theory becomes scientifically useless because
that amounts to assuming what one observes."

But "there need be nothing a priori unscientific about a fine-tuned theory. A fine-tuned theory may be unscientific if one needs to put a lot of information into the initial condition thereby losing explanatory power. But this does not necessarily have to be the case. In fact, according to currently accepted terminology both the standard model of particle physics and the concordance model of cosmology are “fine-tuned” despite arguably being scientifically useful.
One way to avoid that fine-tuning leads to a lack of explanatory power is to find a measure that can be defined in simple terms and that explains which states are “close” to each other and/or which are distant and have measure zero, i.e., are just forbidden.
Bell's and similar examples that rest on arguments from fine-tuning (or sensitivity, or conspiracy) all implicitly assume that there is no simple way to mathematically express the allowed (or likely) initial states that give rise to the predictions of quantum mechanics."

In another paper, it is shown that "one can
violate the (Bell-)Statistical Independence assumption in Bell’s theorem without any
correlations between the measurement outcomes and the hidden variables. The violations of Bell-Statistical Independence can instead come about by the geometry of the
underlying state space. [...] This is a simple way to see that violating
Bell-Statistical Independence does not require fine tuning."

"A lot of people seem to think that violating Statistical Independence is weird, but is no weirder
than quantum mechanics already is because in quantum mechanics too the measurement outcome depends on the detector settings. Yes, it does: You need the detector setting to update the
wave-function. You need it to define the projection operators that enter the collapse postulate.
The reason this doesn’t appear in the math as a violation of Statistical Independence is that
in quantum mechanics we accept that the measurement outcome is not determined, so there
are no hidden variables to correlate with anything. The price we pay is that instead of having
a dynamical law that depends on the detector settings and locally gives rise to a measurement
eigenstate, in quantum mechanics we have a law that does not depend on the detector settings,
but then, when the measurement actually happens, it suddenly becomes undeniable that this
was actually the wrong dynamical law. So then we correct for our mistake by what we call
the update of the wave-function. As a consequence, this update is instantaneous and appears
non-local, but that is because it never described the dynamics of the ontic states to begin with.
One sees this too in Bohmian mechanics and models with spontaneous collapse, both of
which require information about the detector settings to produce dynamics that agree with observation.4
It’s just that followers of these approaches like to insist that really the only thing we
ever measure are positions, in which case there’s no need to choose what you measure, hence
nothing to postulate. Of course this is also why no one who works on quantum field theory
takes either Bohmian mechanics or collapse models seriously."

One argument against superdeterminism posits that superdeterminism would undermine the scientific method entirely. Sabine Hossenfelder summarizes and answers the argument as follows:

"In a nutshell, the argument has it that scientists use the assumption of Statistical Independence to analyze outcomes of random trials. Think about randomly assigning mice to groups
for studying whether tobacco causes cancer, to name a common example. Without Statistical
Independence one would be allowed to postulate that observed correlations (between tobacco
exposure and cancer incidence) just happened to be encoded already in the initial state of the
experimental setup, science would falter, and the world would fall apart into shredded pages of
Epistemological Letters, or so I imagine."

Tim Maudlin puts it like this: “It is like a shill for the tobacco industry first saying that smoking does not cause
cancer, rather there is a common cause that both predisposes one to want to smoke
and also predisposes one to get cancer (this is already pretty desperate), but then
when confronted with randomized experiments on mice, where the mice did not
choose whether or not to smoke, going on to say that the coin flips (or whatever)
somehow always put the mice already disposed to get lung cancer into the experimental group and those not disposed into the control. This is completely and totally
unscientific, and it is an embarrassment that any scientists would take such a claim
seriously.”

However, Hossenfelder goes on to explain:

"The problem with this argument is that, crucially, it leaves aside why we use the assumption
of Statistical Independence and why we do not consider its violations viable scientific explanations for random trials. Let me fill you in: We use the assumption of Statistical Independence
because it works. It has proved to be extremely useful to explain our observations. Merely
assuming that Statistical Independence is violated in such experiments, on the other hand, explains nothing because it can fit any data.
Here, I am using the word “explain” in a very narrow, computational sense. A theory
“explains” data if it requires less detail as input than data it can fit (to some precision). One
could quantify the explanatory power of a theory, but for our purposes that’s an overkill, so here
and in the following I use “explain” to loosely mean “fitting a lot of data with few assumptions”.
This may be somewhat vague but will do.
So, it is good scientific practice to assume Statistical Independence for macroscopic objects
because it explains data and thus help us make sense of the world. It does not, however, follow
that Statistical Independence is also a useful assumption in the quantum realm. And vice versa,
assuming that Statistical Independence is violated in quantum mechanics does certainly not
imply that it must also be violated for mice and cancer cells. In fact, it is extremely implausible
that a quantum effect would persist for macroscopic objects because we know empirically that
this is not the case for any other quantum-typical behavior that we have ever seen.
That it took 43 years for someone to raise this blindingly obvious point goes to show what
happens if you leave philosophers to discuss physics. Luckily, recently an explicit example was
put forward that shows how violations of Statistical Independence in a superdeterministic
model for quantum mechanics are effectively erased along with decoherence. I sincerely hope
(alas, do not actually believe) that this will put to rest the claim that superdeterminism is nonscientific.
Part of the confusion underlying this argument seems to be the belief that giving up Statistical Independence is all there is to superdeterminism. But a violation of Statistical Independence
is merely a property of certain models whose ability to explain data needs to be examined on a
case-by-case basis. Superdeterministic models do not explain any arbitrary set of data thrown at
them; they make specific predictions, to begin with those of quantum mechanics. The ultimate
reason, however, to not assume Statistical Independence for quantum objects is that this way
we might be able to explain more data than we can with quantum mechanics. It is a deterministic approach that in principle predicts individual measurement outcomes, not just averages
thereof. If someone came up with a better explanation for cancer incidence in mice than Statistical Independence and a causal correlation between tobacco exposure and cancer, then we
should certainly take that seriously too.
Needless to say, the claim that superdeterminism would kill science has always been silly
because most scientists care heartily little what is being discussed in the foundations of quantum mechanics. And that’s for good reasons: Quantum effects are entirely negligible for the
description of macroscopic objects."

Now, let's look at superdeterminism in the context of free will. Here, Sabine Hossenfelder differentiates the ability to have done otherwise and the absence of constraints:

"The Statistical Independence assumption is often referred to as “Free Choice,” because it can
be interpreted to imply that the experimenter is free to choose the measurement setting independently of the value of the hidden variables. This has had the effect of anthropomorphising
what is merely a mathematical assumption of a scientific hypothesis. Let us therefore have a
look at the relation between Statistical Independence and the physical processes that underlie
free choice, or free will more generally.
Ever since Hume, the notion of free will has been defined in two different ways:
• as an ability to have done otherwise;
• as an absence of constraints preventing one from doing what one wishes to do.
As in our previous discussion of causality, these definitions are profoundly different in terms of
physical interpretation. An ability to have done otherwise presumes that a hypothetical world
where one did do otherwise is a physically meaningful concept. That is to say, the scientific
meaningfulness of the notion of ‘an ability to have done otherwise’ depends on the extent to
which one’s theory of physics supports the notion of counterfactual worlds: as discussed below,
theories may vary as to this extent.
The second definition, by contrast, does not depend on the existence of counterfactual
worlds. It is defined entirely in terms of events or processes occurring in spacetime. For
example, what one “wishes to do” could be defined in terms of a utility function which the
brain attempts to optimise in coming to what we can call a “choice” or “decision”. This second
definition is often referred to as the compatibilist definition of free will.
Statistical Independence relies on the first of these definitions of free will because (as discussed above) it draws on the notion of counterfactual worlds. The absence of Statistical Independence does not, however, violate the notion of free will as given by the second definition. We
do not usually worry about this distinction because in the theories that we are used to dealing
with, counterfactuals typically lie in the state space of the theory. But the distinction becomes
relevant for superdeterministic theories which may have constraints on state-space that rule out
certain counterfactuals (because otherwise it would imply internal inconsistency). In some superdeterministic models there are just no counterfactuals in state space (for an example, see
Section 5.2), in some cases counterfactuals are partially constrained (see Section 5.1), in others,
large parts of state-space have an almost zero measure (5.3).
One may debate whether it makes sense to speak of free will even in the second case since
a deterministic theory implies that the outcome of any action or decision was in principle fixed
at the beginning of the universe. But even adding a random element (as in quantum mechanics)
does not allow human beings to choose one of several future options, because in this case the
only ambiguities about the future evolution (in the measurement process) are entirely unaffected
by anything to do with human thought. Clearly, the laws of nature are a constraint that can
prevent us from doing what we want to do. To have free will, therefore, requires one to use
the compatibilist notion of free will, even if one takes quantum mechanics in its present form
as fundamental. Free will is then merely a reflection of the fact that no one can tell in advance
what decisions we will make.
But this issue with finding a notion of free will that is compatible with deterministic laws (or
even partly random laws) is not specific to Superdeterminism. It is therefore not an argument
that can be raised against Superdeterminism. Literally all existing scientific theories suffer from
this conundrum. Besides, it is not good scientific practice to discard a scientific hypothesis
simply because one does not like its philosophical implications.
Let us look at a simple example to illustrate why one should not fret about the inability
of the experimenter to prepare a state independently of the detector. Suppose you have two
fermions. The Pauli exclusion principle tells us that it is not possible to put these two particles
into identical states. One could now complain that this violates the experimenter’s free will, but
that would be silly. The Pauli exclusion principle is a law of nature; it’s just how the world is.
Violations of Statistical Independence, likewise, merely tell us what states can exist according
to the laws of nature. And the laws of nature, of course, constrain what we can possibly do.
In summary, raising the issue of free will in the context of Superdeterminism is a red herring.
Superdeterminism does not make it any more or less difficult to reconcile our intuitive notion
of free will with the laws of nature than is the case for the laws we have been dealing with for
hundreds of years already."

Hossenfelder's idea about what a superdeterministic model would require, is called the path integral approach:

"The path integral approach to Superdeterminism rests on the observation that the Feynman
path integral has a future input dependence already, which is the upper time of the integration.
However, in the usual path integral of quantum mechanics (and, likewise, of quantum field
theory), one does not evaluate what is the optimal future state that the system can evolve into.
Instead, one posits that all of the future states are realized, which results in a merely probabilistic
prediction.
The idea is then to take a modified path integral for the combined system of detector and
prepared state and posit that in the underlying theory the combined system evolves along merely
one possible path in state space that optimizes a suitable, to-be-defined, function. This function
must have the property that initial states which evolve into final states containing superpositions
of detector eigenstate states are disfavoured, in the sense that they do not optimize the function.
Instead, the optimal path that the system will chose is one that ends up in states which are
macroscopically classical. One gets back normal quantum mechanics by averaging over initial
states of the detector.
This approach solves the measurement problem because the system does deterministically
evolve into one particular measurement outcome. Exactly which outcome is determined by the
degrees of freedom of the detector that serve as the “hidden variables”. Since it is generically
impossible to exactly know all the detector’s degrees of freedom, quantum mechanics can only
make probabilistic predictions.
The challenge of this approach is to find a suitable function that actually has this behaviour."

But in another text Hossenfelder stresses:

"this does not mean future-input
dependence is retrocausal in the space-time way, so to avoid confusion, I will simply refer to it
as future-input dependence. Future-input dependence means that a model draws on input that
will become available only in the future.
Please keep in mind that the interventionist way to think about causality is all about what
you, as a user, can do with a model, not about the physics which the model describes. So, saying
that some input will only become available in the future does not mean it didn’t previously exist.
In a deterministic model, if some information is present on one space-like hypersurface, it is
present on all of them. Thing is though, this information may not be available in any practically
useful form before a certain point in time. [...] The
input that goes into the superdeterministic model is the measurement setting at the time of
measurement. At the time of preparation, you do not have this input, so it’s fair to call it “future
input”. In principle, the information is there, somewhere, on the preparation hypersurface, but
for all practical purposes you have no way of figuring it out. The result is that you have a model
which makes contingent predictions depending on a parameter you will only learn about in the
future. You may not know at preparation what later will be measured, but you can very well
say: If the measurement setting is this, then the prediction is that.
You may wonder now why not take one of the good, old Hamiltonian evolutions, rather
than one that has future input in the dynamical law? Because if you do this, the model isn’t
useful. You would have to precisely know the details of the initial state to be able to calculate
anything with it. Future-input dependence, hence, is the reason why superdeterminism is not a
conspiracy theory. It demonstrates that there is a simple way to write down the dynamical law
that does not require much information."


To experimentally test superdeterminism, Hossenfelder has proposed conducting measurements on extremely small scales and at very low temperatures. The goal is to reduce the chaotic motion of electrons in order to determine whether they behave more deterministically than previously thought.

In a 2013 presentation, Hossenfelder suggested repeatedly measuring the non-commuting variables of a single particle at very short intervals. Non-commuting variables, such as position and momentum, cannot be precisely determined at the same time. Another approach involves spin measurements in different directions, where each new measurement disrupts the previous state. However, she assumes that hidden variables might change if measurements aren’t performed quickly enough or if the experimental setup allows for interference, which is why low temperatures are necessary. Hossenfelder's idea was inspired by an account of Eugene Wigner about John von Neumann's thought on hidden variables:

“Von Neumann often discussed the measurement of the spin component of a spin-1/2
particle in various directions. Clearly, the possibilities for the two possible outcomes of a
single such measurement can be easily accounted for by hidden variables [...] However,
Von Neumann felt that this is not the case for many consecutive measurements of
the spin component in various different directions. The outcome of the rst such
measurement restricts the range of values which the hidden parameters must have
had before that rst measurement was undertaken. The restriction will be present also
after the measurement so that the probability distribution of the hidden variables
characterizing the spin will be different for particles for which the measurement gave
a positive result from that of the particles for which the measurement gave a negative
result. The range of the hidden variables will be further restricted in the particles for
which a second measurement of the spin component, in a different direction, also gave
a positive result...”

The key to testing superdeterminism is to create a system that interacts with its surroundings as little as possible. If the environment interferes with the experiment, it could alter the hidden variables. Hossenfelder’s proposed experiment consists of a system with two mirrors and two polarized beam splitters—one testing spin up/down and the other testing spin right/left.
According to standard quantum mechanics, a photon encountering a beam splitter has a 50% chance of passing through and a 50% chance of being reflected. If reflected, it exits the system; otherwise, after passing through both beam splitters, it reaches a mirror, repeats the process, encounters another mirror on the other side, and continues indefinitely. Since the photon has a 50% chance of being reflected at each step, the probability of it remaining in the system drops quickly to near zero.

However, in a superdeterministic framework, the outcome would be predetermined. If the photon initially passes through both beam splitters, it must continue doing so in every subsequent cycle—unless an external disturbance alters the hidden variables. Since perfectly isolating such a system is extremely difficult in practice, the best experimental evidence for superdeterminism would be a slight statistical deviation from quantum mechanical predictions. Specifically, there would be a small but measurable increase in the time photons remain in the system compared to what standard quantum mechanics predicts.

Hossenfelder writes:

"Since the theory is deterministic, this tells us that if we manage to create a time-sequence of
initial states similar to each other, then the measurement outcomes should also be similar. This
means conceretely that rather than fulfilling the Born-rule, such an experiment would reveal
time-correlations in the measurement outcomes. The easiest way to understand this is to keep
in mind that if we were able to exactly reproduce the initial state, then in a superdeterministic
theory the measurement outcome would have to be the same each time, in conflict with the
predictions of quantum mechanics.
This raises the question how similar the initial states have to be for this to be observable.
Unfortunately, this is not a question which can be answered in generality; for this one would
need a theory to make the corresponding calculation. However, keeping in mind that the simplest case of hidden variables are the degrees of freedom of other particles and that the theory is
local in the way we are used to it, the obvious thing to try is minimizing changes of the degrees
of freedom of the detecting device. Of course one cannot entirely freeze a detector’s degrees
of freedom, for then it could no longer detect something. But one can at least try to prevent
non-essential changes, i.e., reduce noise.
This means concretely that one should make measurements on states prepared as identically
as possible with devices as small and cool as possible in time-increments as small as possible.
This consideration does not change much if one believes the hidden variables are properties
of the particle after all. In this case, however, the problem is that preparing almost identical initial states is impossible since we do not know how to reproduce the particle’s hidden variables.
One can then try to make repeated measurements of non-commuting observables on the same
states, as previously laid out in. The distinction between the predictions of quantum mechanics and the predictions of the
underlying, superdeterministic theory is not unlike the distinction between climate predictions
and weather forecasts. So far, with quantum mechanics, we have made predictions for longterm averages. But even though we are in both cases dealing with a non-linear and partly
chaotic system, we can in addition also make short-term predictions, although with limited
accuracy. The experiment proposed here amounts to recording short-term trends and examining
the data for regularities that, according to quantum mechanics alone, should not exist.
Needless to say, the obvious solution may not be the right one and testing Superdeterminism
may be more complicated than that. But it seems reasonable to start with the simplest and most
general possibility before turning to model-specific predictions."

The question is, will we be able to measure a statistical deviation between what normal quantum mechanics and what superdeterministic quantum mechanics would predict. Sabine Hossenfelder tried to estimate this possibility in the following way:

"To see whether such an experiment is possible in an interesting parameter
range, let us estimate what is the typical decay time to expect. As an example,
consider a photodetector constituted of N atoms that measure an infalling
photon by excitation of an electron into the conduction band. The energy gap
of the material be ∆E and the temperature be T. At that temperature, one
single atom has a probability of exp(−∆E/T ) to become excited by thermal
motion, and it remains so for the average electron-hole recombination time
τr. The time τ̃  for N atoms to undergo a statistical change is thus

![Equation](https://latex.codecogs.com/svg.latex?%5Ctilde%7B%5Ctau%7D%20%5Capprox%20%5Cfrac%7B%5Cexp%28%5CDelta%20E%2FT%29%7D%7BN%7D%5Ctau_r.)

There are many other sorts of noise, but this is the one with the highest
frequency, and thus the one relevant for the decay time.
We set this time in relation to the decay time of the correlation by τ = ατ̃ .
Here, the dimensionless parameter α is the parameter we want to constrain
by experiment. For a SDHVT [super-deterministic hidden variable theory] one would expect it to be of order one, whereas
for ordinary quantum mechanics it is zero.
If one inserts typical values of ∆E ∼ .5eV, T ∼ 300K, τr ∼ 1 ns, and
N ≈ 10^20 one finds τ̃ ∼ 10^−20s and, for α ≈ 1, a hopelessly small correlation
time. One might think about cooling the whole system down. However, this
would practically necessitate immersion in some liquid, thereby increasing N
and the difficulty to perform an experiment to begin with. However, if we
consider instead a microscopically small detector, maybe with an extension
of some µm, we could get down to N ≈ 10^15. Let us further take a semiconductor with a fairly large band gap of ∆E ≈ 1 eV. Then we get down
τ̃ ≈ 10^−6 s. This now has to be compared with the typical time in which
measurements can be repeated.
In 10^−6 sec a photon travels a distance of about 100m. The experiment
cannot be arbitrarily small because it still should be larger than the wavelength of the photon, but a size of a mm is, with visible light, still large
enough. In this case, the time for a photon to get from one end of the experiment to the other is below 10^−11sec, and is as desired much shorter than
the decay time, leaving space for non-ideal material and other sources of
experimental uncertainty.
For the example of the photon, the detectors A and B could measure
different polarization states in a photonic version of the Stern-Gerlach experiment, when such devices can be made sufficiently small. Alternatively,
one can use electrons and spin-filters. While the research in this area
is not yet sufficiently advanced, it may soon be. To return the electron into
the initial state, the function of a mirror could for example be mimicked by
Bragg-reflection on sufficient layers of suitably spaced atoms. Electrons bring
the advantage that the wavelength is smaller and thus the experiment can
be made smaller. It has the disadvantage that it is more difficult to handle
charged particles.
In either case, the detector would let the particle (photon or electron)
trespass straight for one particular combination of measurement outcomes
(polarization or spin), while the particle would be deviated to leave the loop
if it does not have this particular combination. The quantity of interest is
then time it takes for the particle to leave the loop at either detector A or
B. The average value of these time measurements is the correlation time.
Let us denote the measurement outcomes the particle needs to have to
stay in the loop with a and b, corresponding to the two detectors (for example
spin up and left). The probabilities for these measurement outcomes be pa
and pb, respectively. In ordinary quantum mechanics the probability for a
particle that has entered the system to be deviated out of the system at
detector A or B before it returns to the point of entry is (1−pa)²(1−pb). Then,
it has a probability of p that it leaves through the imperfect one-way mirror,
and the probability to make it into the next loop is (1−p)(1−pa)²(1−pb). The
probability that the particle is still in the system after the mth reflection at
the one-way mirror is (1−p)^m (1−pa)^m+1 (1−pb)^m. In the SDHVT however,
we know that if the particle has fulfilled the conditions to make the loop once
it will continue to do so and, on a duration shorter than the autocorrelationtime, it should return to the mirror on the second screen with probability
(1 − p)^m (1 − pa)(1 − pb).
With p ≪ 1 and pa, pb of order one, in ordinary quantum mechanics the
probability is thus very high that the photon will be detected within only a
few rounds. In the SDHVT, detection should occur either in the first round,
or with a delay relative to the standard case. There is not much use in making
the propagation time of the particle very short by making the experiment
very small, since the time still has to be measureable. However, with presentday technology, it seems feasible to measure a correlation time of 10^−6
sec and
distinguish it from no correlation.
In addition to the above considerations about measurability of the autocorrelation, there is of course experimental error that needs to be taken into
account. Most crucially, we have so far assumed that the mirrors are perfectly
even. In reality, they will of course have a finite surface roughness and the
electron or photon will only return to its initial state with some limited precision, which then alters the probability for it to make the same loop again.
We can estimate this effect by noting it will become relevant if the unwanted
dislocation of the particle per loop, ∆zm, through a non-perfect reflection
makes a non-negligible change to its relative location to the degrees of freedom of the environment. In the case considered here, the relevant distance
would be of about one atomic diameter, or 10^−10m. If we denote this distance
with zN , then we have an additional contribution to the autocorrelation time
that increases with m,

![Equation](https://latex.codecogs.com/svg.latex?Corr_{\kappa}%20=%20\exp\left(-\frac{\kappa}{\alpha\tilde{\tau}}\right)%20+%20\exp\left(-m\frac{\Delta%20z_m}{z_N}\right)) ."
